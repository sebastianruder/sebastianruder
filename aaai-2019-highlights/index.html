<!DOCTYPE html>
<html lang="en">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>AAAI 2019 Highlights: Dialogue, reproducibility, and more</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="stylesheet" type="text/css" href="../assets/built/screen.css?v=96152eef5a" />

    <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon" />
    <link rel="canonical" href="https://ruder.io/aaai-2019-highlights/" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <link rel="amphtml" href="https://ruder.io/aaai-2019-highlights/amp/" />
    
    <meta property="og:site_name" content="Sebastian Ruder" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="AAAI 2019 Highlights: Dialogue, reproducibility, and more" />
    <meta property="og:description" content="This post discusses highlights of AAAI 2019. It covers dialogue, reproducibility, question answering, the Oxford style debate, invited talks, and a diverse set of research papers." />
    <meta property="og:url" content="https://ruder.io/aaai-2019-highlights/" />
    <meta property="og:image" content="https://ruder.io/content/images/2019/02/aaai_reception-1.jpg" />
    <meta property="article:published_time" content="2019-02-07T17:00:00.000Z" />
    <meta property="article:modified_time" content="2019-03-04T20:39:40.000Z" />
    <meta property="article:tag" content="events" />
    <meta property="article:tag" content="natural language processing" />
    <meta property="article:tag" content="transfer learning" />
    <meta property="article:tag" content="word embeddings" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="AAAI 2019 Highlights: Dialogue, reproducibility, and more" />
    <meta name="twitter:description" content="This post discusses highlights of AAAI 2019. It covers dialogue, reproducibility, question answering, the Oxford style debate, invited talks, and a diverse set of research papers." />
    <meta name="twitter:url" content="https://ruder.io/aaai-2019-highlights/" />
    <meta name="twitter:image" content="https://ruder.io/content/images/2019/02/aaai_reception-1.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Sebastian Ruder" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="events, natural language processing, transfer learning, word embeddings" />
    <meta name="twitter:site" content="@seb_ruder" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="1088" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Sebastian Ruder",
        "url": "https://ruder.io/",
        "logo": {
            "@type": "ImageObject",
            "url": {
                "@type": "ImageObject",
                "url": "https://ruder.io/favicon.ico",
                "width": 48,
                "height": 48
            }
        }
    },
    "author": {
        "@type": "Person",
        "name": "Sebastian Ruder",
        "image": {
            "@type": "ImageObject",
            "url": "https://ruder.io/content/images/2019/02/new_profile_photo_square-1.jpg",
            "width": 2000,
            "height": 2000
        },
        "url": "https://ruder.io/author/sebastian/",
        "sameAs": []
    },
    "headline": "AAAI 2019 Highlights: Dialogue, reproducibility, and more",
    "url": "https://ruder.io/aaai-2019-highlights/",
    "datePublished": "2019-02-07T17:00:00.000Z",
    "dateModified": "2019-03-04T20:39:40.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://ruder.io/content/images/2019/02/aaai_reception-1.jpg",
        "width": 2000,
        "height": 1088
    },
    "keywords": "events, natural language processing, transfer learning, word embeddings",
    "description": "This post discusses highlights of AAAI 2019. It covers dialogue, reproducibility, question answering, the Oxford style debate, invited talks, and a diverse set of research papers.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://ruder.io/"
    }
}
    </script>

    <meta name="generator" content="Ghost 3.11" />
    <link rel="alternate" type="application/rss+xml" title="Sebastian Ruder" href="https://ruder.io/rss/" />
    <script>
var profile_title = 'Sebastian Ruder';
</script>
<script>
var disqus_shortname = 'sebastianruder';
</script>
<script>
var profile_resume ='NLP PhD student';
</script>
<script>
var ga_id = 'UA-60512592-1';
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [["$", "$"], ["\\(", "\\)"]],
        processEscapes: true
    }
});
</script>

</head>
<body class="post-template tag-events tag-natural-language-processing tag-transfer-learning tag-word-embeddings">

    <div class="site-wrapper">

        

<header class="site-header">
    <div class="outer site-nav-main">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left-wrapper">
        <div class="site-nav-left">
                <a class="site-nav-logo" href="https://ruder.io">Sebastian Ruder</a>
            <div class="site-nav-content">
                    <ul class="nav" role="menu">
    <li class="nav-about" role="menuitem"><a href="https://ruder.io/about/">About</a></li>
    <li class="nav-tags" role="menuitem"><a href="https://ruder.io/tags/">Tags</a></li>
    <li class="nav-papers" role="menuitem"><a href="https://ruder.io/publications/">Papers</a></li>
    <li class="nav-talks" role="menuitem"><a href="https://ruder.io/talks/">Talks</a></li>
    <li class="nav-news" role="menuitem"><a href="https://ruder.io/news/">News</a></li>
    <li class="nav-faq" role="menuitem"><a href="https://ruder.io/faq/">FAQ</a></li>
    <li class="nav-newsletter" role="menuitem"><a href="https://ruder.io/nlp-news/">Newsletter</a></li>
    <li class="nav-nlp-progress" role="menuitem"><a href="https://nlpprogress.com/">NLP Progress</a></li>
    <li class="nav-media" role="menuitem"><a href="https://ruder.io/media/">Media</a></li>
    <li class="nav-contact" role="menuitem"><a href="https://ruder.io/contact/">Contact</a></li>
</ul>

                    <span class="nav-post-title dash">AAAI 2019 Highlights: Dialogue, reproducibility, and more</span>
            </div>
        </div>
    </div>
    <div class="site-nav-right">
            <div class="social-links">
                    <a class="social-link social-link-tw" href="https://twitter.com/seb_ruder" title="Twitter" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
</a>
            </div>
                <a class="rss-button" href="https://ruder.io/rss/index.rss" title="RSS" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
</a>

    </div>
</nav>
    </div>
</div></header>


<main id="site-main" class="site-main outer">
    <div class="inner">

        <article class="post-full post tag-events tag-natural-language-processing tag-transfer-learning tag-word-embeddings ">

            <header class="post-full-header">

                <section class="post-full-tags">
                    <a href="../tag/events/index.html">events</a>
                </section>

                <h1 class="post-full-title">AAAI 2019 Highlights: Dialogue, reproducibility, and more</h1>

                <p class="post-full-custom-excerpt">This post discusses highlights of AAAI 2019. It covers dialogue, reproducibility, question answering, the Oxford style debate, invited talks, and a diverse set of research papers.</p>

                <div class="post-full-byline">

                    <section class="post-full-byline-content">

                        <ul class="author-list">
                            <li class="author-list-item">

                                <div class="author-card">
                                    <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                                    <div class="author-info">
                                        <h2>Sebastian Ruder</h2>
                                        <p>Read <a href="../author/sebastian/index.html">more posts</a> by this author.</p>
                                    </div>
                                </div>

                                <a href="../author/sebastian/index.html" class="author-avatar">
                                    <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                                </a>

                            </li>
                        </ul>

                        <section class="post-full-byline-meta">
                            <h4 class="author-name"><a href="../author/sebastian/index.html">Sebastian Ruder</a></h4>
                            <div class="byline-meta-content">
                                <time class="byline-meta-date" datetime="2019-02-07">7 Feb 2019</time>
                                <span class="byline-reading-time"><span class="bull">&bull;</span> 11 min read</span>
                            </div>
                        </section>

                    </section>


                </div>
            </header>

            <figure class="post-full-image">
                <img
                    srcset="../content/images/size/w300/2019/02/aaai_reception-1.jpg 300w,
                           ../content/images/size/w600/2019/02/aaai_reception-1.jpgg 600w,
                          ../content/images/size/w1000/2019/02/aaai_reception-1.jpg 1000w,
                         ../content/images/size/w2000/2019/02/aaai_reception-1.jpg 2000w"
                    sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px"
                    src="../content/images/size/w2000/2019/02/aaai_reception-1.jpg"
                    alt="AAAI 2019 Highlights: Dialogue, reproducibility, and more"
                />
            </figure>

            <section class="post-full-content">
                <div class="post-content">
                    <p>This post discusses highlights of the <a href="https://aaai.org/Conferences/AAAI-19/">Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)</a>.</p><p>I attended <a href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a> in Honolulu, Hawaii last week. Overall, I was particularly surprised by the interest in natural language processing at the conference. There were 15 sessions on NLP (most standing-room only) with ≈10 papers each (oral and spotlight presentations), so around 150 NLP papers (out of 1,150 accepted papers overall). I also really enjoyed the diversity of invited speakers who discussed topics from AI for social good, to adversarial learning and imperfect-information games (videos of all invited talks are available <a href="https://aaai.org/Conferences/AAAI-19/invited-speakers/">here</a>). Another cool thing was the <a href="index.html#debate">Oxford style debate</a>, which required debaters to take controversial positions. This was a nice change of pace from panel discussions, which tend to converge to a uniform opinion.</p><p>Table of contents:</p><ul><li><a href="index.html#dialogue">Dialogue</a></li><li><a href="index.html#reproducibility">Reproducibility</a></li><li><a href="index.html#question-answering">Question answering</a></li><li><a href="index.html#ai-for-social-good">AI for social good</a></li><li><a href="index.html#debate">Debate</a></li><li><a href="index.html#adversarial-learning">Adversarial learning</a></li><li><a href="index.html#imperfect-information-games">Imperfect-information games</a></li><li><a href="index.html#inductive-biases">Inductive biases</a></li><li><a href="index.html#transfer-learning">Transfer learning</a></li><li><a href="index.html#word-embeddings">Word embeddings</a></li><li><a href="index.html#miscellaneous">Miscellaneous</a></li></ul><h1 id="dialogue">Dialogue</h1><p>In his talk at the <a href="https://sites.google.com/view/deep-dial-2019/home?authuser=0">Reasoning and Learning for Human-Machine Dialogues workshop</a>, <a href="https://scholar.google.com/citations?user=V72PR9wAAAAJ&amp;hl=en">Phil Cohen</a> argued that <strong>chatbots are an attempt to avoid solving the hard problems of dialogue</strong>. They provide the <em>illusion</em> of having a dialogue but in fact do not have a clue what we are saying or meaning. What we should rather do is <strong>recognize intents via semantic parsing</strong>. We should then reason about the speech acts, infer a user's <em>plan</em>, and help them to succeed. You can find more information about his views in <a href="https://arxiv.org/abs/1812.01144">this position paper</a>.</p><p>During the panel discussion, <a href="https://www.microsoft.com/en-us/research/people/izitouni/">Imed Zitouni</a> highlighted that the limitations of current dialogue models affect user behaviour. <strong>75-80% of the time users only employ 4 skills</strong>: "play music", "set a timer", "set a reminder", and "what is the weather". Phil argued that we should not have to learn how to talk, how to make an offer, etc. all over again for each domain. We can often build simple dialogue agents for new domains <a href="http://www.aclweb.org/anthology/P18-2008">"overnight"</a>.</p><h1 id="reproducibility">Reproducibility</h1><p>At the <a href="https://www.idi.ntnu.no/~odderik/RAI-2019/">Workshop on Reproducible AI</a>, <a href="http://joelgrus.com/">Joel Grus</a> argued that <a href="https://docs.google.com/presentation/d/1ivK8AKgz8Hx-ZYzPC9gJyQK6tzuhR3UuhCEajFGJDlA/"><strong>Jupyter notebooks are bad for reproducibility</strong></a>. As an alternative, he recommended to adopt higher-level abstractions and declarative configurations. Another good resource for reproducibility is the <a href="https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf">ML reproducibility checklist</a> by <a href="https://www.cs.mcgill.ca/~jpineau/">Joelle Pineau</a>, which provides a list of items for algorithms, theory, and empirical results to enforce reproducibility.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/02/unit_tests_for_ai_experiments.png" class="kg-image"><figcaption>Unit tests for AI experiments recommended by Joel Grus</figcaption></figure><p>A team from Facebook <a href="https://arxiv.org/abs/1902.04522">reported on their experiments reproducing AlphaZero</a> in their <a href="https://github.com/pytorch/ELF">ELF framework</a>, training a model using 2,000 GPUs in 2 weeks. <strong>Reproducing an on-policy, distributed RL system such as AlphaZero is particularly challenging</strong> as it does not have a fixed dataset and optimization is dependent on the distributed environment. Training smaller versions and scaling up is key. For reproducibility, the random seed, the git commit number, and the logs should be stored.</p><p>During the panel discussion, <a href="https://www.ntnu.edu/employees/odderik">Odd Eric Gunderson</a> argued that reproducibility should be defined as the <em>ability of an independent research team to produce the same results using the same AI method based on the documentation by the original authors</em>. Degrees of reproducibility can be measured based on the availability of different types of documentation, such as the method description, data, and code.</p><p><a href="https://www.isye.gatech.edu/users/pascal-van-hentenryck">Pascal van Hentenryck</a> argued that reproducibility could be made part of the peer review process, such as in the <a href="http://mpc.zib.de/">Mathematical Programming Computation journal</a> where each submission requires an executable file (which does not need to be public). He also pointed out that—empirically—papers with supplementary materials are more likely to be accepted.</p><h1 id="question-answering">Question answering</h1><p>At the <a href="https://researcher.watson.ibm.com/researcher/view_group_subpage.php?id=9904">Reasoning and Complex QA Workshop</a>, <a href="https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/forbus-ken.html">Ken Forbus</a> discussed an <a href="http://www.qrg.northwestern.edu/papers/Files/QRG_Dist_Files/QRG_2018/Crouse-McFate-Forbus-2018.pdf">analogical training method for QA</a> that adapts a general-purpose semantic parser to a new domain with few examples. At the end of his talk, Ken argued that the <strong>train/test method in ML is holding us back</strong>. Our learning systems should use rich relational representations, gather their own data, and evaluate progress.</p><p><a href="https://allenai.org/team/ashishs/">Ashish Sabharwal</a> discussed the <a href="https://github.com/allenai/OpenBookQA">OpenBookQA dataset</a> presented at EMNLP 2018 during his talk. The open book setting is situated between reading comprehension and open-ended QA on the textual QA spectrum (see below).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/02/textual_qa_spectrum.png" class="kg-image"><figcaption>The textual QA spectrum</figcaption></figure><p>It is designed to probe a deeper understanding rather than memorization skills and requires applying core principles to new situations. He also argued that while entailment is recognized as a core NLP task with many applications, it is still lacking a convincing application to an end-task. This is mainly due to multi-sentence entailment being a lot harder, as irrelevant sentences often have significant textual overlap.</p><p>Furthermore, he discussed the design of leaderboards, which have to make tradeoffs along multiple competing axes with respect to the host, the submitters, and the community. <strong>A particular deficit of current leaderboards is that they make it difficult to share and build upon successful techniques. </strong>For an extensive discussion of the pros and cons of leaderboards, check out <a href="https://soundcloud.com/nlp-highlights/80-leaderboards-and-science-with-siva-reddy">this recent NLP Highlights podcast</a>.</p><p>The first part of the final panel discussion focused on important outstanding technical challenges for question answering. <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-witbrock">Michael Witbrock</a> emphasized <strong>techniques to create datasets that cannot easily be exploited by neural networks</strong>, such as the adversarial filtering in <a href="http://aclweb.org/anthology/D18-1009">SWAG</a>. Ken argued that models should come up with answers and explanations rather than performing multiple choice question answering, while Ashish noted that such explanations need to be automatically validated.</p><p><a href="https://www.cs.cmu.edu/~hovy/">Eduard Hovy</a> suggested that one way towards a system that can perform more complex QA could consist of the following steps:</p><ol><li>Build a symbolic numerical reasoner that leverages relations from an existing KB, such as <a href="http://www.cs.utexas.edu/users/ml/nldata/geoquery.html">Geobase</a>, which contains geography facts.</li><li>Look at the subset of questions in existing natural language datasets, which require reasoning that is possible with the reasoner.</li><li>Annotate these questions with semantic parses and train a semantic parsing model to convert the questions to logical forms. These can then be provided to the reasoner to produce an answer.</li><li>Augment the reasoner with another reasoning component and repeat steps 2-3.</li></ol><p>The panel members noted that such reasoners exist, but lack a common API.</p><p>Finally, here are a few papers on question answering that I enjoyed:</p><ul><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-RuckleA.3648.pdf"><strong>COALA: A Neural Coverage-Based Approach for Long Answer Selection with Small Data</strong></a>: An approach that ranks answers based on how many of the question aspects they cover. They incorporate syntactic information via dependency parses and find that this improves performance.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-DengYang.4661.pdf"><strong>Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge Base Question Answering</strong></a>: Answer selection and knowledge base QA are learned jointly via multi-task learning. Attention is performed on different views of the data.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-TafjordO.6869.pdf"><strong>QUAREL: A Dataset and Models for Answering Questions about Qualitative Relationships</strong></a>: A challenging new QA dataset of 2,771 story questions that require knowledge about qualitative relationships pertaining to 19 quantities such as smoothness, friction, speed, heat, and distance.</li></ul><h1 id="ai-for-social-good">AI for social good</h1><p>During his invited talk, <a href="http://teamcore.usc.edu/tambe/">Milind Tambe</a> looked back on 10 years of research in AI and multiagent systems for social good (video available <a href="https://vimeo.com/313940453">here</a>; slides available <a href="http://teamcore.usc.edu/lectures/AAAI_2019.pdf">here</a>). Milind discussed his research on using game theory to optimize security resources such as <a href="https://pdfs.semanticscholar.org/1a10/5181f785502be8d71e6f6f0569e6eedd60e6.pdf">patrols at airports</a>, <a href="https://research.create.usc.edu/cgi/viewcontent.cgi?referer=https://scholar.google.com/&amp;httpsredir=1&amp;article=1134&amp;context=nonpublished_reports">air marshal assignments on flights</a>, <a href="http://teamcore.usc.edu/projects/coastguard/default.htm">coast guard patrols</a>, and <a href="http://teamcore.usc.edu/people/Paws/index.html">ranger patrols in African national parks to protect against poachers</a>. Overall, his talk was a striking reminder of the positive effects AI can have if it is employed for social good.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/02/ml_for_predicting_poacher_behaviour.png" class="kg-image"><figcaption>An overview of an ML approach for predicting poacher behaviour in an African national park</figcaption></figure><h1 id="debate">Debate</h1><p>The <a href="https://en.wikipedia.org/wiki/Debate#Oxford-style_debating">Oxford style</a> debate focused on the proposition “The AI community today should continue to focus mostly on ML methods” (video available <a href="https://vimeo.com/314378703">here</a>). It pitted <a href="https://www.cs.purdue.edu/homes/neville/">Jennifer Neville</a> and <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a> on the 'pro' side against <a href="http://cs.brown.edu/~mlittman/">Michael Littman</a> and <a href="https://allenai.org/team/orene/">Oren Etzioni</a> on the 'against' side, with <a href="https://www.cs.ubc.ca/~kevinlb/">Kevin Leyton-Brown</a> as moderator. Overall, the debate was entertaining and engaging to watch.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/02/oxford_style_debate.png" class="kg-image"><figcaption>The debater panel (from left to right): Peter Stone, Jennifer Neville, Kevin Leyton-Brown (moderator), Michael Littman, Oren Etzioni</figcaption></figure><p>Here are some representative remarks from each of the debaters that stuck with me:</p><blockquote><em>"The unique strength of the AI community is that we focus on the problems that need to be solved." – Jennifer Neville<br>"We are in the middle of one of the most amazing paradigm shifts in all of science, certainly computer science." – Oren Etzioni<br>"If you want to have an impact, don’t follow the bandwagon. Keep alive other areas." – Peter Stone<br>"Scientists in the natural sciences are actually very excited about ML as much of their research relies on expensive computations, which can be approximated with neural networks." – Michael Littman</em></blockquote><p>There were some important observations and ultimately a general consensus that ML alone is not enough and we need to integrate other methods with ML. Yonatan Belinkov also <a href="https://twitter.com/boknilev/status/1090451665486925825">live tweeted</a>, while I <a href="https://twitter.com/seb_ruder/status/1090454767438946304">tweeted some remarks that elicited laughs</a>.</p><h1 id="adversarial-learning">Adversarial learning</h1><p>During his invited talk (video available <a href="https://vimeo.com/313941176">here</a>), <a href="https://ai.google/research/people/105214">Ian Goodfellow</a> discussed a multiplicity of areas to which adversarial learning has been applied. Among many advances, Ian mentioned that he was impressed by the performance and flexibility of <a href="https://arxiv.org/abs/1805.08318">attention masks for GANs</a>, particularly that they are not restricted to circular masks. </p><p>He discussed adversarial examples, which are a consequence of moving away from i.i.d. data: attackers are able to confuse the model by showing unusual data from a different distribution such as <a href="https://arxiv.org/abs/1707.08945">graffiti on stop signs</a>. He also argued—contrary to the prevalent opinion—that deep models that are more robust are more interpretable than linear models. The main reason is that the latent space of a linear model is totally unintuitive, while a more robust model is more inspectable (as can be seen below).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/02/latent_space_vulnerable_model_vs_robust_model.png" class="kg-image"><figcaption>Traversing the latent space of a linear model (left) vs. a deep, more robust model (right) between different MNIST labels starting from "9"</figcaption></figure><p>Semi-supervised learning with GANs can allow models to be more sample-efficient. What is interesting about such applications is that they focus on the discriminator (which is normally discarded) rather than the generator where the <a href="https://arxiv.org/abs/1606.03498">discriminator is extended</a> to <a href="https://arxiv.org/abs/1606.03498">classify </a><a href="https://arxiv.org/abs/1606.01583"><em>n+1</em> classes</a>. Regarding leveraging GANs for NLP, Ian conceded that we currently have not found a good way to deal with the large action space required to generate sentences with RL.</p><h1 id="imperfect-information-games">Imperfect-information games</h1><p>In his invited talk (video available <a href="https://vimeo.com/313942390">here</a>), <a href="http://www.cs.cmu.edu/~sandholm/">Tuomas Sandholm—</a>whose <a href="https://www.engadget.com/2017/02/10/libratus-ai-poker-winner/">AI Libratus was the first AI to beat top Heads-Up No-Limit Texas Hold'em professionals in January 2017—</a>discussed new results for solving imperfect-information games. He stressed that <strong>only game-theoretically sound techniques yield strategies that are robust against all opponents in imperfect-information games</strong>. Other advantages of a game-theoretic approach are a) that even if humans have access to the entire history of plays of the AI, they still can't find holes in its strategy; and b) it requires no data, just the rules of the game.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/02/real-world_applications_imperfect-information_games.png" class="kg-image"><figcaption>Most real-world applications are imperfect-information games</figcaption></figure><p>For solving such games, the quality of the solution depends on the quality of the abstraction. Developing better abstractions is thus important, which also applies to modelling such games. In imperfect-information games, planning is important. In real-time planning, we must consider how the opponent can adapt to changes in the policy. In contrast to perfect-information games, states do not have well-defined values.</p><h1 id="inductive-biases">Inductive biases</h1><p>There were several papers that incorporated different inductive biases into existing models:</p><ul><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-GuptaPankaj1.4838.pdf"><strong>Document Informed Neural Autoregressive Topic Models with Distributional Prior</strong></a>: An extension of the <a href="https://papers.nips.cc/paper/4613-a-neural-autoregressive-topic-model.pdf">DocNADE</a> topic model using word embedding vectors as prior. The model is evaluated on 15 datasets.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-XiaQ.7468.pdf"><strong>Syntax-aware Neural Semantic Role Labeling</strong></a>: The authors incorporate various syntax features into a semantic role labelling model. In contrast to common practice, which often tries to incorporate syntax via a TreeLSTM, they find that shortest dependency path and tree position features perform best. </li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-LuY.5171.pdf"><strong>Relation Structure-Aware Heterogeneous Information Network Embedding</strong></a>: A network embedding model that treats different relations differently: For affiliation relations (<em>"papers are published in conferences")</em> Euclidean distance is used, while for interaction relations (<em>"authors write papers")</em> a translation-based distance is used.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-GuoM.1484.pdf"><strong>Gaussian Transformer: a Lightweight Approach for Natural Language Inference</strong></a>: A Transformer with a Gaussian prior for the self-attention that encourages focusing on neighbouring tokens.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-LeeJ.6432.pdf"><strong>Gradient-based Inference for Networks with Output Constraints</strong></a>: A method to incorporate output constraints, e.g. matching number of brackets for syntactic parsing, agreement with parse spans for SRL, etc. into the model via gradient-based inference at test-time. The method is extensively evaluated and also performs well on out-of-domain data.</li><li><strong><a href="https://arxiv.org/abs/1811.00146">ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning</a>: </strong>A collection of 300k textual descriptions focusing on if-then relations with variables. Multi-task models that exploit the hierarchical structure of the data perform better.</li></ul><h1 id="transfer-learning">Transfer learning</h1><p>Papers on transfer learning ranged from multi-task learning and semi-supervised learning to sequential and zero-shot transfer:</p><ul><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-ChenLingzhen.6418.pdf"><strong>Transfer Learning for Sequence Labeling using Source Model and Target Data</strong></a>: Extension of fine-tuning techniques for NER for the case where the target task includes labels from the source domain (as well as new labels). 1) Output layer is extended with embeddings for new labels. 2) A BiLSTM takes the features of the source model as input and feeds its output to the target model. </li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-SanhV.7138.pdf"><strong>A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks</strong></a>: A hierarchical model that jointly learns coreference resolution, relation extraction, entity mention detection, and NER. It achieves state of the art on 3/4 tasks. (<em>Disclaimer: I'm a co-author of this paper.</em>)</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-SachanD.7236.pdf"><strong>Revisiting LSTM Networks for Semi-Supervised Text Classification via Mixed Objective Function</strong></a>: A combination of entropy minimization, adversarial and virtual adversarial training with a simple 1-layer BiLSTM achieves state-of-the-art results on multiple text classification datasets. </li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-RijhwaniS.6382.pdf"><strong>Zero-shot Neural Transfer for Cross-lingual Entity Linking</strong></a>: A cross-lingual entity linking model that trains a character-based entity similarity encoder on a bilingual lexicon of entities. Conceptually similar to <a href="https://arxiv.org/abs/1706.04902">cross-lingual word embedding models</a>. For languages that do not share the same script, words are transcribed to phonemes.</li><li><strong><a href="https://arxiv.org/abs/1808.10059">Zero-Shot Adaptive Transfer for Conversational Language Understanding</a>: </strong>A model that performs zero-shot slot tagging by embedding the slot description and fine-tuning a pretrained model on the target domain.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-SiddhantA.2024.pdf"><strong>Unsupervised Transfer learning for Spoken Language Understanding in Intelligent Agents</strong></a>: A more light-weight ELMo model that pretrains a shared BiLSTM layer for intent classification and entity tagging and fine-tunes it with ULMFiT techniques.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-RuderS.6318.pdf"><strong>Latent Multi-task Architecture Learning</strong></a>: A multi-task learning architecture that enables more flexible parameter sharing between tasks and generalizes existing transfer and multi-task learning architectures. (<em>Disclaimer: I'm a co-author of this paper.</em>)</li><li><strong><a href="https://arxiv.org/abs/1811.11456">GIRNet: Interleaved Multi-Task Recurrent State Sequence Models</a></strong>: A multi-task learning model that leverages the output from auxiliary models based on position-dependent gates. The model is applied to sentiment analysis and POS tagging of code-switched data and target-dependent sentiment analysis.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-ZhangLipeng.3275.pdf"><strong>A Generalized Language Model in Tensor Space</strong></a>: A higher-order language model that builds a representation based on the tensor product of word vectors. The model achieves strong results on PTB and WikiText.</li></ul><h1 id="word-embeddings">Word embeddings</h1><p>Naturally there were also a number of papers that provided new methods for learning word embeddings:</p><ul><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-LiuTianlin.5754.pdf"><strong>Unsupervised Post-processing of Word Vectors via Conceptor Negation</strong></a>: A post-processing method that uses conceptors (a linear transformation) to dampen directions where a word vector has high variances. Post-processed embeddings not only improve on word similarity, but also on dialogue state tracking.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-AbdallaM.6635.pdf"><strong>Enriching Word Embeddings with a Regressor Instead of Labeled Corpora</strong></a>: A method that enriches word embeddings during training with sentiment information based on a regressor trained on valence information from a sentiment lexicon. The enriched embeddings improve performance on sentiment and non-sentiment tasks.</li><li><strong><a href="https://arxiv.org/abs/1811.03866">Learning Semantic Representations for Novel Words: Leveraging Both Form and Context</a>: </strong>A model that learns representations for novel words both from the surface form and the context—in contrast to previous models that only leverage one of the sources.</li></ul><h1 id="miscellaneous">Miscellaneous</h1><p>Finally, here are some papers that I enjoyed that do not fit into any of the above categories:</p><ul><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-DalviF.5894.pdf"><strong>What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models</strong></a>: A supervised method to extract relevant neurons with regard to a task (by correlating neurons with the target property) and an unsupervised method to extract salient neurons with regard to the model (by correlating neurons across models). Techniques are evaluated on NMT and language modelling.</li><li><strong><a href="https://arxiv.org/abs/1811.12181">What Should I Learn First: Introducing LectureBank for NLP Education and Prerequisite Chain Learning</a></strong>: A dataset containing 1,352 NLP lecture files classified according to a taxonomy with 208 prerequisite relation topics. A model is trained to learn prerequisite relations to answer "what should one learn first".</li></ul><p><em>Cover image: AAAI-19 Opening Reception</em></p>
                </div>

            </section>
            
           <p align="center">
             <iframe src="https://nlpnewsletter.substack.com/embed" width="480" height="320" style="border:1px solid #EEE; background:white;" frameborder="0" scrolling="no"></iframe>
           </p>



        </article>

    </div>
</main>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = '/aaai-2019-highlights/';  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "ghost-5c57da473adfcb2a3ef289eb"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://EXAMPLE.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                            

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
                <article class="read-next-card">
                    <header class="read-next-card-header">
                        <h3><span>More in</span> <a href="../tag/events/index.html">events</a></h3>
                    </header>
                    <div class="read-next-card-content">
                        <ul>
                            <li>
                                <h4><a href="../acl2022/index.html">ACL 2022 Highlights</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2022-06-06">6 Jun 2022</time> –
                                        16 min read</p>
                                </div>
                            </li>
                            <li>
                                <h4><a href="../state-of-transfer-learning-in-nlp/index.html">The State of Transfer Learning in NLP</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2019-08-18">18 Aug 2019</time> –
                                        15 min read</p>
                                </div>
                            </li>
                            <li>
                                <h4><a href="../eurnlp/index.html">EurNLP</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2019-07-04">4 Jul 2019</time> –
                                        2 min read</p>
                                </div>
                            </li>
                        </ul>
                    </div>
                    <footer class="read-next-card-footer">
                        <a href="../tag/events/index.html">See all 12 posts
                            →</a>
                    </footer>
                </article>

                <article class="post-card post tag-transfer-learning tag-natural-language-processing tag-multi-task-learning tag-domain-adaptation tag-cross-lingual ">

    <a class="post-card-image-link" href="../thesis/index.html">
        <img class="post-card-image"
            srcset="../content/images/size/w300/2019/03/transfer_learning_taxonomy-1.png 300w,
                   ../content/images/size/w600/2019/03/transfer_learning_taxonomy-1.png 600w,
                  ../content/images/size/w1000/2019/03/transfer_learning_taxonomy-1.png 1000w,
                 ../content/images/size/w2000/2019/03/transfer_learning_taxonomy-1.png 2000w"
            sizes="(max-width: 1000px) 400px, 700px"
            src="../content/images/size/w600/2019/03/transfer_learning_taxonomy-1.png"
            alt="Neural Transfer Learning for Natural Language Processing (PhD thesis)"
        />
    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="../thesis/index.html">

            <header class="post-card-header">
                    <div class="post-card-primary-tag">transfer learning</div>
                <h2 class="post-card-title">Neural Transfer Learning for Natural Language Processing (PhD thesis)</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>This post discusses my PhD thesis Neural Transfer Learning for Natural Language Processing and some new material presented in it.</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Sebastian Ruder
                    </div>
            
                    <a href="../author/sebastian/index.html" class="static-avatar">
                        <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/sebastian/index.html">Sebastian Ruder</a></span>
                <span class="post-card-byline-date"><time datetime="2019-03-23">23 Mar 2019</time> <span class="bull">&bull;</span> 1 min read</span>
            </div>
        </footer>

    </div>

</article>

                <article class="post-card post tag-natural-language-processing tag-cross-lingual tag-events ">

    <a class="post-card-image-link" href="../4-biggest-open-problems-in-nlp/index.html">
        <img class="post-card-image"
            srcset="../content/images/size/w300/2019/01/narrativeqa_ghostbuster.png 300w,
                   ../content/images/size/w600/2019/01/narrativeqa_ghostbuster.png 600w,
                  ../content/images/size/w1000/2019/01/narrativeqa_ghostbuster.png 1000w,
                 ../content/images/size/w2000/2019/01/narrativeqa_ghostbuster.png 2000w"
            sizes="(max-width: 1000px) 400px, 700px"
            src="../content/images/size/w600/2019/01/narrativeqa_ghostbuster.png"
            alt="The 4 Biggest Open Problems in NLP"
        />
    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="../4-biggest-open-problems-in-nlp/index.html">

            <header class="post-card-header">
                    <div class="post-card-primary-tag">natural language processing</div>
                <h2 class="post-card-title">The 4 Biggest Open Problems in NLP</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>This is the second post based on the Frontiers of NLP session at the Deep Learning Indaba 2018. It discusses 4 major open problems in NLP.</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Sebastian Ruder
                    </div>
            
                    <a href="../author/sebastian/index.html" class="static-avatar">
                        <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/sebastian/index.html">Sebastian Ruder</a></span>
                <span class="post-card-byline-date"><time datetime="2019-01-15">15 Jan 2019</time> <span class="bull">&bull;</span> 10 min read</span>
            </div>
        </footer>

    </div>

</article>
        </div>
    </div>
</aside>




        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://ruder.io">Sebastian Ruder</a> &copy; 2022</section>
                <nav class="site-footer-nav">
                    <a href="https://ruder.io">Latest Posts</a>
                    
                    <a href="https://twitter.com/seb_ruder" target="_blank" rel="noopener">Twitter</a>
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>


    <script
        src="https://code.jquery.com/jquery-3.4.1.min.js"
        integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
        crossorigin="anonymous">
    </script>
    <script src="../assets/built/casper.js?v=96152eef5a"></script>

    <script>
        // Parse the URL parameter
        function getParameterByName(name, url) {
            if (!url) url = window.location.href;
            name = name.replace(/[\[\]]/g, "\\$&");
            var regex = new RegExp("[?&]" + name + "(=([^&#]*)|&|#|$)"),
                results = regex.exec(url);
            if (!results) return null;
            if (!results[2]) return '';
            return decodeURIComponent(results[2].replace(/\+/g, " "));
        }

        // Give the parameter a variable name
        var action = getParameterByName('action');

        $(document).ready(function () {
            if (action == 'subscribe') {
                $('body').addClass("subscribe-success");
            }

            $('.subscribe-success-message .subscribe-close').click(function () {
                $('.subscribe-success-message').addClass('close');
            });

            // Reset form on opening subscrion overlay
            $('.subscribe-button').click(function() {
                $('.subscribe-overlay form').removeClass();
                $('.subscribe-email').val('');
            });
        });
    </script>

    <script>
    $(document).ready(function () {
        // FitVids - start
        var $postContent = $(".post-full-content");
        $postContent.fitVids();
        // FitVids - end

        // Replace nav with title on scroll - start
        Casper.stickyNavTitle({
            navSelector: '.site-nav-main',
            titleSelector: '.post-full-title',
            activeClass: 'nav-post-title-active'
        });
        // Replace nav with title on scroll - end

        // Hover on avatar
        var hoverTimeout;
        $('.author-list-item').hover(function () {
            var $this = $(this);

            clearTimeout(hoverTimeout);

            $('.author-card').removeClass('hovered');
            $(this).children('.author-card').addClass('hovered');

        }, function () {
            var $this = $(this);

            hoverTimeout = setTimeout(function () {
                $this.children('.author-card').removeClass('hovered');
            }, 800);
        });
    });
</script>


    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/zepto/1.1.6/zepto.min.js"></script>
<script>jQuery = Zepto</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism.min.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/plugins/line-numbers/prism-line-numbers.min.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism-coy.min.css" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/components/prism-python.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-60512592-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-60512592-1');
</script>

</body>
</html>

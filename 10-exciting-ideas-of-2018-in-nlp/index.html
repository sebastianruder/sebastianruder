<!DOCTYPE html>
<html lang="en">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>10 Exciting Ideas of 2018 in NLP</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="stylesheet" type="text/css" href="../assets/built/screen.css?v=66ae98f3dd" />

    <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon" />
    <link rel="canonical" href="https://ruder.io/10-exciting-ideas-of-2018-in-nlp/" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <link rel="amphtml" href="https://ruder.io/10-exciting-ideas-of-2018-in-nlp/amp/" />
    
    <meta property="og:site_name" content="Sebastian Ruder" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="10 Exciting Ideas of 2018 in NLP" />
    <meta property="og:description" content="This post gathers 10 ideas that I found exciting and impactful this year‚Äîand that we&#x27;ll likely see more of in the future. For each idea, it highlights 1-2 papers that execute them well." />
    <meta property="og:url" content="https://ruder.io/10-exciting-ideas-of-2018-in-nlp/" />
    <meta property="og:image" content="https://ruder.io/content/images/2018/12/syntactic_scaffold-1.png" />
    <meta property="article:published_time" content="2018-12-19T19:28:46.000Z" />
    <meta property="article:modified_time" content="2018-12-19T20:10:43.000Z" />
    <meta property="article:tag" content="transfer learning" />
    <meta property="article:tag" content="multi-task learning" />
    <meta property="article:tag" content="meta-learning" />
    <meta property="article:tag" content="natural language processing" />
    <meta property="article:tag" content="semi-supervised learning" />
    <meta property="article:tag" content="language models" />
    <meta property="article:tag" content="cross-lingual" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="10 Exciting Ideas of 2018 in NLP" />
    <meta name="twitter:description" content="This post gathers 10 ideas that I found exciting and impactful this year‚Äîand that we&#x27;ll likely see more of in the future. For each idea, it highlights 1-2 papers that execute them well." />
    <meta name="twitter:url" content="https://ruder.io/10-exciting-ideas-of-2018-in-nlp/" />
    <meta name="twitter:image" content="https://ruder.io/content/images/2018/12/syntactic_scaffold-1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Sebastian Ruder" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="transfer learning, multi-task learning, meta-learning, natural language processing, semi-supervised learning, language models, cross-lingual" />
    <meta name="twitter:site" content="@seb_ruder" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="729" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Sebastian Ruder",
        "url": "https://ruder.io/",
        "logo": {
            "@type": "ImageObject",
            "url": {
                "@type": "ImageObject",
                "url": "https://ruder.io/favicon.ico",
                "width": 48,
                "height": 48
            }
        }
    },
    "author": {
        "@type": "Person",
        "name": "Sebastian Ruder",
        "image": {
            "@type": "ImageObject",
            "url": "https://ruder.io/content/images/2019/02/new_profile_photo_square-1.jpg",
            "width": 2000,
            "height": 2000
        },
        "url": "https://ruder.io/author/sebastian/",
        "sameAs": []
    },
    "headline": "10 Exciting Ideas of 2018 in NLP",
    "url": "https://ruder.io/10-exciting-ideas-of-2018-in-nlp/",
    "datePublished": "2018-12-19T19:28:46.000Z",
    "dateModified": "2018-12-19T20:10:43.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://ruder.io/content/images/2018/12/syntactic_scaffold-1.png",
        "width": 2000,
        "height": 729
    },
    "keywords": "transfer learning, multi-task learning, meta-learning, natural language processing, semi-supervised learning, language models, cross-lingual",
    "description": "This post gathers 10 ideas that I found exciting and impactful this year‚Äîand that we&#x27;ll likely see more of in the future. For each idea, it highlights 1-2 papers that execute them well.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://ruder.io/"
    }
}
    </script>

    <meta name="generator" content="Ghost 3.11" />
    <link rel="alternate" type="application/rss+xml" title="Sebastian Ruder" href="https://ruder.io/rss/" />
    <script>
var profile_title = 'Sebastian Ruder';
</script>
<script>
var disqus_shortname = 'sebastianruder';
</script>
<script>
var profile_resume ='NLP PhD student';
</script>
<script>
var ga_id = 'UA-60512592-1';
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [["$", "$"], ["\\(", "\\)"]],
        processEscapes: true
    }
});
</script>

</head>
<body class="post-template tag-transfer-learning tag-multi-task-learning tag-meta-learning tag-natural-language-processing tag-semi-supervised-learning tag-language-models tag-cross-lingual">

    <div class="site-wrapper">

        

<header class="site-header">
    <div class="outer site-nav-main">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left-wrapper">
        <div class="site-nav-left">
                <a class="site-nav-logo" href="https://ruder.io">Sebastian Ruder</a>
            <div class="site-nav-content">
                    <ul class="nav" role="menu">
    <li class="nav-about" role="menuitem"><a href="https://ruder.io/about/">About</a></li>
    <li class="nav-tags" role="menuitem"><a href="https://ruder.io/tags/">Tags</a></li>
    <li class="nav-papers" role="menuitem"><a href="https://ruder.io/publications/">Papers</a></li>
    <li class="nav-talks" role="menuitem"><a href="https://ruder.io/talks/">Talks</a></li>
    <li class="nav-news" role="menuitem"><a href="https://ruder.io/news/">News</a></li>
    <li class="nav-faq" role="menuitem"><a href="https://ruder.io/faq/">FAQ</a></li>
    <li class="nav-sign-up-for-nlp-news" role="menuitem"><a href="https://ruder.io/nlp-news/">Sign up for NLP News</a></li>
    <li class="nav-nlp-progress" role="menuitem"><a href="https://nlpprogress.com/">NLP Progress</a></li>
    <li class="nav-media" role="menuitem"><a href="https://ruder.io/media/">Media</a></li>
    <li class="nav-contact" role="menuitem"><a href="https://ruder.io/contact/">Contact</a></li>
</ul>

                    <span class="nav-post-title dash">10 Exciting Ideas of 2018 in NLP</span>
            </div>
        </div>
    </div>
    <div class="site-nav-right">
            <div class="social-links">
                    <a class="social-link social-link-tw" href="https://twitter.com/seb_ruder" title="Twitter" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
</a>
            </div>
                <a class="rss-button" href="https://ruder.io/rss/index.rss" title="RSS" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
</a>

    </div>
</nav>
    </div>
</div></header>


<main id="site-main" class="site-main outer">
    <div class="inner">

        <article class="post-full post tag-transfer-learning tag-multi-task-learning tag-meta-learning tag-natural-language-processing tag-semi-supervised-learning tag-language-models tag-cross-lingual ">

            <header class="post-full-header">

                <section class="post-full-tags">
                    <a href="../tag/transfer-learning/index.html">transfer learning</a>
                </section>

                <h1 class="post-full-title">10 Exciting Ideas of 2018 in NLP</h1>

                <p class="post-full-custom-excerpt">This post gathers 10 ideas that I found exciting and impactful this year‚Äîand that we&#x27;ll likely see more of in the future. For each idea, it highlights 1-2 papers that execute them well.</p>

                <div class="post-full-byline">

                    <section class="post-full-byline-content">

                        <ul class="author-list">
                            <li class="author-list-item">

                                <div class="author-card">
                                    <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                                    <div class="author-info">
                                        <h2>Sebastian Ruder</h2>
                                        <p>Read <a href="../author/sebastian/index.html">more posts</a> by this author.</p>
                                    </div>
                                </div>

                                <a href="../author/sebastian/index.html" class="author-avatar">
                                    <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                                </a>

                            </li>
                        </ul>

                        <section class="post-full-byline-meta">
                            <h4 class="author-name"><a href="../author/sebastian/index.html">Sebastian Ruder</a></h4>
                            <div class="byline-meta-content">
                                <time class="byline-meta-date" datetime="2018-12-19">19 Dec 2018</time>
                                <span class="byline-reading-time"><span class="bull">&bull;</span> 8 min read</span>
                            </div>
                        </section>

                    </section>


                </div>
            </header>

            <figure class="post-full-image">
                <img
                    srcset="../content/images/size/w300/2018/12/syntactic_scaffold-1.png 300w,
                           ../content/images/size/w600/2018/12/syntactic_scaffold-1.png 600w,
                          ../content/images/size/w1000/2018/12/syntactic_scaffold-1.png 1000w,
                         ../content/images/size/w2000/2018/12/syntactic_scaffold-1.png 2000w"
                    sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px"
                    src="../content/images/size/w2000/2018/12/syntactic_scaffold-1.png"
                    alt="10 Exciting Ideas of 2018 in NLP"
                />
            </figure>

            <section class="post-full-content">
                <div class="post-content">
                    <p>This post gathers 10 ideas that I found exciting and impactful this year‚Äîand that we'll likely see more of in the future.</p><p>For each idea, I will highlight 1-2 papers that execute them well. I tried to keep the list succinct, so apologies if I did not cover all relevant work. The list is necessarily subjective and covers ideas mainly related to transfer learning and generalization. Most of these (with some exceptions) are not trends (but I suspect that some might become more 'trendy' in 2019). Finally, I would love to read about your highlights in the comments or see highlights posts about other areas.</p><h2 id="1-unsupervised-mt">1) Unsupervised MT</h2><p>There were <a href="https://arxiv.org/abs/1711.00043">two</a> <a href="https://arxiv.org/abs/1710.11041">unsupervised</a> MT papers at ICLR 2018. They were <em>surprising</em> in that they worked at all, but results were still low compared to supervised systems. At EMNLP 2018, unsupervised MT hit its stride with <a href="https://arxiv.org/abs/1804.07755">two</a> <a href="https://arxiv.org/abs/1809.01272">papers</a> from the same two groups that significantly improve upon their previous methods. My highlight:</p><ul><li><a href="https://arxiv.org/abs/1804.07755"><strong>Phrase-Based &amp; Neural Unsupervised Machine Translation</strong> (EMNLP 2018)</a>: ¬†The paper does a nice job in distilling the three key requirements for unsupervised MT: a good initialization, language modelling, and modelling the inverse task (via back-translation). All three are also beneficial in other unsupervised scenarios, as we will see below. Modelling the inverse task enforces cyclical consistency, which has been employed in different approaches‚Äîmost prominently in <a href="https://arxiv.org/abs/1703.10593">CycleGAN</a>. The paper performs extensive experiments and evaluates even on two low-resource language pairs, English-Urdu and English-Romanian. We will hopefully see more work on low-resource languages in the future.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2018/12/phrase_based_and_neural_unsupervised_mt.png" class="kg-image"><figcaption>Toy illustration of the three principles of unsupervised MT. A) Two monolingual datasets. B) Initialization. C) Language modelling. D) Back-translation <a href="https://arxiv.org/abs/1804.07755">(Lample et al., 2018)</a>.</figcaption></figure><h2 id="2-pretrained-language-models">2) Pretrained language models</h2><p>Using pretrained language models is probably the <a href="http://ruder.io/nlp-imagenet/">most significant NLP trend</a> this year, so I won't spend much time on it here. There have been a slew of memorable approaches: <a href="https://arxiv.org/abs/1802.05365">ELMo</a>, <a href="https://arxiv.org/abs/1801.06146">ULMFiT</a>, <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">OpenAI Transformer</a>, and <a href="https://arxiv.org/abs/1810.04805">BERT</a>. My highlight:</p><ul><li><a href="https://arxiv.org/abs/1802.05365"><strong>Deep contextualized word representations</strong> (NAACL-HLT 2018)</a>: The paper that introduced ELMo has been much lauded. Besides the impressive empirical results, where it shines is the careful analysis section that teases out the impact of various factors and analyses the information captured in the representations. The word sense disambiguation (WSD) analysis by itself (below on the left) is well executed. Both demonstrate that a LM on its own provides WSD and POS tagging performance close to the state-of-the-art.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2018/12/wsd_and_pos_tagging_results.png" class="kg-image"><figcaption>Word sense disambiguation (left) and POS tagging (right) results of first and second layer bidirectional language model compared to baselines <a href="https://arxiv.org/abs/1802.05365">(Peters et al., 2018)</a>.</figcaption></figure><h2 id="3-common-sense-inference-datasets">3) Common sense inference datasets</h2><p>Incorporating common sense into our models is one of the most important directions moving forward. However, creating good datasets is not easy and even popular ones <a href="http://aclweb.org/anthology/N18-2017">show</a> <a href="http://www.aclweb.org/anthology/S18-2023">large</a> biases. This year, there have been some well-executed datasets that seek to teach models some common sense such as <a href="https://arxiv.org/abs/1805.06939">Event2Mind</a> and <a href="https://arxiv.org/abs/1808.05326">SWAG</a>, both from the University of Washington. SWAG was solved <a href="https://twitter.com/seb_ruder/status/1050727451138150400">unexpectedly quickly</a>. My highlight:</p><ul><li><strong><a href="http://visualcommonsense.com/">Visual Commonsense Reasoning</a></strong><a href="http://visualcommonsense.com/"> (arXiv 2018)</a>: This is the first visual QA dataset that includes a rationale (an explantation) with each answer. In addition, questions require complex reasoning. The creators go to great lengths to address possible bias by ensuring that every answer's prior probability of being correct is 25% (every answer appears 4 times in the entire dataset, 3 times as an incorrect answer and 1 time as the correct answer); this requires solving a constrained optimization problem using models that compute relevance and similarity. Hopefully preventing possible bias will become a common component when creating datasets. Finally, just look at the gorgeous presentation of the data üëá.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2018/12/visual_commonsense_reasoning.png" class="kg-image"><figcaption>VCR: Given an image, a list of regions, and a question, a model must answer the question and provide a rationale explaining why its answer is right <a href="https://arxiv.org/abs/1811.10830">(Zellers et al., 2018)</a>.</figcaption></figure><h2 id="4-meta-learning">4) Meta-learning</h2><p>Meta-learning has seen much use in few-shot learning, reinforcement learning, and robotics‚Äîthe most prominent example: <a href="https://arxiv.org/abs/1703.03400">model-agnostic meta-learning (MAML)</a>‚Äîbut successful applications in NLP have been rare. Meta-learning is most useful for problems with a limited number of training examples. My highlight:</p><ul><li><a href="http://aclweb.org/anthology/D18-1398"><strong>Meta-Learning for Low-Resource Neural Machine Translation</strong> (EMNLP 2018)</a>: The authors use MAML to learn a good initialization for translation, treating each language pair as a separate meta-task. Adapting to low-resource languages is probably the most useful setting for meta-learning in NLP. In particular, combining multilingual transfer learning (such as <a href="https://github.com/google-research/bert/blob/master/multilingual.md">multilingual BERT</a>), unsupervised learning, and meta-learning is a promising direction.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2018/12/meta-learning_vs_transfer_learning.png" class="kg-image"><figcaption>The difference between transfer learning multilingual transfer learning, and meta-learning. Solid lines: learning of the initialization. Dashed lines: Path of fine-tuning <a href="http://aclweb.org/anthology/D18-1398">(Gu et al., 2018)</a>.</figcaption></figure><h2 id="5-robust-unsupervised-methods">5) Robust unsupervised methods</h2><p>This year, <a href="http://aclweb.org/anthology/P18-1072">we</a> <a href="http://aclweb.org/anthology/D18-1056">and</a> others have observed that unsupervised cross-lingual word embedding methods break down when languages are dissimilar. This is a common phenomenon in transfer learning where a discrepancy between source and target settings (e.g. domains in <a href="https://www.cs.jhu.edu/~mdredze/publications/sentiment_acl07.pdf">domain adaptation</a>, tasks in <a href="https://arxiv.org/abs/1706.08840">continual learning</a> and <a href="http://www.aclweb.org/anthology/E17-1005">multi-task learning</a>) leads to deterioration or failure of the model. Making models more robust to such changes is thus important. My highlight:</p><ul><li><a href="http://www.aclweb.org/anthology/P18-1073"><strong>A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings</strong> (ACL 2018)</a>: Instead of meta-learning an initialization, this paper uses their understanding of the problem to craft a better initialization. In particular, they pair words in both languages that have a similar distribution of words they are similar to. This is a great example of using domain expertise and insights from an analysis to make a model more robust.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2018/12/similarity_distribution.png" class="kg-image"><figcaption>The similarity distributions of three words. Equivalent translations ('two' and 'due') have more similar distributions than non-related words ('two' and 'cane'‚Äîmeaning 'dog'; <a href="http://www.aclweb.org/anthology/P18-1073">Artexte et al., 2018</a>).</figcaption></figure><h2 id="6-understanding-representations">6) Understanding representations</h2><p>There have been a lot of efforts in better understanding representations. In particular, <a href="https://arxiv.org/abs/1608.04207">'diagnostic classifiers'</a> (tasks that aim to measure if learned representations can predict certain attributes) have become <a href="http://arxiv.org/abs/1805.01070">quite common</a>. My highlight:</p><ul><li><a href="http://aclweb.org/anthology/D18-1179"><strong>Dissecting Contextual Word Embeddings: Architecture and Representation</strong> (EMNLP 2018)</a>: This paper does a great job of better understanding pretrained language model representations. They extensively study learned word and span representations on carefully designed unsupervised and supervised tasks. The resulting finding: Pretrained representations learn tasks related to low-level morphological and syntactic tasks at lower layers and longer range semantics at higher layers. To me this really shows that pretrained language models indeed capture similar properties as <a href="https://thegradient.pub/nlp-imagenet/">computer vision models pretrained on ImageNet</a>.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2018/12/bilm_transformer_information.png" class="kg-image"><figcaption>Per-layer performance of BiLSTM and Transformer pretrained representations on (from left to right) POS tagging, constituency parsing, and unsupervised coreference resolution <a href="http://aclweb.org/anthology/D18-1179">(Peters et al., 2018)</a>.</figcaption></figure><h2 id="7-clever-auxiliary-tasks">7) Clever auxiliary tasks</h2><p>In many settings, we have seen an increasing usage of multi-task learning with carefully chosen auxiliary tasks. For a good auxiliary task, data must be easily accessible. One of the most prominent examples is <a href="https://arxiv.org/abs/1810.04805">BERT</a>, which uses next-sentence prediction (that has been used in <a href="https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf">Skip-thoughts</a> and more recently in <a href="https://arxiv.org/pdf/1803.02893.pdf">Quick-thoughts</a>) to great effect. My highlights: </p><ul><li><a href="http://aclweb.org/anthology/D18-1412"><strong>Syntactic Scaffolds for Semantic Structures</strong> (EMNLP 2018)</a>: This paper proposes an auxiliary task that pretrains span representations by predicting for each span the corresponding syntactic constituent type. Despite being conceptually simple, the auxiliary task leads to large improvements on span-level prediction tasks such as semantic role labelling and coreference resolution. This papers shows that specialised representations learned at the level required by the target task (here: spans) are immensely beneficial.</li><li><strong><a href="https://arxiv.org/abs/1810.08854">pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence Inference</a></strong><a href="https://arxiv.org/abs/1810.08854"> (arXiv 2018)</a>: In a similar vein, this paper pretrains <em>word pair representations</em> by maximizing the pointwise mutual information of pairs of words with their context. This encourages the model to learn more meaningful representations of word pairs than with more general objectives, such as language modelling. The pretrained representations are effective in tasks such as SQuAD and MultiNLI that require cross-sentence inference. We can expect to see more pretraining tasks that capture properties particularly suited to certain downstream tasks and are complementary to more general-purpose tasks like language modelling.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2018/12/syntactic_scaffold.png" class="kg-image"><figcaption>Syntactic, PropBank and coreference annotations from OntoNotes. PropBank SRL arguments and coreference mentions are annotated on top of syntactic constituents. Almost every argument is related to a syntactic constituent <a href="http://aclweb.org/anthology/D18-1412">(Swayamdipta et al., 2018)</a>.</figcaption></figure><h2 id="8-combining-semi-supervised-learning-with-transfer-learning">8) Combining semi-supervised learning with transfer learning</h2><p>With the recent advances in transfer learning, we should not forget more explicit ways of using target task-specific data. In fact, pretrained representations are complementary with many forms of semi-supervised learning. We have explored <a href="http://aclweb.org/anthology/P18-1096">self-labelling approaches</a>, a particular category of semi-supervised learning. My highlight: </p><ul><li><a href="http://aclweb.org/anthology/D18-1217"><strong>Semi-Supervised Sequence Modeling with Cross-View Training</strong> (EMNLP 2018)</a>: This paper shows that a conceptually very simple idea, making sure that the predictions on different views of the input agree with the prediction of the main model, can lead to gains on a diverse set of tasks. The idea is similar to word dropout but allows leveraging unlabelled data to make the model more robust. Compared to other self-ensembling models such as <a href="https://papers.nips.cc/paper/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.pdf">mean teacher</a>, it is specifically designed for particular NLP tasks. With much work on <em>implicit</em> semi-supervised learning, we will hopefully see more work that explicitly tries to model the target predictions going forward.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2018/12/cross-view_training-1.png" class="kg-image"><figcaption>Inputs seen by auxiliary prediction modules: Auxiliary 1: <em>They traveled to</em> __________________. Auxiliary 2: <em>They traveled to</em> <strong>Washington</strong> _______. Auxiliary 3: _____________ <strong>Washington</strong> <em>by plane</em>. Auxiliary 4: ________________________ <em>by plane</em> <a href="http://aclweb.org/anthology/D18-1217">(Clark et al., 2018)</a>.</figcaption></figure><h2 id="9-qa-and-reasoning-with-large-documents">9) QA and reasoning with large documents</h2><p>There have been a lot of developments in question answering (QA), with an <a href="https://arxiv.org/abs/1809.09600">array</a> <a href="https://stanfordnlp.github.io/coqa/">of</a> <a href="http://quac.ai/">new</a> <a href="https://arxiv.org/abs/1806.03822">QA</a> <a href="http://qangaroo.cs.ucl.ac.uk/">datasets</a>. Besides conversational QA and performing multi-step reasoning, the most challenging aspect of QA is to synthesize narratives and large bodies of information. My highlight: </p><ul><li><a href="http://aclweb.org/anthology/Q18-1023"><strong>The NarrativeQA Reading Comprehension Challenge</strong> (TACL 2018)</a>: This paper proposes a challenging new QA dataset based on answering questions about entire movie scripts and books. While this task is still out of reach for current methods, models are provided the option of using a summary (rather than the entire book) as context, of selecting the answer (rather than generate it), and of using the output from an IR model. These variants make the task more feasible and enable models to gradually scale up to the full setting. We need more datasets like this that present ambitious problems, but still manage to make them accessible.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2018/12/narrative_qa.png" class="kg-image"><figcaption>Comparison of QA datasets <a href="http://aclweb.org/anthology/Q18-1023">(Koƒçisk√Ω et al., 2018)</a>.&nbsp;</figcaption></figure><h2 id="10-inductive-bias">10) Inductive bias</h2><p>Inductive biases such as convolutions in a CNN, regularization, dropout, and other mechanisms are core parts of neural network models that act as a regularizer and make models more sample-efficient. However, coming up with a broadly useful inductive bias and incorporating it into a model is challenging. My highlights:</p><ul><li><a href="http://aclweb.org/anthology/K18-1030"><strong>Sequence classification with human attention</strong> (CoNLL 2018)</a>: This paper proposes to use human attention from eye-tracking corpora to regularize attention in RNNs. Given that many current models such as Transformers use attention, finding ways to train it more efficiently is an important direction. It is also great to see another example that human language learning can help improve our computational models. </li><li><a href="http://aclweb.org/anthology/D18-1548"><strong>Linguistically-Informed Self-Attention for Semantic Role Labeling</strong> (EMNLP 2018)</a>: This paper has a lot to like: a Transformer trained jointly on both syntactic and semantic tasks; the ability to inject high-quality parses at test time; and out-of-domain evaluation. It also regularizes the Transformer's multi-head attention to be more sensitive to syntax by training one attention head to attend to the syntactic parents of each token. We will likely see more examples of Transformer attention heads used as auxiliary predictors focusing on particular aspects of the input.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2018/12/out-of-domain_srl_performance.png" class="kg-image"><figcaption>10 years of PropBank semantic role labeling. Comparison of Linguistically-Informed Self-Attention (LISA) with other methods on out-of-domain data <a href="https://people.cs.umass.edu/~strubell/">(Strubell et al., 2018)</a>.</figcaption></figure>
                </div>
            </section>



        </article>

    </div>
</main>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = '/10-exciting-ideas-of-2018-in-nlp/';  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "ghost-5c10d6561020694dd13c911c"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://EXAMPLE.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                            

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
                <article class="read-next-card">
                    <header class="read-next-card-header">
                        <h3><span>More in</span> <a href="../tag/transfer-learning/index.html">transfer learning</a></h3>
                    </header>
                    <div class="read-next-card-content">
                        <ul>
                            <li>
                                <h4><a href="../research-highlights-2020/index.html">ML and NLP Research Highlights of 2020</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2021-01-19">19 Jan 2021</time> ‚Äì
                                        15 min read</p>
                                </div>
                            </li>
                            <li>
                                <h4><a href="../research-highlights-2019/index.html">10 ML &amp; NLP Research Highlights of 2019</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2020-01-06">6 Jan 2020</time> ‚Äì
                                        12 min read</p>
                                </div>
                            </li>
                            <li>
                                <h4><a href="../unsupervised-cross-lingual-learning/index.html">Unsupervised Cross-lingual Representation Learning</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2019-10-26">26 Oct 2019</time> ‚Äì
                                        20 min read</p>
                                </div>
                            </li>
                        </ul>
                    </div>
                    <footer class="read-next-card-footer">
                        <a href="../tag/transfer-learning/index.html">See all 17 posts
                            ‚Üí</a>
                    </footer>
                </article>

                <article class="post-card post tag-natural-language-processing tag-cross-lingual tag-events ">

    <a class="post-card-image-link" href="../4-biggest-open-problems-in-nlp/index.html">
        <img class="post-card-image"
            srcset="../content/images/size/w300/2019/01/narrativeqa_ghostbuster.png 300w,
                   ../content/images/size/w600/2019/01/narrativeqa_ghostbuster.png 600w,
                  ../content/images/size/w1000/2019/01/narrativeqa_ghostbuster.png 1000w,
                 ../content/images/size/w2000/2019/01/narrativeqa_ghostbuster.png 2000w"
            sizes="(max-width: 1000px) 400px, 700px"
            src="../content/images/size/w600/2019/01/narrativeqa_ghostbuster.png"
            alt="The 4 Biggest Open Problems in NLP"
        />
    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="../4-biggest-open-problems-in-nlp/index.html">

            <header class="post-card-header">
                    <div class="post-card-primary-tag">natural language processing</div>
                <h2 class="post-card-title">The 4 Biggest Open Problems in NLP</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>This is the second post based on the Frontiers of NLP session at the Deep Learning Indaba 2018. It discusses 4 major open problems in NLP.</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Sebastian Ruder
                    </div>
            
                    <a href="../author/sebastian/index.html" class="static-avatar">
                        <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/sebastian/index.html">Sebastian Ruder</a></span>
                <span class="post-card-byline-date"><time datetime="2019-01-15">15 Jan 2019</time> <span class="bull">&bull;</span> 10 min read</span>
            </div>
        </footer>

    </div>

</article>

                <article class="post-card post tag-events tag-transfer-learning tag-cross-lingual tag-word-embeddings tag-language-models tag-natural-language-processing ">

    <a class="post-card-image-link" href="../emnlp-2018-highlights/index.html">
        <img class="post-card-image"
            srcset="../content/images/size/w300/2018/11/emnlp_conference_garden_view.jpg 300w,
                   ../content/images/size/w600/2018/11/emnlp_conference_garden_view.jpgg 600w,
                  ../content/images/size/w1000/2018/11/emnlp_conference_garden_view.jpg 1000w,
                 ../content/images/size/w2000/2018/11/emnlp_conference_garden_view.jpg 2000w"
            sizes="(max-width: 1000px) 400px, 700px"
            src="../content/images/size/w600/2018/11/emnlp_conference_garden_view.jpg"
            alt="EMNLP 2018 Highlights: Inductive bias, cross-lingual learning, and more"
        />
    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="../emnlp-2018-highlights/index.html">

            <header class="post-card-header">
                    <div class="post-card-primary-tag">events</div>
                <h2 class="post-card-title">EMNLP 2018 Highlights: Inductive bias, cross-lingual learning, and more</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>This post discusses highlights of EMNLP 2018. It focuses on talks and papers dealing with inductive bias, cross-lingual learning, word embeddings, latent variable models, language models, and datasets.</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Sebastian Ruder
                    </div>
            
                    <a href="../author/sebastian/index.html" class="static-avatar">
                        <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/sebastian/index.html">Sebastian Ruder</a></span>
                <span class="post-card-byline-date"><time datetime="2018-11-06">6 Nov 2018</time> <span class="bull">&bull;</span> 11 min read</span>
            </div>
        </footer>

    </div>

</article>
        </div>
    </div>
</aside>




        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://ruder.io">Sebastian Ruder</a> &copy; 2021</section>
                <nav class="site-footer-nav">
                    <a href="https://ruder.io">Latest Posts</a>
                    
                    <a href="https://twitter.com/seb_ruder" target="_blank" rel="noopener">Twitter</a>
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>


    <script
        src="https://code.jquery.com/jquery-3.4.1.min.js"
        integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
        crossorigin="anonymous">
    </script>
    <script src="../assets/built/casper.js?v=66ae98f3dd"></script>

    <script>
        // Parse the URL parameter
        function getParameterByName(name, url) {
            if (!url) url = window.location.href;
            name = name.replace(/[\[\]]/g, "\\$&");
            var regex = new RegExp("[?&]" + name + "(=([^&#]*)|&|#|$)"),
                results = regex.exec(url);
            if (!results) return null;
            if (!results[2]) return '';
            return decodeURIComponent(results[2].replace(/\+/g, " "));
        }

        // Give the parameter a variable name
        var action = getParameterByName('action');

        $(document).ready(function () {
            if (action == 'subscribe') {
                $('body').addClass("subscribe-success");
            }

            $('.subscribe-success-message .subscribe-close').click(function () {
                $('.subscribe-success-message').addClass('close');
            });

            // Reset form on opening subscrion overlay
            $('.subscribe-button').click(function() {
                $('.subscribe-overlay form').removeClass();
                $('.subscribe-email').val('');
            });
        });
    </script>

    <script>
    $(document).ready(function () {
        // FitVids - start
        var $postContent = $(".post-full-content");
        $postContent.fitVids();
        // FitVids - end

        // Replace nav with title on scroll - start
        Casper.stickyNavTitle({
            navSelector: '.site-nav-main',
            titleSelector: '.post-full-title',
            activeClass: 'nav-post-title-active'
        });
        // Replace nav with title on scroll - end

        // Hover on avatar
        var hoverTimeout;
        $('.author-list-item').hover(function () {
            var $this = $(this);

            clearTimeout(hoverTimeout);

            $('.author-card').removeClass('hovered');
            $(this).children('.author-card').addClass('hovered');

        }, function () {
            var $this = $(this);

            hoverTimeout = setTimeout(function () {
                $this.children('.author-card').removeClass('hovered');
            }, 800);
        });
    });
</script>


    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/zepto/1.1.6/zepto.min.js"></script>
<script>jQuery = Zepto</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism.min.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/plugins/line-numbers/prism-line-numbers.min.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism-coy.min.css" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/components/prism-python.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-60512592-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-60512592-1');
</script>

</body>
</html>

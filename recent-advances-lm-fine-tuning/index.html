<!DOCTYPE html>
<html lang="en">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>Recent Advances in Language Model Fine-tuning</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="stylesheet" type="text/css" href="../assets/built/screen.css?v=b377148439" />

    <meta name="description" content="This article provides an overview of recent methods to fine-tune large pre-trained language models." />
    <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon" />
    <link rel="canonical" href="https://ruder.io/recent-advances-lm-fine-tuning/" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <link rel="amphtml" href="https://ruder.io/recent-advances-lm-fine-tuning/amp/" />
    
    <meta property="og:site_name" content="Sebastian Ruder" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Recent Advances in Language Model Fine-tuning" />
    <meta property="og:description" content="This article provides an overview of recent methods to fine-tune large pre-trained language models." />
    <meta property="og:url" content="https://ruder.io/recent-advances-lm-fine-tuning/" />
    <meta property="og:image" content="https://ruder.io/content/images/2021/02/fine-tuning_methods.png" />
    <meta property="article:published_time" content="2021-02-24T09:00:00.000Z" />
    <meta property="article:modified_time" content="2021-02-24T12:01:05.000Z" />
    <meta property="article:tag" content="language models" />
    <meta property="article:tag" content="natural language processing" />
    <meta property="article:tag" content="transfer learning" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Recent Advances in Language Model Fine-tuning" />
    <meta name="twitter:description" content="This post provides an overview of recent methods to fine-tune large pre-trained language models." />
    <meta name="twitter:url" content="https://ruder.io/recent-advances-lm-fine-tuning/" />
    <meta name="twitter:image" content="https://ruder.io/content/images/2021/02/fine-tuning_methods.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Sebastian Ruder" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="language models, natural language processing, transfer learning" />
    <meta name="twitter:site" content="@seb_ruder" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="671" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Sebastian Ruder",
        "url": "https://ruder.io/",
        "logo": {
            "@type": "ImageObject",
            "url": {
                "@type": "ImageObject",
                "url": "https://ruder.io/favicon.ico",
                "width": 48,
                "height": 48
            }
        }
    },
    "author": {
        "@type": "Person",
        "name": "Sebastian Ruder",
        "image": {
            "@type": "ImageObject",
            "url": "https://ruder.io/content/images/2019/02/new_profile_photo_square-1.jpg",
            "width": 2000,
            "height": 2000
        },
        "url": "https://ruder.io/author/sebastian/",
        "sameAs": []
    },
    "headline": "Recent Advances in Language Model Fine-tuning",
    "url": "https://ruder.io/recent-advances-lm-fine-tuning/",
    "datePublished": "2021-02-24T09:00:00.000Z",
    "dateModified": "2021-02-24T12:01:05.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://ruder.io/content/images/2021/02/fine-tuning_methods.png",
        "width": 2000,
        "height": 671
    },
    "keywords": "language models, natural language processing, transfer learning",
    "description": "This article provides an overview of recent methods to fine-tune large pre-trained language models.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://ruder.io/"
    }
}
    </script>

    <meta name="generator" content="Ghost 3.11" />
    <link rel="alternate" type="application/rss+xml" title="Sebastian Ruder" href="https://ruder.io/rss/" />
    <script>
var profile_title = 'Sebastian Ruder';
</script>
<script>
var disqus_shortname = 'sebastianruder';
</script>
<script>
var profile_resume ='NLP PhD student';
</script>
<script>
var ga_id = 'UA-60512592-1';
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [["$", "$"], ["\\(", "\\)"]],
        processEscapes: true
    }
});
</script>

</head>
<body class="post-template tag-language-models tag-natural-language-processing tag-transfer-learning">

    <div class="site-wrapper">

        

<header class="site-header">
    <div class="outer site-nav-main">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left-wrapper">
        <div class="site-nav-left">
                <a class="site-nav-logo" href="https://ruder.io">Sebastian Ruder</a>
            <div class="site-nav-content">
                    <ul class="nav" role="menu">
    <li class="nav-about" role="menuitem"><a href="https://ruder.io/about/">About</a></li>
    <li class="nav-tags" role="menuitem"><a href="https://ruder.io/tags/">Tags</a></li>
    <li class="nav-papers" role="menuitem"><a href="https://ruder.io/publications/">Papers</a></li>
    <li class="nav-talks" role="menuitem"><a href="https://ruder.io/talks/">Talks</a></li>
    <li class="nav-news" role="menuitem"><a href="https://ruder.io/news/">News</a></li>
    <li class="nav-faq" role="menuitem"><a href="https://ruder.io/faq/">FAQ</a></li>
    <li class="nav-newsletter" role="menuitem"><a href="https://ruder.io/nlp-news/">Newsletter</a></li>
    <li class="nav-nlp-progress" role="menuitem"><a href="https://nlpprogress.com/">NLP Progress</a></li>
    <li class="nav-media" role="menuitem"><a href="https://ruder.io/media/">Media</a></li>
    <li class="nav-contact" role="menuitem"><a href="https://ruder.io/contact/">Contact</a></li>
</ul>

                    <span class="nav-post-title dash">Recent Advances in Language Model Fine-tuning</span>
            </div>
        </div>
    </div>
    <div class="site-nav-right">
            <div class="social-links">
                    <a class="social-link social-link-tw" href="https://twitter.com/seb_ruder" title="Twitter" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
</a>
            </div>
                <a class="rss-button" href="https://ruder.io/rss/index.rss" title="RSS" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
</a>

    </div>
</nav>
    </div>
</div></header>


<main id="site-main" class="site-main outer">
    <div class="inner">

        <article class="post-full post tag-language-models tag-natural-language-processing tag-transfer-learning ">

            <header class="post-full-header">

                <section class="post-full-tags">
                    <a href="../tag/language-models/index.html">language models</a>
                </section>

                <h1 class="post-full-title">Recent Advances in Language Model Fine-tuning</h1>

                <p class="post-full-custom-excerpt">This article provides an overview of recent methods to fine-tune large pre-trained language models.</p>

                <div class="post-full-byline">

                    <section class="post-full-byline-content">

                        <ul class="author-list">
                            <li class="author-list-item">

                                <div class="author-card">
                                    <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                                    <div class="author-info">
                                        <h2>Sebastian Ruder</h2>
                                        <p>Read <a href="../author/sebastian/index.html">more posts</a> by this author.</p>
                                    </div>
                                </div>

                                <a href="../author/sebastian/index.html" class="author-avatar">
                                    <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                                </a>

                            </li>
                        </ul>

                        <section class="post-full-byline-meta">
                            <h4 class="author-name"><a href="../author/sebastian/index.html">Sebastian Ruder</a></h4>
                            <div class="byline-meta-content">
                                <time class="byline-meta-date" datetime="2021-02-24">24 Feb 2021</time>
                                <span class="byline-reading-time"><span class="bull">&bull;</span> 13 min read</span>
                            </div>
                        </section>

                    </section>


                </div>
            </header>

            <figure class="post-full-image">
                <img
                    srcset="../content/images/size/w300/2021/02/fine-tuning_methods.png 300w,
                           ../content/images/size/w600/2021/02/fine-tuning_methods.png 600w,
                          ../content/images/size/w1000/2021/02/fine-tuning_methods.png 1000w,
                         ../content/images/size/w2000/2021/02/fine-tuning_methods.png 2000w"
                    sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px"
                    src="../content/images/size/w2000/2021/02/fine-tuning_methods.png"
                    alt="Recent Advances in Language Model Fine-tuning"
                />
            </figure>

            <section class="post-full-content">
                <div class="post-content">
                    <p>Fine-tuning a pre-trained language model (LM) has become the de facto standard for doing transfer learning in natural language processing. Over the last three years (<a href="https://thegradient.pub/nlp-imagenet/">Ruder, 2018</a>), fine-tuning (<a href="https://www.aclweb.org/anthology/P18-1031/">Howard &amp; Ruder, 2018</a>) has superseded the use of feature extraction of pre-trained embeddings (<a href="https://www.aclweb.org/anthology/N18-1202/">Peters et al., 2018</a>) while pre-trained language models are favoured over models trained on translation (<a href="https://papers.nips.cc/paper/2017/file/20c86a628232a67e7bd46f76fba7ce12-Paper.pdf">McCann et al., 2018</a>), natural language inference (<a href="https://www.aclweb.org/anthology/D17-1070/">Conneau et al., 2017</a>), and other tasks due to their increased sample efficiency and performance (<a href="https://www.aclweb.org/anthology/W18-5448/">Zhang and Bowman, 2018</a>). The empirical success of these methods has led to the development of ever larger models (<a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>; <a href="https://jmlr.org/papers/v21/20-074.html">Raffel et al., 2020</a>). Recent models are so large in fact that they can achieve reasonable performance <em>without</em> any parameter updates (<a href="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown et al., 2020</a>). The limitations of this zero-shot setting (see <a href="index.html#text-to-text-fine-tuning">this section</a>), however, make it likely that in order to achieve the best performance or stay reasonably efficient, <strong>fine-tuning will continue to be the modus operandi when using large pre-trained LMs in practice.</strong></p><p>In the standard transfer learning setup (see below; see <a href="https://ruder.io/state-of-transfer-learning-in-nlp/">this post</a> for a general overview), a model is first pre-trained on large amounts of unlabelled data using a language modelling loss such as masked language modelling (MLM; <a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>). The pre-trained model is then fine-tuned on labelled data of a downstream task using a standard cross-entropy loss.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2021/02/pretraining_finetuning.png" class="kg-image"><figcaption>The standard pre-training—fine-tuning setting (adapted from <a href="https://tiny.cc/NAACLTransfer">(Ruder et al., 2019)</a>)</figcaption></figure><p>While pre-training is compute-intensive, fine-tuning can be done comparatively inexpensively. Fine-tuning is more important for the practical usage of such models as individual pre-trained models are downloaded—and fine-tuned—millions of times (see <a href="https://huggingface.co/models?sort=downloads">the Hugging Face models repository</a>). Consequently, fine-tuning is the main focus of this post. In particular, I will highlight the most recent advances that have shaped or are likely to change the way we fine-tune language models, which can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2021/02/fine-tuning_methods_overview.png" class="kg-image"><figcaption>Overview of fine-tuning methods discussed in this post.</figcaption></figure><h2 id="adaptive-fine-tuning">Adaptive fine-tuning</h2><p>Even though pre-trained language models are more robust in terms of out-of-distribution generalisation than previous models (<a href="https://www.aclweb.org/anthology/2020.acl-main.244/">Hendrycks et al., 2020</a>), they are still poorly equipped to deal with data that is substantially different from the one they have been pre-trained on. Adaptive fine-tuning is a way to bridge such a shift in distribution by fine-tuning the model on data that is closer to the distribution of the target data. Specifically, adaptive fine-tuning involves fine-tuning the model on additional data prior to task-specific fine-tuning, which can be seen below. Importantly, the model is fine-tuned with the pre-training objective, so adaptive fine-tuning only requires unlabelled data.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2021/02/adaptive_fine-tuning-1.png" class="kg-image"><figcaption>Adaptive fine-tuning as part of the standard transfer learning setting. A pre-trained model is trained with the pre-training loss (typically masked language modelling) on data that is closer to the target distribution.</figcaption></figure><!--kg-card-begin: markdown--><p>Formally, given a target domain $\mathcal{D}_T$ consisting of a feature space $\mathcal{X}$ and a marginal probability distribution over the feature space $P(X)$ where $X = \{x_1, \ldots, x_n \} \in \mathcal{X}$ (<a href="https://ieeexplore.ieee.org/document/5288526">Pan and Yang, 2009</a>; <a href="https://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=62">Ruder, 2019</a>), adaptive fine-tuning allows us to learn about both the feature space $\mathcal{X}$ and the distribution of the target data $P(X)$.</p>
<!--kg-card-end: markdown--><p>Variants of adaptive fine-tuning—domain, task, and language-adaptive fine-tuning—have been used to adapt a model to data of the target domain, target task, and target language respectively. <a href="https://papers.nips.cc/paper/2015/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf">Dai and Le (2015)</a> first showed the benefits of domain-adaptive fine-tuning. <a href="https://www.aclweb.org/anthology/P18-1031/">Howard and Ruder (2018)</a> later demonstrated improved sample efficiency by fine-tuning on in-domain data as part of ULMFiT. They also proposed task-adaptive fine-tuning, which fine-tunes the model with the pre-training objective on the task training data. As the pre-training loss provides richer information for modelling the target data compared to the cross-entropy over one-hot task labels, task-adaptive fine-tuning is useful beyond regular fine-tuning. Alternatively, adaptive and regular fine-tuning can be done jointly via multi-task learning (<a href="https://www.aclweb.org/anthology/N19-1213/">Chronopoulou et al., 2019</a>).</p><p>Domain and task-adaptive fine-tuning have recently been applied to the latest generation of pre-trained models (<a href="https://www.aclweb.org/anthology/P19-1335/">Logeswaran et al., 2019</a>; <a href="https://www.aclweb.org/anthology/D19-1433/">Han and Eisenstein, 2019</a>; <a href="https://www.aclweb.org/anthology/P19-1373/">Mehri et al., 2019</a>). <a href="https://www.aclweb.org/anthology/2020.acl-main.740/">Gururangan et al. (2020)</a> show that adapting to data of the target domain and target task are complementary. Recently, <a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/">Pfeiffer et al. (2020)</a> proposed language-adaptive fine-tuning to adapt a model to new languages.</p><p>An adaptively fine-tuned model is specialised to a particular data distribution, which it will be able to model well. However, this comes at the expense of its ability to be a general model of language. <strong>Adaptive fine-tuning is thus most useful when high performance on (potentially multiple) tasks of a single domain is important</strong> and can be computationally inefficient if a pre-trained model should be adapted to a large number of domains.</p><h2 id="behavioural-fine-tuning">Behavioural fine-tuning</h2><!--kg-card-begin: markdown--><p>While adaptive fine-tuning enables us to specialise our model to $\mathcal{D}_T$, it does not teach us anything directly about the target task. Formally, a target task $\mathcal{T}_T$ consists of a label space $\mathcal{Y}$, a prior distribution $P(Y)$ where $Y = \{y_1, \ldots, y_n \} \in \mathcal{Y}$, and a conditional probability distribution $P(Y | X)$. Alternatively, we can teach a model capabilities useful for doing well on the target task by fine-tuning it on relevant tasks, as can be seen below. We will refer to this setting as <em>behavioural fine-tuning</em> as it focuses on learning useful behaviours and to distinguish it from adaptive fine-tuning.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2021/02/behavioural_fine-tuning.png" class="kg-image"><figcaption>Behavioural fine-tuning of a pre-trained model. The pre-trained model is trained with task-specific supervised or self-supervised objectives on tasks that are relevant for the target task.</figcaption></figure><p>One way to teach a model relevant capabilities is to fine-tune it on relevant labelled data of a related task prior to task-specific fine-tuning (<a href="https://arxiv.org/abs/1811.01088">Phang et al., 2018</a>). This so-called intermediate-task training works best with tasks that require high-level inference and reasoning capabilities (<a href="https://www.aclweb.org/anthology/2020.acl-main.467/">Pruksachatkun et al., 2020</a>; <a href="https://www.aclweb.org/anthology/2020.aacl-main.56/">Phang et al., 2020</a>). Behavioural fine-tuning with labelled data has been used to teach a model information about named entities (<a href="https://www.aclweb.org/anthology/K19-1063/">Broscheit, 2019)</a>, paraphrasing (<a href="https://www.aclweb.org/anthology/D19-1542/">Arase and Tsujii, 2019</a>), syntax (<a href="https://arxiv.org/abs/2008.06788">Glavaš and Vulić, 2020</a>), answer sentence selection (<a href="https://arxiv.org/abs/1911.04118">Garg et al., 2020</a>), and question answering (<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.171/">Khashabi et al., 2020)</a>. <a href="https://arxiv.org/abs/2101.11038">Aghajanyan et al. (2021)</a> fine-tune on around 50 labelled datasets in a massively multi-task setting and observe that a large, diverse collection of tasks is important for good transfer performance. </p><p>As supervised data of such high-level reasoning tasks is generally hard to obtain, we can instead train on objectives that teach the model capabilities that are relevant for the downstream task but which can still be learned in a self-supervised manner. For instance, <a href="https://arxiv.org/abs/2101.08231">Dou and Neubig (2021)</a> fine-tune a model for word alignment with an objective that teaches it to identify parallel sentences, among others. <a href="https://www.aclweb.org/anthology/2020.acl-main.704/">Sellam et al. (2020)</a> fine-tune BERT for quality evaluation with a range of sentence similarity signals. In both cases, a diversity of learning signals is important.</p><p>Another effective way is to frame the target task as a form of masked language modelling. To this end, <a href="https://www.aclweb.org/anthology/2020.tacl-1.33/">Ben-David et al. (2020)</a> fine-tune a model for sentiment domain adaptation with a pivot-based objective. Others propose pre-training objectives, which can be used similarly during fine-tuning: <a href="https://arxiv.org/abs/2101.00438">Ram et al. (2021)</a> pre-train a model for QA with a span selection task while <a href="https://www.aclweb.org/anthology/2020.emnlp-main.38/">Bansal et al. (2020)</a> pre-train a model for few-shot learning by automatically generating cloze-style multi-class classification tasks.</p><!--kg-card-begin: markdown--><p><strong>Distinguishing between adaptive and behavioural fine-tuning encourages us to consider the inductive biases we aim to instill in our model and whether they relate to properties of the domain $\mathcal{D}$ or the task $\mathcal{T}$</strong>. Disentangling the role of domain and task is important as information about a domain can often be learned using limited unlabelled data (<a href="https://www.aclweb.org/anthology/2020.coling-main.603/">Ramponi and Plank, 2020</a>) while the acquisition of high-level natural language understanding skills with current methods generally requires billions of pre-training data samples (<a href="https://arxiv.org/abs/2011.04946">Zhang et al., 2020</a>).</p>
<p>The distinction between task and domain becomes fuzzier, however, when we frame tasks in terms of the pre-training objective. A sufficiently general pre-training task such as MLM may provide useful information for learning $P(Y | X)$ but likely does not contain every signal important for the task. For instance, models pre-trained with MLM struggle with modelling negations, numbers, or named entities (<a href="https://www.aclweb.org/anthology/2020.tacl-1.54/">Rogers et al., 2020</a>).</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>Similarly, the use of data augmentation entangles the roles of $\mathcal{D}$ and $\mathcal{T}$ as it allows us to encode the desired capabilities directly in the data. For instance, by fine-tuning a model on text where gendered words are replaced with those of the opposite gender, a model can be made more robust to gender bias (<a href="https://www.aclweb.org/anthology/N18-2003/">Zhao et al., 2018</a>; <a href="https://www.aclweb.org/anthology/N19-1064/">Zhao et al., 2019</a>; <a href="https://arxiv.org/abs/2101.09688">Manela et al., 2021</a>).</p>
<!--kg-card-end: markdown--><h2 id="parameter-efficient-fine-tuning">Parameter-efficient fine-tuning</h2><p>When a model needs to be fine-tuned in many settings such as for a large number of users, it is computationally expensive to store a copy of a fine-tuned model for every scenario. Consequently, recent work has focused on keeping most of the model parameters fixed and fine-tuning a small number of parameters per task. In practice, this enables storing a single copy of a large model and many much smaller files with task-specific modifications.</p><p>The first approaches in this line of work are based on adapters (<a href="https://proceedings.neurips.cc/paper/2017/hash/e7b24b112a44fdd9ee93bdf998c6ca0e-Abstract.html">Rebuffi et al., 2017</a>), small bottleneck layers that are inserted between the layers of a pre-trained model (<a href="http://proceedings.mlr.press/v97/houlsby19a.html">Houlsby et al., 2019</a>; <a href="http://proceedings.mlr.press/v97/stickland19a.html">Stickland and Murray, 2019</a>) whose parameters are fixed. Adapters render common settings such as storing multiple checkpoints during training as well as more advanced techniques such as checkpoint averaging (<a href="https://arxiv.org/abs/1803.05407">Izmailov et al., 2018</a>), snapshot ensembling (<a href="https://openreview.net/forum?id=BJYwwY9ll">Huang et al., 2017</a>) and temporal ensembling (<a href="https://openreview.net/forum?id=BJ6oOfqge">Laine and Aila, 2017</a>) much more space-efficient. Using adapters, a general-purpose model can be efficiently adapted to many settings such as different languages (<a href="https://www.aclweb.org/anthology/D19-1165/">Bapna and Firat, 2019</a>). <a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/">Pfeiffer et al. (2020)</a> recently demonstrated that adapters are modular and can be combined via stacking, which enables learning specialised representations in isolation. This is particularly useful when working with the previously discussed methods: <strong>an adaptively or behaviourally fine-tuned adapter can be evaluated without any task-specific fine-tuning by stacking a trained task adapter on top of it</strong>. This setting can be seen below where a task adapter trained on named entity recognition (NER) is stacked on either an English (left) or Quechua language adapter (right).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2021/02/modular_adapters.png" class="kg-image"><figcaption>Task and language adapters inserted in a Transformer block in the MAD-X framework (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/">Pfeiffer et al., 2020</a>). Adapters learn encapsulated representations and can be replaced with each other, enabling zero-shot transfer.</figcaption></figure><!--kg-card-begin: markdown--><p>While adapters modify the model's activations without changing the underlying parameters, another line of work modifies the pre-trained parameters directly. To illustrate this set of methods, we can view fine-tuning as learning how to perturb the parameters of a pre-trained model. Formally, in order to obtain the parameters of a fine-tuned model $\theta_{\text{fine-tuned}} \in \mathbb{R}^D$ where $D$ is the dimensionality of the model, we learn a task-specific parameter vector $\theta_{\text{task}} \in \mathbb{R}^D$ that captures how to change the pre-trained model parameters $\theta_{\text{pre-trained}} \in \mathbb{R}^D$. The fine-tuned parameters are the result of applying the task-specific permutations to the pre-trained parameters:<br>
\begin{equation}<br>
\theta_{\text{fine-tuned}} = \theta_{\text{pre-trained}} + \theta_{\text{task}}<br>
\end{equation}</p>
<p>Instead of storing a copy of $\theta_{\text{fine-tuned}}$ for every task, we can store a single copy of $\theta_{\text{pre-trained}}$ and a copy of $\theta_{\text{task}}$ for every task. This setting is cheaper if we can parameterise $\theta_{\text{task}}$ more efficiently. To this end, <a href="https://arxiv.org/abs/2012.07463">Guo et al. (2020)</a> learn $\theta_{\text{task}}$ as a sparse vector. <a href="https://arxiv.org/abs/2012.13255">Aghajanyan et al. (2020)</a> set $\theta_{\text{task}} = \theta_\text{low} \textbf{M}$ where $\theta_\text{low}$ is a low-dimensional vector and $\textbf{M}$ is a random linear projection (in their case, the FastFood transform (<a href="https://openreview.net/forum?id=ryup8-WCW">Li et al., 2018</a>)).</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>Alternatively, we can apply modifications only to a subset of the pre-trained parameters. A classic method in computer vision (<a href="http://proceedings.mlr.press/v32/donahue14.html">Donahue et al., 2014</a>) fine-tunes only the last layer of the model. Let $\theta_{\text{pre-trained}}$ be the collection of pre-trained parameters across all $L$ layers of the model, i.e. $\theta_{\text{pre-trained}} = \bigcup\limits_{l=1}^{L} \theta_{\text{pre-trained}}^l$ where $\theta_{\text{pre-trained}}^l$ is the parameter vector associated with the $l$-th layer, with analogous notation for $\theta_{\text{fine-tuned}}$ and $\theta_{\text{task}}$. Fine-tuning only the last layer is then equivalent to:<br>
\begin{equation}<br>
\begin{split}<br>
\theta_{\text{fine-tuned}} = &amp; (\bigcup\limits_{l=1}^{L-1} \theta_{\text{pre-trained}}^l) \\<br>
&amp; \cup (\theta_{\text{pre-trained}}^L + \theta_{\text{task}}^L)<br>
\end{split}<br>
\end{equation}</p>
<!--kg-card-end: markdown--><p>While this works less well in NLP (<a href="https://www.aclweb.org/anthology/P18-1031/">Howard &amp; Ruder, 2018</a>), there are other subsets of parameters that are more effective to fine-tune. For instance, <a href="https://nlp.biu.ac.il/~yogo/bitfit.pdf">Ben-Zaken et al. (2020)</a> achieve competitive performance by only fine-tuning a model's bias parameters. </p><p>Another line of work prunes parameters of the pre-trained model during fine-tuning. Such methods use different criteria for pruning weights such as based on zero-th or first-order information about a weight's importance (<a href="https://papers.nips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf">Sanh et al., 2020</a>). As there is limited support of sparse architectures with current hardware, approaches that are <em>structurally</em> sparse, i.e. where updates are concentrated in a limited set of layers, matrices, or vectors are currently preferable. For instance, the last few layers of pre-trained models have been shown to be of limited use during fine-tuning and can be randomly reinitialised (<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.125/">Tamkin et al., 2020</a>; <a href="https://openreview.net/forum?id=cO1IH43yUF">Zhang et al., 2021</a>) or even completely removed (<a href="https://openreview.net/forum?id=xpFFI_NtgpW">Chung et al., 2021</a>).</p><!--kg-card-begin: markdown--><p>While pruning methods focus on reducing the total number of parameters of task-specific models, most of the other methods focus on reducing the number of <em>trainable</em> parameters—while maintaining a copy of $\theta_{\text{pre-trained}}$. The most recent of the latter approaches generally match the performance of full fine-tuning while training around 0.5% of the model's parameters per task (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/">Pfeiffer et al., 2020</a>; <a href="https://arxiv.org/abs/2012.07463">Guo et al., 2020</a>; <a href="https://nlp.biu.ac.il/~yogo/bitfit.pdf">Ben-Zaken et al., 2020</a>).</p>
<!--kg-card-end: markdown--><p>There is increasing evidence that large pre-trained language models learn representations that compress NLP tasks well (<a href="https://openreview.net/forum?id=ryup8-WCW">Li et al., 2018</a>; <a href="https://www.aclweb.org/anthology/2020.repl4nlp-1.18/">Gordon et al., 2020</a>; <a href="https://arxiv.org/abs/2012.13255">Aghajanyan et al., 2020</a>). This practical evidence coupled with their <strong>convenience, availability (<a href="https://www.aclweb.org/anthology/2020.emnlp-demos.7/">Pfeiffer et al., 2020</a>) as well as recent empirical successes make these methods promising both for conducting experiments as well as in practical settings.</strong></p><h2 id="text-to-text-fine-tuning">Text-to-text fine-tuning</h2><p>Another development in transfer learning is a move from masked language models such as BERT (<a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>) and RoBERTa (<a href="https://arxiv.org/abs/1907.11692">Liu et al., 2019</a>) to autoregressive models of language such as T5 (<a href="https://jmlr.org/papers/v21/20-074.html">Raffel et al., 2019</a>) and GPT-3 (<a href="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown et al., 2020</a>). While both sets of methods can be used to assign likelihood scores to text (<a href="https://www.aclweb.org/anthology/2020.acl-main.240/">Salazar et al., 2020</a>), autoregressive LMs are easier to sample from. In contrast, masked LMs are generally restricted to fill-in-the-blank settings, e.g. (<a href="https://www.aclweb.org/anthology/D19-1250/">Petroni et al., 2019</a>).</p><p>The standard way to use masked LMs for fine-tuning is to replace the output layer used for MLM with a randomly initialised task-specific head that is learned on the target task (<a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>). Alternatively, the pre-trained model's output layer can be reused by recasting a task as MLM in a cloze-style format (<a href="https://www.aclweb.org/anthology/2020.tacl-1.48/">Talmor et al., 2020</a>; <a href="https://arxiv.org/abs/2001.07676">Schick and Schütze, 2021</a>). Analogously, autoregressive LMs generally cast the target task in a text-to-text format (<a href="https://arxiv.org/abs/1806.08730">McCann et al., 2018</a>; <a href="https://jmlr.org/papers/v21/20-074.html">Raffel et al., 2020</a>; <a href="https://openreview.net/forum?id=US-TP-xnXI">Paolini et al., 2021</a>). In both settings, <strong>the models are able to benefit from all their pre-trained knowledge and do not need to learn any new parameters from scratch, which improves their sample efficiency.</strong></p><p>In the extreme when no parameters are fine-tuned, framing a target task in terms of the pre-training objective enables zero-shot or few-shot learning using a task-specific prompt and a small number of examples of a task (<a href="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown et al., 2020</a>). However, while such few-shot learning is possible, it is not the most effective way to use such models (<a href="https://arxiv.org/abs/2009.07118">Schick and Schütze, 2020</a>; see <a href="https://ruder.io/research-highlights-2020/#3-few-shot-learning">this post</a> for a brief overview). Learning without updates requires a huge model as the model needs to rely entirely on its existing knowledge. The amount of information available to the model is also restricted by its context window and the prompts shown to the model need to be carefully engineered.</p><p>Retrieval augmentation (see <a href="https://ruder.io/research-highlights-2020/#2-retrieval-augmentation">this post</a> for an overview) can be used to off-load the storage of external knowledge and symbolic approaches could be used to teach a model task-specific rules akin to (<a href="https://openreview.net/forum?id=SkeuexBtDr">Awasthi et al., 2020</a>). Pre-trained models will also become larger and more powerful and may be behaviourally fine-tuned to be good at the zero-shot setting. However, without fine-tuning a model is ultimately limited in its ability to adapt to a new task.</p><p>Consequently, <strong>for most practical settings the best path forward arguably is to fine-tune all or a subset of the model's parameters using the methods described in the previous sections.</strong> <strong>In addition, we will increasingly see an emphasis of pre-trained models' generative capabilities.</strong> While current methods generally focus on modifying a model's natural language input such as via automatic prompt design (<a href="https://arxiv.org/abs/2009.07118">Schick and Schütze, 2020</a>; <a href="https://arxiv.org/abs/2012.15723">Gao et al., 2020</a>; <a href="https://arxiv.org/abs/2010.15980">Shin et al., 2020</a>), the most effective way to modulate the output of such models will likely act directly on their hidden representations (<a href="https://openreview.net/forum?id=H1edEyBKDS">Dathathri et al., 2020</a>; see <a href="https://lilianweng.github.io/lil-log/2021/01/02/controllable-neural-text-generation.html">Lillian Weng's post</a> for an overview of methods for controllable generation). </p><h2 id="mitigating-fine-tuning-instabilities">Mitigating fine-tuning instabilities</h2><p>A practical problem with fine-tuning pre-trained models is that performance can vary drastically between different runs, particularly on small datasets (<a href="https://arxiv.org/abs/1811.01088">Phang et al., 2018</a>). <a href="https://arxiv.org/abs/2002.06305">Dodge et al., 2020</a> find that both the weight initialisation of the output layer and the order of the training data contribute to variation in performance. As instabilities are generally apparent early in training, they recommend stopping the least promising runs early after 20-30% of training. <a href="https://openreview.net/forum?id=nzpLWnVAyah">Mosbach et al. (2021</a>) additionally recommend using small learning rates and to increase the number of epochs when fine-tuning BERT.</p><p>A number of recent methods seek to mitigate instabilities during fine-tuning by relying on adversarial or trust region-based approaches (<a href="https://openreview.net/forum?id=BygzbyHFvB">Zhu et al., 2019</a>; <a href="https://www.aclweb.org/anthology/2020.acl-main.197/">Jiang et al., 2020</a>; <a href="https://openreview.net/forum?id=OQ08SN70M1V">Aghajanyan et al., 2021</a>). Such methods generally augment the fine-tuning loss with a regularisation term that bounds the divergence between update steps.</p><p>In light of the previous section, we can make another recommendation for minimising instabilities during fine-tuning: <strong>Avoid using a randomly initialised output layer on the target task for small datasets by framing the target task as a form of LM or use behavioural fine-tuning to fine-tune the output layer prior to task-specific fine-tuning.</strong> While text-to-text models are thus more robust to fine-tuning on small datasets, they suffer from instabilities in the few-shot setting and are sensitive to the prompt and few-shot examples (<a href="https://arxiv.org/abs/2102.09690">Zhao et al., 2021</a>).</p><p>Overall, as models are increasingly applied to challenging tasks with fewer training examples, it is crucial to develop methods that are robust to possible variations and that can be reliably fine-tuned.</p><h2 id="citation">Citation</h2><p>For attribution in academic contexts, please cite this work as:</p><pre><code>@misc{ruder2021lmfine-tuning,
  author = {Ruder, Sebastian},
  title = {{Recent Advances in Language Model Fine-tuning}},
  year = {2021},
  howpublished = {\url{http://ruder.io/recent-advances-lm-fine-tuning}},
}</code></pre>
                </div>

<h2 id="citation">Newsletter</h2>

<style>
input {
  color: black;
}
</style>

If you want to receive regular updates about advances in machine learning and natural language processing, subscribe to <a href="https://newsletter.ruder.io/">my newsletter</a> below. 

<div id="revue-embed">
  <form action="https://newsletter.ruder.io/add_subscriber" method="post" id="revue-form" name="revue-form" $
  <div class="revue-form-group">
    <label for="member_email">Email address: </label>
    <input class="revue-form-field" placeholder="Your email address" type="email" name="member[email]" id="$
  </div>
  <div class="revue-form-group">
    <label for="member_first_name">First name <span class="optional">(Optional)</span>:</label>
    <input class="revue-form-field" placeholder="First name " type="text" name="member[first_name]" id="memb$
  </div>
  <div class="revue-form-group">
    <label for="member_last_name">Last name <span class="optional">(Optional)</span>:</label>
    <input class="revue-form-field" placeholder="Last name" type="text" name="member[last_name]" id="member$
  </div>
  <div class="revue-form-actions">
    <input type="submit" value="Subscribe" name="member[subscribe]" id="member_submit">
  </div>
  </form>
</div>

            </section>



        </article>

    </div>
</main>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = '/recent-advances-lm-fine-tuning/';  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "ghost-60226bf5a96736463841e6a2"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://EXAMPLE.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                            

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
                <article class="read-next-card">
                    <header class="read-next-card-header">
                        <h3><span>More in</span> <a href="../tag/language-models/index.html">language models</a></h3>
                    </header>
                    <div class="read-next-card-content">
                        <ul>
                            <li>
                                <h4><a href="../research-highlights-2020/index.html">ML and NLP Research Highlights of 2020</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2021-01-19">19 Jan 2021</time> –
                                        15 min read</p>
                                </div>
                            </li>
                            <li>
                                <h4><a href="../10-exciting-ideas-of-2018-in-nlp/index.html">10 Exciting Ideas of 2018 in NLP</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2018-12-19">19 Dec 2018</time> –
                                        8 min read</p>
                                </div>
                            </li>
                            <li>
                                <h4><a href="../emnlp-2018-highlights/index.html">EMNLP 2018 Highlights: Inductive bias, cross-lingual learning, and more</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2018-11-06">6 Nov 2018</time> –
                                        11 min read</p>
                                </div>
                            </li>
                        </ul>
                    </div>
                    <footer class="read-next-card-footer">
                        <a href="../tag/language-models/index.html">See all 7 posts
                            →</a>
                    </footer>
                </article>


                <article class="post-card post tag-transfer-learning tag-natural-language-processing tag-language-models tag-reinforcement-learning ">

    <a class="post-card-image-link" href="../research-highlights-2020/index.html">
        <img class="post-card-image"
            srcset="../content/images/size/w300/2021/01/lra_analysis-2.png 300w,
                   ../content/images/size/w600/2021/01/lra_analysis-2.png 600w,
                  ../content/images/size/w1000/2021/01/lra_analysis-2.png 1000w,
                 ../content/images/size/w2000/2021/01/lra_analysis-2.png 2000w"
            sizes="(max-width: 1000px) 400px, 700px"
            src="../content/images/size/w600/2021/01/lra_analysis-2.png"
            alt="ML and NLP Research Highlights of 2020"
        />
    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="../research-highlights-2020/index.html">

            <header class="post-card-header">
                    <div class="post-card-primary-tag">transfer learning</div>
                <h2 class="post-card-title">ML and NLP Research Highlights of 2020</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>This post summarizes progress in 10 exciting and impactful directions in ML and NLP in 2020.</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Sebastian Ruder
                    </div>
            
                    <a href="../author/sebastian/index.html" class="static-avatar">
                        <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/sebastian/index.html">Sebastian Ruder</a></span>
                <span class="post-card-byline-date"><time datetime="2021-01-19">19 Jan 2021</time> <span class="bull">&bull;</span> 15 min read</span>
            </div>
        </footer>

    </div>

</article>
        </div>
    </div>
</aside>




        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://ruder.io">Sebastian Ruder</a> &copy; 2021</section>
                <nav class="site-footer-nav">
                    <a href="https://ruder.io">Latest Posts</a>
                    
                    <a href="https://twitter.com/seb_ruder" target="_blank" rel="noopener">Twitter</a>
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>


    <script
        src="https://code.jquery.com/jquery-3.4.1.min.js"
        integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
        crossorigin="anonymous">
    </script>
    <script src="../assets/built/casper.js?v=b377148439"></script>

    <script>
        // Parse the URL parameter
        function getParameterByName(name, url) {
            if (!url) url = window.location.href;
            name = name.replace(/[\[\]]/g, "\\$&");
            var regex = new RegExp("[?&]" + name + "(=([^&#]*)|&|#|$)"),
                results = regex.exec(url);
            if (!results) return null;
            if (!results[2]) return '';
            return decodeURIComponent(results[2].replace(/\+/g, " "));
        }

        // Give the parameter a variable name
        var action = getParameterByName('action');

        $(document).ready(function () {
            if (action == 'subscribe') {
                $('body').addClass("subscribe-success");
            }

            $('.subscribe-success-message .subscribe-close').click(function () {
                $('.subscribe-success-message').addClass('close');
            });

            // Reset form on opening subscrion overlay
            $('.subscribe-button').click(function() {
                $('.subscribe-overlay form').removeClass();
                $('.subscribe-email').val('');
            });
        });
    </script>

    <script>
    $(document).ready(function () {
        // FitVids - start
        var $postContent = $(".post-full-content");
        $postContent.fitVids();
        // FitVids - end

        // Replace nav with title on scroll - start
        Casper.stickyNavTitle({
            navSelector: '.site-nav-main',
            titleSelector: '.post-full-title',
            activeClass: 'nav-post-title-active'
        });
        // Replace nav with title on scroll - end

        // Hover on avatar
        var hoverTimeout;
        $('.author-list-item').hover(function () {
            var $this = $(this);

            clearTimeout(hoverTimeout);

            $('.author-card').removeClass('hovered');
            $(this).children('.author-card').addClass('hovered');

        }, function () {
            var $this = $(this);

            hoverTimeout = setTimeout(function () {
                $this.children('.author-card').removeClass('hovered');
            }, 800);
        });
    });
</script>


    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/zepto/1.1.6/zepto.min.js"></script>
<script>jQuery = Zepto</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism.min.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/plugins/line-numbers/prism-line-numbers.min.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism-coy.min.css" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/components/prism-python.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-60512592-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-60512592-1');
</script>

</body>
</html>

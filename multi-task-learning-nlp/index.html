<!DOCTYPE html>
<html lang="en">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>Multi-Task Learning Objectives for Natural Language Processing</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="stylesheet" type="text/css" href="../assets/built/screen.css?v=6133c8c0f7" />

    <meta name="description" content="An overview of auxiliary tasks and objectives that have been used for multi-task learning for natural language processing." />
    <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon" />
    <link rel="canonical" href="https://ruder.io/multi-task-learning-nlp/" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <link rel="amphtml" href="https://ruder.io/multi-task-learning-nlp/amp/" />
    
    <meta property="og:site_name" content="Sebastian Ruder" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Multi-Task Learning Objectives for Natural Language Processing" />
    <meta property="og:description" content="Multi-task learning is becoming increasingly popular in NLP but it is still not understood very well which tasks are useful. As inspiration, this post gives an overview of the most common auxiliary tasks used for multi-task learning for NLP." />
    <meta property="og:url" content="https://ruder.io/multi-task-learning-nlp/" />
    <meta property="og:image" content="https://ruder.io/content/images/2017/09/soft_parameter_sharing.png" />
    <meta property="article:published_time" content="2017-09-24T13:42:00.000Z" />
    <meta property="article:modified_time" content="2018-10-24T13:23:12.000Z" />
    <meta property="article:tag" content="multi-task learning" />
    <meta property="article:tag" content="natural language processing" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Multi-Task Learning Objectives for Natural Language Processing" />
    <meta name="twitter:description" content="Multi-task learning is becoming increasingly popular in NLP but it is still not understood very well which tasks are useful. As inspiration, this post gives an overview of the most common auxiliary tasks used for multi-task learning for NLP." />
    <meta name="twitter:url" content="https://ruder.io/multi-task-learning-nlp/" />
    <meta name="twitter:image" content="https://ruder.io/content/images/2017/09/soft_parameter_sharing.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Sebastian Ruder" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="multi-task learning, natural language processing" />
    <meta name="twitter:site" content="@seb_ruder" />
    <meta property="og:image:width" content="1005" />
    <meta property="og:image:height" content="385" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Sebastian Ruder",
        "url": "https://ruder.io/",
        "logo": {
            "@type": "ImageObject",
            "url": {
                "@type": "ImageObject",
                "url": "https://ruder.io/favicon.ico",
                "width": 48,
                "height": 48
            }
        }
    },
    "author": {
        "@type": "Person",
        "name": "Sebastian Ruder",
        "image": {
            "@type": "ImageObject",
            "url": "https://ruder.io/content/images/2019/02/new_profile_photo_square-1.jpg",
            "width": 2000,
            "height": 2000
        },
        "url": "https://ruder.io/author/sebastian/",
        "sameAs": []
    },
    "headline": "Multi-Task Learning Objectives for Natural Language Processing",
    "url": "https://ruder.io/multi-task-learning-nlp/",
    "datePublished": "2017-09-24T13:42:00.000Z",
    "dateModified": "2018-10-24T13:23:12.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://ruder.io/content/images/2017/09/soft_parameter_sharing.png",
        "width": 1005,
        "height": 385
    },
    "keywords": "multi-task learning, natural language processing",
    "description": "Multi-task learning is becoming increasingly popular in NLP but it is still not understood very well which tasks are useful. As inspiration, this post gives an overview of the most common auxiliary tasks used for multi-task learning for NLP.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://ruder.io/"
    }
}
    </script>

    <meta name="generator" content="Ghost 3.11" />
    <link rel="alternate" type="application/rss+xml" title="Sebastian Ruder" href="https://ruder.io/rss/" />
    <script>
var profile_title = 'Sebastian Ruder';
</script>
<script>
var disqus_shortname = 'sebastianruder';
</script>
<script>
var profile_resume ='NLP PhD student';
</script>
<script>
var ga_id = 'UA-60512592-1';
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [["$", "$"], ["\\(", "\\)"]],
        processEscapes: true
    }
});
</script>

</head>
<body class="post-template tag-multi-task-learning tag-natural-language-processing">

    <div class="site-wrapper">

        

<header class="site-header">
    <div class="outer site-nav-main">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left-wrapper">
        <div class="site-nav-left">
                <a class="site-nav-logo" href="https://ruder.io">Sebastian Ruder</a>
            <div class="site-nav-content">
                    <ul class="nav" role="menu">
    <li class="nav-about" role="menuitem"><a href="https://ruder.io/about/">About</a></li>
    <li class="nav-tags" role="menuitem"><a href="https://ruder.io/tags/">Tags</a></li>
    <li class="nav-papers" role="menuitem"><a href="https://ruder.io/publications/">Papers</a></li>
    <li class="nav-talks" role="menuitem"><a href="https://ruder.io/talks/">Talks</a></li>
    <li class="nav-news" role="menuitem"><a href="https://ruder.io/news/">News</a></li>
    <li class="nav-faq" role="menuitem"><a href="https://ruder.io/faq/">FAQ</a></li>
    <li class="nav-sign-up-for-nlp-news" role="menuitem"><a href="https://ruder.io/nlp-news/">Sign up for NLP News</a></li>
    <li class="nav-nlp-progress" role="menuitem"><a href="https://nlpprogress.com/">NLP Progress</a></li>
    <li class="nav-media" role="menuitem"><a href="https://ruder.io/media/">Media</a></li>
    <li class="nav-contact" role="menuitem"><a href="https://ruder.io/contact/">Contact</a></li>
</ul>

                    <span class="nav-post-title dash">Multi-Task Learning Objectives for Natural Language Processing</span>
            </div>
        </div>
    </div>
    <div class="site-nav-right">
            <div class="social-links">
                    <a class="social-link social-link-tw" href="https://twitter.com/seb_ruder" title="Twitter" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
</a>
            </div>
                <a class="rss-button" href="https://ruder.io/rss/index.rss" title="RSS" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
</a>

    </div>
</nav>
    </div>
</div></header>


<main id="site-main" class="site-main outer">
    <div class="inner">

        <article class="post-full post tag-multi-task-learning tag-natural-language-processing ">

            <header class="post-full-header">

                <section class="post-full-tags">
                    <a href="../tag/multi-task-learning/index.html">multi-task learning</a>
                </section>

                <h1 class="post-full-title">Multi-Task Learning Objectives for Natural Language Processing</h1>

                <p class="post-full-custom-excerpt">Multi-task learning is becoming increasingly popular in NLP but it is still not understood very well which tasks are useful. As inspiration, this post gives an overview of the most common auxiliary tasks used for multi-task learning for NLP.</p>

                <div class="post-full-byline">

                    <section class="post-full-byline-content">

                        <ul class="author-list">
                            <li class="author-list-item">

                                <div class="author-card">
                                    <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                                    <div class="author-info">
                                        <h2>Sebastian Ruder</h2>
                                        <p>Read <a href="../author/sebastian/index.html">more posts</a> by this author.</p>
                                    </div>
                                </div>

                                <a href="../author/sebastian/index.html" class="author-avatar">
                                    <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                                </a>

                            </li>
                        </ul>

                        <section class="post-full-byline-meta">
                            <h4 class="author-name"><a href="../author/sebastian/index.html">Sebastian Ruder</a></h4>
                            <div class="byline-meta-content">
                                <time class="byline-meta-date" datetime="2017-09-24">24 Sep 2017</time>
                                <span class="byline-reading-time"><span class="bull">&bull;</span> 16 min read</span>
                            </div>
                        </section>

                    </section>


                </div>
            </header>

            <figure class="post-full-image">
                <img
                    srcset="../content/images/size/w300/2017/09/soft_parameter_sharing.png 300w,
                           ../content/images/size/w600/2017/09/soft_parameter_sharing.png 600w,
                          ../content/images/size/w1000/2017/09/soft_parameter_sharing.png 1000w,
                         ../content/images/size/w2000/2017/09/soft_parameter_sharing.png 2000w"
                    sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px"
                    src="../content/images/size/w2000/2017/09/soft_parameter_sharing.png"
                    alt="Multi-Task Learning Objectives for Natural Language Processing"
                />
            </figure>

            <section class="post-full-content">
                <div class="post-content">
                    <!--kg-card-begin: markdown--><p>This post discusses the most common auxiliary tasks used in multi-task learning in natural language processing.</p>
<p>This post has two main parts: In the first part, I will talk about artificial tasks that can be used as auxiliary objectives for MTL. In the second part, I will focus on common NLP tasks and discuss which other NLP tasks have benefited them.</p>
<p>In a <a href="http://ruder.io/multi-task/index.html">previous blog post</a>, I discussed how multi-task learning (MTL) can be used to improve the performance of a model by leveraging a related task. Multi-task learning consists of two main components: a) The architecture used for learning and b) the auxiliary task(s) that are trained jointly. Both facets still have a lot of room for improvement. In addition, multi-task learning has the potential to be a key technique on the path to more robust models that learn from limited data: Training a model to acquire proficiency in performing a wide range of NLP tasks would allow us to induce representations, which should be useful for transferring knowledge to many other tasks, as outlined in <a href="http://ruder.io/transfer-learning/index.html">this blog post</a>.</p>
<p>On the way to this goal, we first need to learn more about the relationships between our tasks, what we can learn from each, and how to combine them most effectively. Most of the existing theory in MTL has focused on homogeneous tasks, i.e. tasks that are variations of the same classification or regression problem, such as classifying individual MNIST digits. These guarantees, however, do not hold for the heterogeneous tasks to which MTL is most often applied in Natural Language Processing (NLP) and Computer Vision.</p>
<p>There have been some recent studies looking into when multi-task learning between different NLP tasks works but we still do not understand very well which tasks are useful. To this end, as inspiration, I will give an overview in the following of different approaches for multi-task learning for NLP. I will focus on the second component of multi-task learning; instead of discussing <em>how</em> a model is trained, as most architectures only differ in which layers they share, I will concentrate on the auxiliary tasks and objectives that are used for learning.</p>
<p>This post has two main parts: In the first part, I will talk about artificial tasks that can be used as auxiliary objectives for MTL. In the second part, I will focus on common NLP tasks and discuss which other NLP tasks have benefited them.</p>
<h1 id="artificialauxiliaryobjectives">Artificial auxiliary objectives</h1>
<p>Multi-task learning is all about coming up with ways to add a suitable bias to your model. Incorporating artificial auxiliary tasks that cleverly complement your target task is arguably one of the most ingenious and fun ways to do MTL. It is a feature-engineering of sorts: instead of engineering the features, you are engineering the auxiliary task you optimize. Similarly to feature engineering, domain expertise is therefore required as we will see in the following:</p>
<p><strong>Language modelling</strong> Language modelling has been shown to be beneficial for many NLP tasks and can be incorporated in various ways. Word embeddings pre-trained by word2vec have been shown to beneficial -- as is known, word2vec approximates the language modelling objective; languages models have been used to pre-train MT and sequence-to-sequence models <sup class="footnote-ref"><a href="index.html#fn1" id="fnref1">[1]</a></sup>; contextual language model embeddings have also been found useful for many tasks <sup class="footnote-ref"><a href="index.html#fn2" id="fnref2">[2]</a></sup>. In this context, we can also treat language modelling as an auxiliary task that is learned together with the main task. Rei (2017) <sup class="footnote-ref"><a href="index.html#fn3" id="fnref3">[3]</a></sup> shows that this improves performance on several sequence labelling tasks.</p>
<p><strong>Conditioning the initial state</strong>   The initial state of a recurrent neural network is typically initialized to a \(0\) vector. According to a <a href="https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf">lecture by Hinton in 2013</a>, it is beneficial to learn the initial state just like any other sets of weights. While a learned state will be more helpful than a \(0\) vector it will be independent of the sequence and thus unable to adapt. Weng et al. (2017) <sup class="footnote-ref"><a href="index.html#fn4" id="fnref4">[4]</a></sup> propose to add a suitable bias to the initial encoder and decoder states for NMT by training it to predict the words in the sentence. In this sense, this objective can essentially be seen as a <em>language modelling objective for the initial state</em> and might thus be helpful for other tasks. Similarly, we can think of other task-specific biases that could be encoded in the initial state to aid learning: A sentiment model might benefit from knowing about the general audience response to a movie or whether a user is more likely to be sarcastic while a parser might be able to leverage prior knowledge of the domain's tree depth or complexity.</p>
<p><strong>Adversarial loss</strong>   An auxiliary adversarial loss was first found to be useful for domain adaptation <sup class="footnote-ref"><a href="index.html#fn5" id="fnref5">[5]</a></sup>, <sup class="footnote-ref"><a href="index.html#fn6" id="fnref6">[6]</a></sup>, where it is used to learn domain-invariant representations by rendering the model unable to distinguish between different domains. This is typically done by adding a gradient reversal layer that reverses the sign of the gradient during back-propagation, which in turn leads to a maximization rather than a minimization of the adversarial loss. It is not to be confused with adversarial examples <sup class="footnote-ref"><a href="index.html#fn7" id="fnref7">[7]</a></sup>, which significantly increase the model's loss typically via small perturbations to its input; adversarial training <sup class="footnote-ref"><a href="index.html#fn8" id="fnref8">[8]</a></sup>, which trains a model to correctly classify such examples; or Generative Adversarial Networks, which are trained to generate some output representation. An adversarial loss can be added to many tasks in order to learn task-independent representations <sup class="footnote-ref"><a href="index.html#fn9" id="fnref9">[9]</a></sup>. It can also be used to ignore certain features of the input that have been found to be detrimental to generalization, such as data-specific properties that are unlikely to generalize. Finally, an adversarial auxiliary task might also help to combat bias and ensure more privacy by encouraging the model to learn representations, which do not contain information that would allow the reconstruction of sensitive user attributes.</p>
<p><strong>Predicting data statistics</strong>   An auxiliary loss can also be to predict certain underlying statistics of the training data. In contrast to the adversarial loss, which tries to make the model oblivious to certain features, this auxiliary task explicitly encourages the model to predict certain data statistics. Plank et al. (2016) <sup class="footnote-ref"><a href="index.html#fn10" id="fnref10">[10]</a></sup> predict the log frequency of a word as an auxiliary task for language modelling. Intuitively, this makes the representation predictive of frequency, which encourages the model to not share representations between common and rare words, which benefits the handling of rare tokens. Another facet of this auxiliary task is to predict attributes of the user, such as their gender, which has been shown to be beneficial for predicting mental health conditions <sup class="footnote-ref"><a href="index.html#fn11" id="fnref11">[11]</a></sup> or other demographic information <sup class="footnote-ref"><a href="index.html#fn12" id="fnref12">[12]</a></sup>. We can think of other statistics that might be beneficial for a model to encode, such as the frequency of POS tags, parsing structures, or entities, the preferences of users, a sentence's coverage for summarization, or even a user's website usage patterns.</p>
<p><strong>Learning the inverse</strong>   Another auxiliary task that might be useful in many circumstances is to learn the inverse of the task together with the main task. A popular example of this framework is CycleGAN <sup class="footnote-ref"><a href="index.html#fn13" id="fnref13">[13]</a></sup>, which can <a href="https://github.com/junyanz/CycleGAN">generate photos from paintings</a>. An inverse auxiliary loss, however, is applicable to many other tasks: MT might be the most intuitive, as every translation direction such as English-&gt;French directly provides data for the inverse direction, as Xia et al. (2016) <sup class="footnote-ref"><a href="index.html#fn14" id="fnref14">[14]</a></sup> demonstrate. Xia et al. (2017) <sup class="footnote-ref"><a href="index.html#fn15" id="fnref15">[15]</a></sup> show that this has applications not only to MT, but also to image classification (with image generation as its inverse) and sentiment classification (paired with sentence generation). For multimodal translation, Elliott and Kádár (2017) <sup class="footnote-ref"><a href="index.html#fn16" id="fnref16">[16]</a></sup> jointly learn an inverse task by predicting image representations. It is not difficult to think of inverse complements for many other tasks: Entailment has hypothesis generation; video captioning has video generation; speech recognition has speech synthesis, etc.</p>
<p><strong>Predicting what should be there</strong>   For many tasks, where a model has to pick up on certain features of the training data, we can focus the model's attention on these characteristics by encouraging it explicitly to predict them. For sentiment analysis, for instance, Yu and Jiang (2016) <sup class="footnote-ref"><a href="index.html#fn17" id="fnref17">[17]</a></sup> predict whether the sentence contains a positive or negative domain-independent sentiment word, which sensitizes the model towards the sentiment of the words in the sentence. For name error detection, Cheng et al. (2015) <sup class="footnote-ref"><a href="index.html#fn18" id="fnref18">[18]</a></sup> predict if a sentence contains a name. We can envision similar auxiliary tasks that might be useful for other tasks: Predicting whether certain entities occur in a sentence might be useful for relation extraction; predicting whether a headline contains certain lurid terms might help for clickbait detection, while predicting whether an emotion word occurs in the sentence might benefit emotion detection. In summary, this auxiliary task should be helpful whenever a task includes certain highly predictive terms or features.</p>
<h1 id="jointtrainingofexistingnlptasks">Joint training of existing NLP tasks</h1>
<p>In this second section, we will now look at existing NLP tasks, which have been used to improve the performance of a main task. While certain tasks such as chunking and semantic tagging have been found to be useful for many tasks <sup class="footnote-ref"><a href="index.html#fn19" id="fnref19">[19]</a></sup>, the choice whether to use a particular auxiliary task largely depends on characteristics of the main task. In the following, I will thus highlight different strategies and rationals that were used to select auxiliary tasks for many common tasks in NLP:</p>
<p><strong>Speech recognition</strong>   Recent multi-task learning approaches for automatic speech recognition (ASR) typically use additional supervision signals that are available in the speech recognition pipeline as auxiliary tasks to train an ASR model end-to-end. Phonetic recognition and frame-level state classification can be used as auxiliary tasks to induce helpful intermediate representations. Toshniwal et al. (2017) <sup class="footnote-ref"><a href="index.html#fn20" id="fnref20">[20]</a></sup> find that positioning the auxiliary loss at an intermediate layer improves performance. Similarly, Arık et al. (2017) <sup class="footnote-ref"><a href="index.html#fn21" id="fnref21">[21]</a></sup> predict the phoneme duration and frequency profile as auxiliary tasks for speech synthesis.</p>
<p><strong>Machine translation</strong>   The main benefit MTL has brought to machine translation (MT) is by jointly training translation models from and to different languages: Dong et al. (2015) <sup class="footnote-ref"><a href="index.html#fn22" id="fnref22">[22]</a></sup> jointly train the decoders; Zoph and Knight (2016) <sup class="footnote-ref"><a href="index.html#fn23" id="fnref23">[23]</a></sup> jointly train the encoders, while Johnson et al. (2016) <sup class="footnote-ref"><a href="index.html#fn24" id="fnref24">[24]</a></sup> jointly train both encoders and decoders; Malaviya et al. (2017) <sup class="footnote-ref"><a href="index.html#fn25" id="fnref25">[25]</a></sup> train one model to translate from 1017 languages into English.</p>
<p>Other tasks have also shown to be useful for MT: Luong et al. (2015) <sup class="footnote-ref"><a href="index.html#fn26" id="fnref26">[26]</a></sup> show gains using parsing and image captioning as auxiliary tasks; Niehues and Cho (2017) <sup class="footnote-ref"><a href="index.html#fn27" id="fnref27">[27]</a></sup> combine NMT with POS tagging and NER; Wu et al. (2017) <sup class="footnote-ref"><a href="index.html#fn28" id="fnref28">[28]</a></sup> jointly model the target word sequence and its dependency tree structure.</p>
<p><strong>Multilingual tasks</strong>   Similarly to MT, it can often be beneficial to jointly train models for different languages: Gains have been shown for dependency parsing <sup class="footnote-ref"><a href="index.html#fn29" id="fnref29">[29]</a></sup>, <sup class="footnote-ref"><a href="index.html#fn30" id="fnref30">[30]</a></sup>, named entity recognition <sup class="footnote-ref"><a href="index.html#fn31" id="fnref31">[31]</a></sup>, part-of-speech tagging <sup class="footnote-ref"><a href="index.html#fn32" id="fnref32">[32]</a></sup>, document classification <sup class="footnote-ref"><a href="index.html#fn33" id="fnref33">[33]</a></sup>, discourse segmentation <sup class="footnote-ref"><a href="index.html#fn34" id="fnref34">[34]</a></sup>, and sequence tagging <sup class="footnote-ref"><a href="index.html#fn35" id="fnref35">[35]</a></sup>.</p>
<p><strong>Language grounding</strong>   For grounding language in images or videos, it is often useful to enable the model to learn causal relationships in the data. For video captioning, Pasunuru and Bansal (2017) <sup class="footnote-ref"><a href="index.html#fn36" id="fnref36">[36]</a></sup> jointly learn to predict the next frame in the video and to predict entailment, while Hermann et al. (2017) <sup class="footnote-ref"><a href="index.html#fn37" id="fnref37">[37]</a></sup> also predict the next frame in a video and the words that represent the visual state for language learning in a simulated environment.</p>
<p><strong>Semantic parsing</strong>   For a task where multiple label sets or formalisms are available such as for semantic parsing, an interesting MTL strategy is to learn these formalisms together: To this end, Guo et al. (2016) <sup class="footnote-ref"><a href="index.html#fn38" id="fnref38">[38]</a></sup> jointly train on multi-typed treebanks; Peng et al. (2017) <sup class="footnote-ref"><a href="index.html#fn39" id="fnref39">[39]</a></sup> learn three semantic dependency graph formalisms simultaneously; Fan et al. (2017) <sup class="footnote-ref"><a href="index.html#fn40" id="fnref40">[40]</a></sup> jointly learn different Alexa-based semantic parsing formalisms; and Zhao and Huang (2017) <sup class="footnote-ref"><a href="index.html#fn41" id="fnref41">[41]</a></sup> jointly train a syntactic and a discourse parser. For more shallow semantic parsing such as frame-semantic argument identification, Swayamdipta et al. (2017) <sup class="footnote-ref"><a href="index.html#fn42" id="fnref42">[42]</a></sup> predict whether an n-gram is syntactically meaningful, i.e. a syntactic constituent.</p>
<p><strong>Representation learning</strong>   For learning general-purpose representations, the challenge often is in defining the objective. Most existing representation learning models have been based on a single loss function, such as predicting the next word <sup class="footnote-ref"><a href="index.html#fn43" id="fnref43">[43]</a></sup> or sentence <sup class="footnote-ref"><a href="index.html#fn44" id="fnref44">[44]</a></sup> or training on a certain task such as entailment <sup class="footnote-ref"><a href="index.html#fn45" id="fnref45">[45]</a></sup> or MT <sup class="footnote-ref"><a href="index.html#fn46" id="fnref46">[46]</a></sup>. Rather than learning representations based on a single loss, intuitively, representations should become more general as more tasks are used to learn them. As an example of this strategy, Hashimoto et al. (2017) <sup class="footnote-ref"><a href="index.html#fn47" id="fnref47">[47]</a></sup> jointly train a model on multiple NLP tasks, while Jernite et al. (2017) <sup class="footnote-ref"><a href="index.html#fn48" id="fnref48">[48]</a></sup> propose several discourse-based artificial auxiliary tasks for sentence representation learning.</p>
<p><strong>Question answering</strong>   For question answering (QA) and reading comprehension, it is beneficial to learn the different parts of a more complex end-to-end model together: Choi et al. (2017) <sup class="footnote-ref"><a href="index.html#fn49" id="fnref49">[49]</a></sup> jointly learn a sentence selection and answer generation model, while Wang et al. (2017) <sup class="footnote-ref"><a href="index.html#fn50" id="fnref50">[50]</a></sup> jointly train a ranking and reader model for open-domain QA.</p>
<p><strong>Information retrieval</strong>   For relation extraction, information related to different relations or roles can often be shared. To this end, Jiang (2009) <sup class="footnote-ref"><a href="index.html#fn51" id="fnref51">[51]</a></sup> jointly learn linear models between different relation types; Yang and Mitchell (2017) <sup class="footnote-ref"><a href="index.html#fn52" id="fnref52">[52]</a></sup> jointly predict semantic role labels and relations; Katiyar and Cardie (2017) <sup class="footnote-ref"><a href="index.html#fn53" id="fnref53">[53]</a></sup> jointly extract entities and relations; and Liu et al. (2015) <sup class="footnote-ref"><a href="index.html#fn54" id="fnref54">[54]</a></sup> jointly train domain classification and web search ranking.</p>
<p><strong>Chunking</strong>   Chunking has been shown to benefit from being jointly trained with low-level tasks such as POS tagging <sup class="footnote-ref"><a href="index.html#fn55" id="fnref55">[55]</a></sup>, <sup class="footnote-ref"><a href="index.html#fn56" id="fnref56">[56]</a></sup>, <sup class="footnote-ref"><a href="index.html#fn57" id="fnref57">[57]</a></sup>.</p>
<p><strong>Miscellaneous</strong>   Besides the tasks mentioned above, various other tasks have been shown to benefit from MTL: Balikas and Moura (2017) <sup class="footnote-ref"><a href="index.html#fn58" id="fnref58">[58]</a></sup> jointly train coarse-grained and fine-grained sentiment analysis; Luo et al. (2017) <sup class="footnote-ref"><a href="index.html#fn59" id="fnref59">[59]</a></sup> jointly predict charges and extract articles; Augenstein and Søgaard (2017) <sup class="footnote-ref"><a href="index.html#fn60" id="fnref60">[60]</a></sup> use several auxiliary tasks for keyphrase boundary detection; and Isonuma et al. (2017) <sup class="footnote-ref"><a href="index.html#fn61" id="fnref61">[61]</a></sup> pair sentence extraction with document classification.</p>
<h1 id="conclusion">Conclusion</h1>
<p>I hope this blog post was able to provide you with some insight with regard to which strategies are employed to select auxiliary tasks and objectives for multi-task learning in NLP. As I mentioned <a href="http://ruder.io/multi-task/index.html">before</a>, multi-task learning can be very broadly defined. I have tried to provide as broad of an overview as possible but I still likely have omitted many relevant approaches. If you are aware of an approach that provides a valuable perspective that is not represented here, please let me know in the comments below.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Ramachandran, P., Liu, P. J., &amp; Le, Q. V. (2016). Unsupervised Pretrainig for Sequence to Sequence Learning. arXiv Preprint arXiv:1611.02683. <a href="index.html#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Peters, M. E., Ammar, W., Bhagavatula, C., &amp; Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 1756–1765). <a href="index.html#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Rei, M. (2017). Semi-supervised Multitask Learning for Sequence Labeling. In Proceedings of ACL 2017. <a href="index.html#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>Weng, R., Huang, S., Zheng, Z., Dai, X., &amp; Chen, J. (2017). Neural Machine Translation with Word Predictions. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. <a href="index.html#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>Ganin, Y., &amp; Lempitsky, V. (2015). Unsupervised Domain Adaptation by Backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. (Vol. 37). <a href="index.html#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., … Lempitsky, V. (2016). Domain-Adversarial Training of Neural Networks. Journal of Machine Learning Research, 17, 1–35. <a href="http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf">http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf</a> <a href="index.html#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p>Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., &amp; Fergus, R. (2014). Intriguing properties of neural networks. In ICLR 2014. Retrieved from <a href="http://arxiv.org/abs/1312.6199">http://arxiv.org/abs/1312.6199</a> <a href="index.html#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p>Miyato, T., Dai, A. M., &amp; Goodfellow, I. (2016). Virtual Adversarial Training for Semi-Supervised Text Classification. Retrieved from <a href="http://arxiv.org/abs/1605.07725">http://arxiv.org/abs/1605.07725</a> <a href="index.html#fnref8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p>Liu, P., Qiu, X., &amp; Huang, X. (2017). Adversarial Multi-task Learning for Text Classification. In ACL 2017. Retrieved from <a href="http://arxiv.org/abs/1704.05742">http://arxiv.org/abs/1704.05742</a> <a href="index.html#fnref9" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p>Plank, B., Søgaard, A., &amp; Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. <a href="index.html#fnref10" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p>Benton, A., Mitchell, M., &amp; Hovy, D. (2017). Multi-Task Learning for Mental Health using Social Media Text. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. Retrieved from <a href="http://m-mitchell.com/publications/multitask-clinical.pdf">http://m-mitchell.com/publications/multitask-clinical.pdf</a> <a href="index.html#fnref11" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p>Roy, D. (2017). Twitter Demographic Classification Using Deep Multi-modal Multi-task Learning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 478–483). <a href="index.html#fnref12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p>Zhu, J., Park, T., Efros, A. A., Ai, B., &amp; Berkeley, U. C. (2017). Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. <a href="index.html#fnref13" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn14" class="footnote-item"><p>Xia, Y., He, D., Qin, T., Wang, L., Yu, N., Liu, T.-Y., &amp; Ma, W.-Y. (2016). Dual Learning for Machine Translation. In Advances in Neural Information Processing Systems 29 (NIPS 2016) (pp. 1–9). Retrieved from <a href="http://arxiv.org/abs/1611.00179">http://arxiv.org/abs/1611.00179</a> <a href="index.html#fnref14" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn15" class="footnote-item"><p>Xia, Y., Qin, T., Chen, W., Bian, J., Yu, N., &amp; Liu, T. (2017). Dual Supervised Learning. In ICML. <a href="index.html#fnref15" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn16" class="footnote-item"><p>Elliott, D., &amp; Kádár, Á. (2017). Imagination improves Multimodal Translation. Retrieved from <a href="http://arxiv.org/abs/1705.04350">http://arxiv.org/abs/1705.04350</a> <a href="index.html#fnref16" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn17" class="footnote-item"><p>Yu, J., &amp; Jiang, J. (2016). Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP2016), 236–246. Retrieved from <a href="http://www.aclweb.org/anthology/D/D16/D16-1023.pdf">http://www.aclweb.org/anthology/D/D16/D16-1023.pdf</a> <a href="index.html#fnref17" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn18" class="footnote-item"><p>Cheng, H., Fang, H., &amp; Ostendorf, M. (2015). Open-Domain Name Error Detection using a Multi-Task RNN. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (pp. 737–746). <a href="index.html#fnref18" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn19" class="footnote-item"><p>Bingel, J., &amp; Søgaard, A. (2017). Identifying beneficial task relations for multi-task learning in deep neural networks. In EACL. Retrieved from <a href="http://arxiv.org/abs/1702.08303">http://arxiv.org/abs/1702.08303</a> <a href="index.html#fnref19" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn20" class="footnote-item"><p>Toshniwal, S., Tang, H., Lu, L., &amp; Livescu, K. (2017). Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition. Retrieved from <a href="http://arxiv.org/abs/1704.01631">http://arxiv.org/abs/1704.01631</a> <a href="index.html#fnref20" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn21" class="footnote-item"><p>Arık, S. Ö., Chrzanowski, M., Coates, A., Diamos, G., Gibiansky, A., Kang, Y., … Shoeybi, M. (2017). Deep Voice: Real-time Neural Text-to-Speech. In ICML 2017. <a href="index.html#fnref21" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn22" class="footnote-item"><p>Dong, D., Wu, H., He, W., Yu, D., &amp; Wang, H. (2015). Multi-Task Learning for Multiple Language Translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (pp. 1723–1732). <a href="index.html#fnref22" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn23" class="footnote-item"><p>Zoph, B., &amp; Knight, K. (2016). Multi-Source Neural Translation. NAACL, 30–34. Retrieved from <a href="http://arxiv.org/abs/1601.00710">http://arxiv.org/abs/1601.00710</a> <a href="index.html#fnref23" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn24" class="footnote-item"><p>Johnson, M., Schuster, M., Le, Q. V, Krikun, M., Wu, Y., Chen, Z., … Dean, J. (2016). Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation. arXiv Preprint arXiv:1611.0455. <a href="index.html#fnref24" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn25" class="footnote-item"><p>Malaviya, C., Neubig, G., &amp; Littell, P. (2017). Learning Language Representations for Typology Prediction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from <a href="http://arxiv.org/abs/1707.09569">http://arxiv.org/abs/1707.09569</a> <a href="index.html#fnref25" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn26" class="footnote-item"><p>Luong, M.-T., Le, Q. V., Sutskever, I., Vinyals, O., &amp; Kaiser, L. (2015). Multi-task Sequence to Sequence Learning. In arXiv preprint arXiv:1511.06114. Retrieved from <a href="http://arxiv.org/abs/1511.06114">http://arxiv.org/abs/1511.06114</a> <a href="index.html#fnref26" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn27" class="footnote-item"><p>Niehues, J., &amp; Cho, E. (2017). Exploiting Linguistic Resources for Neural Machine Translation Using Multi-task Learning. In WMT 2017. Retrieved from <a href="http://arxiv.org/abs/1708.00993">http://arxiv.org/abs/1708.00993</a> <a href="index.html#fnref27" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn28" class="footnote-item"><p>Wu, S., Zhang, D., Yang, N., Li, M., &amp; Zhou, M. (2017). Sequence-to-Dependency Neural Machine Translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 698–707). <a href="index.html#fnref28" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn29" class="footnote-item"><p>Duong, L., Cohn, T., Bird, S., &amp; Cook, P. (2015). Low Resource Dependency Parsing: Cross-lingual Parameter Sharing in a Neural Network Parser. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), 845–850. <a href="index.html#fnref29" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn30" class="footnote-item"><p>Ammar, W., Mulcaire, G., Ballesteros, M., Dyer, C., &amp; Smith, N. A. (2016). One Parser, Many Languages. Transactions of the Association for Computational Linguistics, Vol. 4, Pp. 431–444, 2016, 4, 431–444. Retrieved from <a href="http://arxiv.org/abs/1602.01595">http://arxiv.org/abs/1602.01595</a> <a href="index.html#fnref30" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn31" class="footnote-item"><p>Gillick, D., Brunk, C., Vinyals, O., &amp; Subramanya, A. (2016). Multilingual Language Processing From Bytes. NAACL, 1296–1306. Retrieved from <a href="http://arxiv.org/abs/1512.00103">http://arxiv.org/abs/1512.00103</a> <a href="index.html#fnref31" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn32" class="footnote-item"><p>Fang, M., &amp; Cohn, T. (2017). Model Transfer for Tagging Low-resource Languages using a Bilingual Dictionary. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017). <a href="index.html#fnref32" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn33" class="footnote-item"><p>Nikolaos Pappas and Andrei Popescu-Belis (2017). Multilingual Hierarchical Attention Networks for Document Classification. In Proceedings of the Eighth International Joint Conference on Natural Language Processing. Retrieved from <a href="https://arxiv.org/pdf/1707.00896.pdf">https://arxiv.org/pdf/1707.00896.pdf</a> <a href="index.html#fnref33" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn34" class="footnote-item"><p>Braud, C., Lacroix, O., &amp; Søgaard, A. (2017). Cross-lingual and cross-domain discourse segmentation of entire documents. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. <a href="index.html#fnref34" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn35" class="footnote-item"><p>Yang, Z., Salakhutdinov, R., &amp; Cohen, W. (2016). Multi-Task Cross-Lingual Sequence Tagging from Scratch. <a href="index.html#fnref35" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn36" class="footnote-item"><p>Pasunuru, R., &amp; Bansal, M. (2017). Multi-Task Video Captioning with Video and Entailment Generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017). <a href="index.html#fnref36" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn37" class="footnote-item"><p>Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R., Soyer, H., … Phil Blunsom. (2017). Grounded Language Learning in a Simulated 3D World. Retrieved from <a href="https://arxiv.org/pdf/1706.06551.pdf">https://arxiv.org/pdf/1706.06551.pdf</a> <a href="index.html#fnref37" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn38" class="footnote-item"><p>Guo, J., Che, W., Wang, H., &amp; Liu, T. (2016). Exploiting Multi-typed Treebanks for Parsing with Deep Multi-task Learning. Retrieved from <a href="http://arxiv.org/abs/1606.01161">http://arxiv.org/abs/1606.01161</a> <a href="index.html#fnref38" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn39" class="footnote-item"><p>Peng, H., Thomson, S., Smith, N. A., &amp; Allen, P. G. (2017). Deep Multitask Learning for Semantic Dependency Parsing. In ACL 2017. Retrieved from <a href="https://arxiv.org/pdf/1704.06855.pdf">https://arxiv.org/pdf/1704.06855.pdf</a> <a href="index.html#fnref39" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn40" class="footnote-item"><p>Fan, X., Monti, E., Mathias, L., &amp; Dreyer, M. (2017). Transfer Learning for Neural Semantic Parsing. ACL Repl4NLP 2017. Retrieved from <a href="http://arxiv.org/abs/1706.04326">http://arxiv.org/abs/1706.04326</a> <a href="index.html#fnref40" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn41" class="footnote-item"><p>Zhao, K., &amp; Huang, L. (2017). Joint Syntacto-Discourse Parsing and the Syntacto-Discourse Treebank. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. <a href="index.html#fnref41" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn42" class="footnote-item"><p>Swayamdipta, S., Thomson, S., Dyer, C., &amp; Smith, N. A. (2017). Frame-Semantic Parsing with Softmax-Margin Segmental RNNs and a Syntactic Scaffold. Retrieved from <a href="http://arxiv.org/abs/1706.09528">http://arxiv.org/abs/1706.09528</a> <a href="index.html#fnref42" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn43" class="footnote-item"><p>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS. <a href="index.html#fnref43" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn44" class="footnote-item"><p>Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., &amp; Fidler, S. (2015). Skip-Thought Vectors, (786). Retrieved from <a href="http://arxiv.org/abs/1506.06726">http://arxiv.org/abs/1506.06726</a> <a href="index.html#fnref44" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn45" class="footnote-item"><p>Conneau, A., Kiela, D., Schwenk, H., Barrault, L., &amp; Bordes, A. (2017). Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. <a href="index.html#fnref45" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn46" class="footnote-item"><p>Mccann, B., Bradbury, J., Xiong, C., &amp; Socher, R. (2017). Learned in Translation: Contextualized Word Vectors. <a href="index.html#fnref46" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn47" class="footnote-item"><p>Hashimoto, K., Xiong, C., Tsuruoka, Y., &amp; Socher, R. (2017). A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from <a href="http://arxiv.org/abs/1611.01587">http://arxiv.org/abs/1611.01587</a> <a href="index.html#fnref47" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn48" class="footnote-item"><p>Jernite, Y., Bowman, S. R., &amp; Sontag, D. (2017). Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning. Retrieved from <a href="http://arxiv.org/abs/1705.00557">http://arxiv.org/abs/1705.00557</a> <a href="index.html#fnref48" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn49" class="footnote-item"><p>Choi, E., Hewlett, D., Uszkoreit, J., Lacoste, A., &amp; Berant, J. (2017). Coarse-to-Fine Question Answering for Long Documents. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 209–220). <a href="index.html#fnref49" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn50" class="footnote-item"><p>Wang, S., Yu, M., Guo, X., Wang, Z., Klinger, T., &amp; Zhang, W. (2017). R^3: Reinforced Reader-Ranker for Open-Domain Question Answering. <a href="index.html#fnref50" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn51" class="footnote-item"><p>Jiang, J. (2009). Multi-task transfer learning for weakly-supervised relation extraction. Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, (August), 1012–1020. <a href="https://doi.org/10.3115/1690219.1690288">https://doi.org/10.3115/1690219.1690288</a> <a href="index.html#fnref51" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn52" class="footnote-item"><p>Yang, B., &amp; Mitchell, T. (2017). A Joint Sequential and Relational Model for Frame-Semantic Parsing. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. <a href="index.html#fnref52" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn53" class="footnote-item"><p>Katiyar, A., &amp; Cardie, C. (2017). Going out on a limb : Joint Extraction of Entity Mentions and Relations without Dependency Trees. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 917–928). <a href="index.html#fnref53" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn54" class="footnote-item"><p>Liu, X., Gao, J., He, X., Deng, L., Duh, K., &amp; Wang, Y.-Y. (2015). Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval. NAACL-2015, 912–921. <a href="index.html#fnref54" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn55" class="footnote-item"><p>Collobert, R., &amp; Weston, J. (2008). A unified architecture for natural language processing. Proceedings of the 25th International Conference on Machine Learning - ICML ’08, 20(1), 160–167. <a href="https://doi.org/10.1145/1390156.1390177">https://doi.org/10.1145/1390156.1390177</a> <a href="index.html#fnref55" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn56" class="footnote-item"><p>Søgaard, A., &amp; Goldberg, Y. (2016). Deep multi-task learning with low level tasks supervised at lower layers. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 231–235. <a href="index.html#fnref56" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn57" class="footnote-item"><p>Ruder, S., Bingel, J., Augenstein, I., &amp; Søgaard, A. (2017). Sluice networks: Learning what to share between loosely related tasks. arXiv Preprint arXiv:1705.08142. Retrieved from <a href="http://arxiv.org/abs/1705.08142">http://arxiv.org/abs/1705.08142</a> <a href="index.html#fnref57" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn58" class="footnote-item"><p>Balikas, G., &amp; Moura, S. (2017). Multitask Learning for Fine-Grained Twitter Sentiment Analysis. In International ACM SIGIR Conference on Research and Development in Information Retrieval 2017. <a href="index.html#fnref58" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn59" class="footnote-item"><p>Luo, B., Feng, Y., Xu, J., Zhang, X., &amp; Zhao, D. (2017). Learning to Predict Charges for Criminal Cases with Legal Basis. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from <a href="http://arxiv.org/abs/1707.09168">http://arxiv.org/abs/1707.09168</a> <a href="index.html#fnref59" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn60" class="footnote-item"><p>Augenstein, I., &amp; Søgaard, A. (2017). Multi-Task Learning of Keyphrase Boundary Classification. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Retrieved from <a href="http://arxiv.org/abs/1704.00514">http://arxiv.org/abs/1704.00514</a> <a href="index.html#fnref60" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn61" class="footnote-item"><p>Isonuma, M., Fujino, T., Mori, J., Matsuo, Y., &amp; Sakata, I. (2017). Extractive Summarization Using Multi-Task Learning with Document Classification. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 2091–2100). <a href="index.html#fnref61" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
<!--kg-card-end: markdown-->
                </div>
            </section>



        </article>

    </div>
</main>

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
                <article class="read-next-card">
                    <header class="read-next-card-header">
                        <h3><span>More in</span> <a href="../tag/multi-task-learning/index.html">multi-task learning</a></h3>
                    </header>
                    <div class="read-next-card-content">
                        <ul>
                            <li>
                                <h4><a href="../thesis/index.html">Neural Transfer Learning for Natural Language Processing (PhD thesis)</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2019-03-23">23 Mar 2019</time> –
                                        1 min read</p>
                                </div>
                            </li>
                            <li>
                                <h4><a href="../10-exciting-ideas-of-2018-in-nlp/index.html">10 Exciting Ideas of 2018 in NLP</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2018-12-19">19 Dec 2018</time> –
                                        8 min read</p>
                                </div>
                            </li>
                            <li>
                                <h4><a href="../a-review-of-the-recent-history-of-nlp/index.html">A Review of the Neural History of Natural Language Processing</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2018-10-01">1 Oct 2018</time> –
                                        29 min read</p>
                                </div>
                            </li>
                        </ul>
                    </div>
                    <footer class="read-next-card-footer">
                        <a href="../tag/multi-task-learning/index.html">See all 5 posts
                            →</a>
                    </footer>
                </article>

                <article class="post-card post tag-word-embeddings tag-natural-language-processing tag-cross-lingual ">

    <a class="post-card-image-link" href="../word-embeddings-2017/index.html">
        <img class="post-card-image"
            srcset="../content/images/size/w300/2017/10/semantic_change.png 300w,
                   ../content/images/size/w600/2017/10/semantic_change.png 600w,
                  ../content/images/size/w1000/2017/10/semantic_change.png 1000w,
                 ../content/images/size/w2000/2017/10/semantic_change.png 2000w"
            sizes="(max-width: 1000px) 400px, 700px"
            src="../content/images/size/w600/2017/10/semantic_change.png"
            alt="Word embeddings in 2017: Trends and future directions"
        />
    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="../word-embeddings-2017/index.html">

            <header class="post-card-header">
                    <div class="post-card-primary-tag">word embeddings</div>
                <h2 class="post-card-title">Word embeddings in 2017: Trends and future directions</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>Word embeddings are an integral part of current NLP models, but approaches  that supersede the original word2vec have not been proposed. This post focuses on the deficiencies of word embeddings and how recent approaches have tried to resolve them.</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Sebastian Ruder
                    </div>
            
                    <a href="../author/sebastian/index.html" class="static-avatar">
                        <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/sebastian/index.html">Sebastian Ruder</a></span>
                <span class="post-card-byline-date"><time datetime="2017-10-21">21 Oct 2017</time> <span class="bull">&bull;</span> 17 min read</span>
            </div>
        </footer>

    </div>

</article>

                <article class="post-card post tag-natural-language-processing tag-word-embeddings tag-cross-lingual tag-events ">

    <a class="post-card-image-link" href="../highlights-emnlp-2017/index.html">
        <img class="post-card-image"
            srcset="../content/images/size/w300/2017/09/emnlp_landscape.jpg 300w,
                   ../content/images/size/w600/2017/09/emnlp_landscape.jpgg 600w,
                  ../content/images/size/w1000/2017/09/emnlp_landscape.jpg 1000w,
                 ../content/images/size/w2000/2017/09/emnlp_landscape.jpg 2000w"
            sizes="(max-width: 1000px) 400px, 700px"
            src="../content/images/size/w600/2017/09/emnlp_landscape.jpg"
            alt="Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more"
        />
    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="../highlights-emnlp-2017/index.html">

            <header class="post-card-header">
                    <div class="post-card-primary-tag">natural language processing</div>
                <h2 class="post-card-title">Highlights of EMNLP 2017: Exciting datasets, return of the clusters, and more</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>This post discusses highlights of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017). These include exciting datasets, new cluster-based methods, distant supervision, data selection, character-level models, and many more.</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Sebastian Ruder
                    </div>
            
                    <a href="../author/sebastian/index.html" class="static-avatar">
                        <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/sebastian/index.html">Sebastian Ruder</a></span>
                <span class="post-card-byline-date"><time datetime="2017-09-22">22 Sep 2017</time> <span class="bull">&bull;</span> 10 min read</span>
            </div>
        </footer>

    </div>

</article>
        </div>
    </div>
</aside>




        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://ruder.io">Sebastian Ruder</a> &copy; 2020</section>
                <nav class="site-footer-nav">
                    <a href="https://ruder.io">Latest Posts</a>
                    
                    <a href="https://twitter.com/seb_ruder" target="_blank" rel="noopener">Twitter</a>
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>


    <script
        src="https://code.jquery.com/jquery-3.4.1.min.js"
        integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
        crossorigin="anonymous">
    </script>
    <script src="../assets/built/casper.js?v=6133c8c0f7"></script>

    <script>
        // Parse the URL parameter
        function getParameterByName(name, url) {
            if (!url) url = window.location.href;
            name = name.replace(/[\[\]]/g, "\\$&");
            var regex = new RegExp("[?&]" + name + "(=([^&#]*)|&|#|$)"),
                results = regex.exec(url);
            if (!results) return null;
            if (!results[2]) return '';
            return decodeURIComponent(results[2].replace(/\+/g, " "));
        }

        // Give the parameter a variable name
        var action = getParameterByName('action');

        $(document).ready(function () {
            if (action == 'subscribe') {
                $('body').addClass("subscribe-success");
            }

            $('.subscribe-success-message .subscribe-close').click(function () {
                $('.subscribe-success-message').addClass('close');
            });

            // Reset form on opening subscrion overlay
            $('.subscribe-button').click(function() {
                $('.subscribe-overlay form').removeClass();
                $('.subscribe-email').val('');
            });
        });
    </script>

    <script>
    $(document).ready(function () {
        // FitVids - start
        var $postContent = $(".post-full-content");
        $postContent.fitVids();
        // FitVids - end

        // Replace nav with title on scroll - start
        Casper.stickyNavTitle({
            navSelector: '.site-nav-main',
            titleSelector: '.post-full-title',
            activeClass: 'nav-post-title-active'
        });
        // Replace nav with title on scroll - end

        // Hover on avatar
        var hoverTimeout;
        $('.author-list-item').hover(function () {
            var $this = $(this);

            clearTimeout(hoverTimeout);

            $('.author-card').removeClass('hovered');
            $(this).children('.author-card').addClass('hovered');

        }, function () {
            var $this = $(this);

            hoverTimeout = setTimeout(function () {
                $this.children('.author-card').removeClass('hovered');
            }, 800);
        });
    });
</script>


    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/zepto/1.1.6/zepto.min.js"></script>
<script>jQuery = Zepto</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism.min.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/plugins/line-numbers/prism-line-numbers.min.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism-coy.min.css" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/components/prism-python.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-60512592-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-60512592-1');
</script>

</body>
</html>

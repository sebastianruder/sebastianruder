<!DOCTYPE html>
<html lang="en">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>The State of Transfer Learning in NLP</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="stylesheet" type="text/css" href="../assets/built/screen.css?v=d60178fff0" />

    <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon" />
    <link rel="canonical" href="https://ruder.io/state-of-transfer-learning-in-nlp/" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <link rel="amphtml" href="https://ruder.io/state-of-transfer-learning-in-nlp/amp/" />
    
    <meta property="og:site_name" content="Sebastian Ruder" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="The State of Transfer Learning in NLP" />
    <meta property="og:description" content="This post expands on the NAACL 2019 tutorial on Transfer Learning in NLP. It highlights key insights and takeaways and provides updates based on recent work." />
    <meta property="og:url" content="https://ruder.io/state-of-transfer-learning-in-nlp/" />
    <meta property="og:image" content="https://ruder.io/content/images/2019/08/transfer_learning_methods_small.png" />
    <meta property="article:published_time" content="2019-08-18T15:22:00.000Z" />
    <meta property="article:modified_time" content="2020-10-16T21:47:19.000Z" />
    <meta property="article:tag" content="transfer learning" />
    <meta property="article:tag" content="natural language processing" />
    <meta property="article:tag" content="events" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="The State of Transfer Learning in NLP" />
    <meta name="twitter:description" content="This post expands on the NAACL 2019 tutorial on Transfer Learning in NLP. It highlights key insights and takeaways and provides updates based on recent work." />
    <meta name="twitter:url" content="https://ruder.io/state-of-transfer-learning-in-nlp/" />
    <meta name="twitter:image" content="https://ruder.io/content/images/2019/08/transfer_learning_methods_small.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Sebastian Ruder" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="transfer learning, natural language processing, events" />
    <meta name="twitter:site" content="@seb_ruder" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="774" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Sebastian Ruder",
        "url": "https://ruder.io/",
        "logo": {
            "@type": "ImageObject",
            "url": {
                "@type": "ImageObject",
                "url": "https://ruder.io/favicon.ico",
                "width": 48,
                "height": 48
            }
        }
    },
    "author": {
        "@type": "Person",
        "name": "Sebastian Ruder",
        "image": {
            "@type": "ImageObject",
            "url": "https://ruder.io/content/images/2019/02/new_profile_photo_square-1.jpg",
            "width": 2000,
            "height": 2000
        },
        "url": "https://ruder.io/author/sebastian/",
        "sameAs": []
    },
    "headline": "The State of Transfer Learning in NLP",
    "url": "https://ruder.io/state-of-transfer-learning-in-nlp/",
    "datePublished": "2019-08-18T15:22:00.000Z",
    "dateModified": "2020-10-16T21:47:19.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://ruder.io/content/images/2019/08/transfer_learning_methods_small.png",
        "width": 2000,
        "height": 774
    },
    "keywords": "transfer learning, natural language processing, events",
    "description": "This post expands on the NAACL 2019 tutorial on Transfer Learning in NLP. It highlights key insights and takeaways and provides updates based on recent work.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://ruder.io/"
    }
}
    </script>

    <meta name="generator" content="Ghost 3.11" />
    <link rel="alternate" type="application/rss+xml" title="Sebastian Ruder" href="https://ruder.io/rss/" />
    <script>
var profile_title = 'Sebastian Ruder';
</script>
<script>
var disqus_shortname = 'sebastianruder';
</script>
<script>
var profile_resume ='NLP PhD student';
</script>
<script>
var ga_id = 'UA-60512592-1';
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [["$", "$"], ["\\(", "\\)"]],
        processEscapes: true
    }
});
</script>

</head>
<body class="post-template tag-transfer-learning tag-natural-language-processing tag-events">

    <div class="site-wrapper">

        

<header class="site-header">
    <div class="outer site-nav-main">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left-wrapper">
        <div class="site-nav-left">
                <a class="site-nav-logo" href="https://ruder.io">Sebastian Ruder</a>
            <div class="site-nav-content">
                    <ul class="nav" role="menu">
    <li class="nav-about" role="menuitem"><a href="https://ruder.io/about/">About</a></li>
    <li class="nav-tags" role="menuitem"><a href="https://ruder.io/tags/">Tags</a></li>
    <li class="nav-papers" role="menuitem"><a href="https://ruder.io/publications/">Papers</a></li>
    <li class="nav-talks" role="menuitem"><a href="https://ruder.io/talks/">Talks</a></li>
    <li class="nav-news" role="menuitem"><a href="https://ruder.io/news/">News</a></li>
    <li class="nav-faq" role="menuitem"><a href="https://ruder.io/faq/">FAQ</a></li>
    <li class="nav-newsletter" role="menuitem"><a href="https://ruder.io/nlp-news/">Newsletter</a></li>
    <li class="nav-nlp-progress" role="menuitem"><a href="https://nlpprogress.com/">NLP Progress</a></li>
    <li class="nav-media" role="menuitem"><a href="https://ruder.io/media/">Media</a></li>
    <li class="nav-contact" role="menuitem"><a href="https://ruder.io/contact/">Contact</a></li>
</ul>

                    <span class="nav-post-title dash">The State of Transfer Learning in NLP</span>
            </div>
        </div>
    </div>
    <div class="site-nav-right">
            <div class="social-links">
                    <a class="social-link social-link-tw" href="https://twitter.com/seb_ruder" title="Twitter" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
</a>
            </div>
                <a class="rss-button" href="https://ruder.io/rss/index.rss" title="RSS" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
</a>

    </div>
</nav>
    </div>
</div></header>


<main id="site-main" class="site-main outer">
    <div class="inner">

        <article class="post-full post tag-transfer-learning tag-natural-language-processing tag-events ">

            <header class="post-full-header">

                <section class="post-full-tags">
                    <a href="../tag/transfer-learning/index.html">transfer learning</a>
                </section>

                <h1 class="post-full-title">The State of Transfer Learning in NLP</h1>

                <p class="post-full-custom-excerpt">This post expands on the NAACL 2019 tutorial on Transfer Learning in NLP. It highlights key insights and takeaways and provides updates based on recent work.</p>

                <div class="post-full-byline">

                    <section class="post-full-byline-content">

                        <ul class="author-list">
                            <li class="author-list-item">

                                <div class="author-card">
                                    <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                                    <div class="author-info">
                                        <h2>Sebastian Ruder</h2>
                                        <p>Read <a href="../author/sebastian/index.html">more posts</a> by this author.</p>
                                    </div>
                                </div>

                                <a href="../author/sebastian/index.html" class="author-avatar">
                                    <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                                </a>

                            </li>
                        </ul>

                        <section class="post-full-byline-meta">
                            <h4 class="author-name"><a href="../author/sebastian/index.html">Sebastian Ruder</a></h4>
                            <div class="byline-meta-content">
                                <time class="byline-meta-date" datetime="2019-08-18">18 Aug 2019</time>
                                <span class="byline-reading-time"><span class="bull">&bull;</span> 15 min read</span>
                            </div>
                        </section>

                    </section>


                </div>
            </header>

            <figure class="post-full-image">
                <img
                    srcset="../content/images/size/w300/2019/08/transfer_learning_methods_small.png 300w,
                           ../content/images/size/w600/2019/08/transfer_learning_methods_small.png 600w,
                          ../content/images/size/w1000/2019/08/transfer_learning_methods_small.png 1000w,
                         ../content/images/size/w2000/2019/08/transfer_learning_methods_small.png 2000w"
                    sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px"
                    src="../content/images/size/w2000/2019/08/transfer_learning_methods_small.png"
                    alt="The State of Transfer Learning in NLP"
                />
            </figure>

            <section class="post-full-content">
                <div class="post-content">
                    <p>Update 16.10.2020: Added <a href="https://www.infoq.cn/article/zD5QkcIzF9253friWPVd">Chinese</a> and <a href="https://www.ibidemgroup.com/edu/traduccion-aprendizaje-pnl/">Spanish</a> translations.</p><p>This post expands on the <a href="http://tiny.cc/NAACLTransfer">NAACL 2019 tutorial on Transfer Learning in NLP</a>. The tutorial was organized by Matthew Peters, Swabha Swayamdipta, Thomas Wolf, and me. In this post, I highlight key insights and takeaways and provide updates based on recent work. You can see the structure of this post below:</p><figure class="kg-card kg-image-card"><img src="https://ruder.io/content/images/2019/08/agenda.png" class="kg-image"></figure><p>The <a href="http://tiny.cc/NAACLTransfer">slides</a>, a <a href="http://tiny.cc/NAACLTransferColab">Colaboratory notebook</a>, and <a href="http://tiny.cc/NAACLTransferCode">code</a> of the tutorial are available online.</p><h1 id="introduction">Introduction</h1><p>For an overview of what transfer learning is, have a look at <a href="http://ruder.io/transfer-learning/">this blog post</a>. Our go-to definition throughout this post will be the following, which is illustrated in the diagram below:</p><blockquote>Transfer learning is a means to extract knowledge from a source setting and apply it to a different target setting.</blockquote><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/08/transfer_learning_scenario.png" class="kg-image"><figcaption>An illustration of the process of transfer learning.</figcaption></figure><p>In the span of little more than a year, transfer learning in the form of pretrained language models has <a href="https://thegradient.pub/nlp-imagenet/">become ubiquitous in NLP</a> and has contributed to the state of the art on a wide range of tasks. However, transfer learning is not a recent phenomenon in NLP. One illustrative example is progress on the task of Named Entity Recognition (NER), which can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/08/ner_results.png" class="kg-image"><figcaption>Performance on Named Entity Recognition (NER) on CoNLL-2003 (English) over time.</figcaption></figure><p>Throughout its history, most of the major improvements on this task have been driven by different forms of transfer learning: from early self-supervised learning with auxiliary tasks (<a href="http://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf">Ando and Zhang, 2005</a>) and phrase &amp; word clusters (<a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35520.pdf">Lin and Wu, 2009</a>) to the language model embeddings (<a href="https://arxiv.org/pdf/1705.00108.pdf">Peters et al., 2017</a>) and pretrained language models (<a href="https://aclweb.org/anthology/N18-1202">Peters et al., 2018</a>; <a href="https://alanakbik.github.io/papers/coling2018.pdf">Akbik et al., 2018</a>; <a href="https://arxiv.org/abs/1903.07785">Baevski et al., 2019</a>) of recent years.</p><p>There are different types of transfer learning common in current NLP. These can be roughly classified along three dimensions based on a) whether the source and target settings deal with the same task; and b) the nature of the source and target domains; and c) the order in which the tasks are learned. A taxonomy that highlights the variations can be seen below:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/08/transfer_learning_taxonomy.png" class="kg-image"><figcaption>A taxonomy for transfer learning in NLP (<a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=64">Ruder, 2019</a>).</figcaption></figure><p>Sequential transfer learning is the form that has led to the biggest improvements so far. The general practice is to pretrain representations on a large unlabelled text corpus using your method of choice and then to adapt these representations to a supervised target task using labelled data as can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/08/pretraining_adaptation.png" class="kg-image"><figcaption>The general procedure of sequential transfer learning.</figcaption></figure><h3 id="major-themes">Major themes</h3><p>Several major themes can be observed in how this paradigm has been applied:</p><p><strong>From words to words-in-context</strong>  Over time, representations incorporate more context. Early approaches such as word2vec (<a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et al., 2013</a>) learned a single representation for every word independent of its context. Later approaches then scaled these representations to sentences and documents (<a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf">Le and Mikolov, 2014</a>; <a href="https://arxiv.org/abs/1705.02364">Conneau et al., 2017</a>). Current approaches learn word representations that change based on the word's context (<a href="http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf">McCann et al., 2017</a>; <a href="https://aclweb.org/anthology/N18-1202">Peters et al., 2018</a>). </p><p><strong>LM pretraining</strong>   Many successful pretraining approaches are based on variants of language modelling (LM). Advantages of LM are that it does not require any human annotation and that many languages have enough text available to learn reasonable models. In addition, LM is versatile and enables learning both sentence and word representations with a variety of objective functions.</p><p><strong>From shallow to deep</strong>  Over the last years, state-of-the-art models in NLP have become progressively deeper. Up to two years ago, the state of the art on most tasks was a 2-3 layer deep BiLSTM, with machine translation being an outlier with 16 layers (<a href="https://arxiv.org/abs/1609.08144">Wu et al., 2016</a>). In contrast, current models like BERT-Large and GPT-2 consist of 24 Transformer blocks and recent models are even deeper.</p><p><strong>Pretraining vs target task</strong>  The choice of pretraining and target tasks is closely intertwined. For instance, sentence representations are not useful for word-level predictions, while span-based pretraining is important for span-level predictions. On the whole, for the best target performance, it is beneficial to choose a similar pretraining task.</p><h1 id="pretraining">Pretraining</h1><h3 id="why-does-language-modelling-work-so-well">Why does language modelling work so well?</h3><p>The remarkable success of pretrained language models is surprising. One reason for the success of language modelling may be that it is a very difficult task, even for humans. To have any chance at solving this task, a model is required to learn about syntax, semantics, as well as certain facts about the world. Given enough data, a large number of parameters, and enough compute, a model can do a reasonable job. Empirically, language modelling works better than other pretraining tasks such as translation or autoencoding (<a href="https://arxiv.org/abs/1809.10040">Zhang et al. 2018</a>; <a href="https://www.aclweb.org/anthology/P19-1439">Wang et al., 2019</a>).</p><p>A recent predictive-rate distortion (PRD) analysis of human language (<a href="http://socsci.uci.edu/~rfutrell/papers/hahn2019estimating.pdf">Hahn and Futrell, 2019</a>) suggests that human language—and language modelling—has infinite statistical complexity but that it can be approximated well at lower levels. This observation has two implications: 1) We can obtain good results with comparatively small models; and 2) there is a lot of potential for scaling up our models. For both implications we have empirical evidence, as we can see in the next sections.</p><h3 id="sample-efficiency">Sample efficiency</h3><p>One of the main benefits of pretraining is that it reduces the need for annotated data. In practice, transfer learning has often been shown to achieve similar performance compared to a non-pretrained model with 10x fewer examples or more as can be seen below for ULMFiT (<a href="https://aclweb.org/anthology/P18-1031">Howard and Ruder, 2018</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/08/sample_efficiency.png" class="kg-image"><figcaption>Performance of a model trained from scratch (blue) vs. two pretrained models fine-tuned on labelled target data (orange) as well as unlabelled target data (green) respectively (<a href="https://aclweb.org/anthology/P18-1031">Howard and Ruder, 2018</a>).</figcaption></figure><h3 id="scaling-up-pretraining">Scaling up pretraining</h3><p>Pretrained representations can generally be improved by jointly increasing the number of model parameters and the amount of pretraining data. Returns start to diminish as the amount of pretraining data grows huge. Current performance curves such as the one below, however, do not indicate that we have reached a plateau. We can thus expect to see even bigger models trained on more data. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/08/scaling_up_pretraining.png" class="kg-image"><figcaption>Average GLUE score with different amounts of Common Crawl data for pretraining (<a href="https://arxiv.org/abs/1903.07785">Baevski et al., 2019</a>).&nbsp;</figcaption></figure><p>Recent examples of this trend are <a href="https://arxiv.org/abs/1907.12412">ERNIE 2.0</a>, <a href="https://arxiv.org/abs/1906.08237">XLNet</a>, <a href="https://devblogs.nvidia.com/training-bert-with-gpus/">GPT-2 8B</a>, and <a href="https://arxiv.org/abs/1907.11692">RoBERTa</a>. The latter in particular finds that simply training BERT for longer and on more data improves results, while GPT-2 8B reduces perplexity on a language modelling dataset (though only by a comparatively small factor).</p><h3 id="cross-lingual-pretraining">Cross-lingual pretraining</h3><p>A major promise of pretraining is that it can help us bridge the digital language divide and can enable us learn NLP models for more of the world's 6,000 languages. Much work on cross-lingual learning has focused on training separate word embeddings in different languages and learning to align them (<a href="https://www.jair.org/index.php/jair/article/view/11640">Ruder et al., 2019</a>). In the same vein, we can learn to align contextual representations (<a href="https://www.aclweb.org/anthology/N19-1162">Schuster et al., 2019</a>). Another common method is to share a subword vocabulary and train one model on many languages (<a href="https://www.aclweb.org/anthology/N19-1423">Devlin et al., 2019</a>; <a href="https://arxiv.org/abs/1812.10464">Artetxe and Schwenk, 2019</a>; <a href="https://www.aclweb.org/anthology/N19-1392">Mulcaire et al., 2019</a>; <a href="https://arxiv.org/abs/1901.07291">Lample and Conneau, 2019</a>). While this is easy to implement and is a strong cross-lingual baseline, it leads to under-representation of low-resource languages (<a href="https://www.aclweb.org/anthology/P19-1027">Heinzerling and Strube, 2019</a>). Multilingual BERT in particular has been the subject of much recent attention (<a href="https://www.aclweb.org/anthology/P19-1493">Pires et al., 2019</a>; <a href="https://arxiv.org/abs/1904.09077">Wu and Dredze, 2019</a>). Despite its strong zero-shot performance, dedicated monolingual language models often are competitive, while being more efficient (<a href="http://ruder.io/publications/">Eisenschlos et al., 2019</a>).</p><h3 id="practical-considerations">Practical considerations</h3><p>Pretraining is cost-intensive. Pretraining the Transformer-XL style model we used in the tutorial takes 5h–20h on 8 V100 GPUs (a few days with 1 V100) to reach a good perplexity. Sharing pretrained models is thus very important. Pretraining is relatively robust to the choice of hyper-parameters—apart from needing a learning rate warm-up for transformers. As a general rule, your model should not have enough capacity to overfit if your dataset is large enough. Masked language modeling (as in BERT) is typically 2-4 times slower to train than standard LM as masking only a fraction of words yields a smaller signal.</p><h1 id="what-is-in-a-representation">What is in a representation?</h1><p>Representations have been shown to be predictive of certain linguistic phenomena such as alignments in translation or syntactic hierarchies. Better performance has been achieved when pretraining with syntax; even when syntax is not explicitly encoded, representations still learn some notion of syntax (<a href="https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00019">Williams et al. 2018</a>). Recent work has furthermore shown that knowledge of syntax can be distilled efficiently into state-of-the-art models (<a href="https://www.aclweb.org/anthology/P19-1337">Kuncoro et al., 2019</a>). Network architectures generally determine what is in a representation. For instance, BERT has been observed to capture syntax (<a href="https://arxiv.org/abs/1905.05950">Tenney et al., 2019</a>; <a href="https://arxiv.org/abs/1901.05287">Goldberg, 2019</a>). Different architectures show different layer-wise trends in terms of what information they capture (<a href="https://www.aclweb.org/anthology/N19-1112">Liu et al., 2019</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/08/probing_setup.png" class="kg-image"><figcaption>The general setup in probing tasks used to study linguistic knowledge within contextual word representations (<a href="https://www.aclweb.org/anthology/N19-1112">Liu et al., 2019</a>).</figcaption></figure><p>The information that a model captures also depends how you look at it: Visualizing activations or attention weights provides a bird's eye view of the model's knowledge, but focuses on a few samples; probes that train a classifier on top of learned representations in order to predict certain properties (as can be seen above) discover corpus-wide specific characteristics, but may introduce their own biases; finally, network ablations are great for improving the model, but may be task-specific.</p><h1 id="adaptation">Adaptation</h1><p>For adapting a pretrained model to a target task, there are several orthogonal directions we can make decisions on: architectural modifications, optimization schemes, and whether to obtain more signal.</p><h2 id="architectural-modifications">Architectural modifications</h2><p>For architectural modifications, the two general options we have are:</p><p><strong>a) Keep the pretrained model internals unchanged</strong>  This can be as simple as adding one or more linear layers on top of a pretrained model, which is commonly done with BERT. Instead, we can also use the model output as input to a separate model, which is often beneficial when a target task requires interactions that are not available in the pretrained embedding, such as span representations or modelling cross-sentence relations.</p><p><strong>b) Modify the pretrained model internal architecture</strong>  One reason why we might want to do this is in order to adapt to a structurally different target task such as one with several input sequences. In this case, we can use the pretrained model to initialize as much as possible of a structurally different target task model. We might also want to apply task-specific modifications such as adding skip or residual connections or attention. Finally, modifying the target task parameters may reduce the number of parameters that need to be fine-tuned by adding bottleneck modules (“adapters”) between the layers of the pretrained model (<a href="https://arxiv.org/abs/1902.00751">Houlsby et al., 2019</a>; <a href="https://arxiv.org/abs/1902.02671">Stickland and Murray, 2019</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/08/adapter_layer_small.png" class="kg-image"><figcaption>An adapter layer (right) as used in a Transformer block (left) (<a href="https://arxiv.org/abs/1902.00751">Houlsby et al., 2019</a>).</figcaption></figure><h2 id="optimization-schemes">Optimization schemes</h2><p>In terms of optimizing the model, we can choose which weights we should update and how and when to update those weights.</p><h3 id="which-weights-to-update">Which weights to update</h3><p>For updating the weights, we can either tune or not tune (the pretrained weights):</p><p><strong>a) Do not change the pretrained weights (feature extraction)</strong>  In practice, a linear classifier is trained on top of the pretrained representations. The best performance is typically achieved by using the representation not just of the top layer, but learning a linear combination of layer representations (<a href="https://arxiv.org/abs/1802.05365">Peters et al., 2018</a>, <a href="https://arxiv.org/abs/1705.08142">Ruder et al., 2019</a>). Alternatively, pretrained representations can be used as features in a downstream model. When adding adapters, only the adapter layers are trained.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/08/feature_extraction.png" class="kg-image"><figcaption>Use of a pretrained model as features in a separate downstream model.</figcaption></figure><p><strong>b) Change the pretrained weights (fine-tuning)</strong>  The pretrained weights are used as initialization for parameters of the downstream model. The whole pretrained architecture is then trained during the adaptation phase.</p><h3 id="how-and-when-to-update-the-weights">How and when to update the weights</h3><p>The main motivation for choosing the order and how to update the weights is that we want to avoid overwriting useful pretrained information and maximize positive transfer. Related to this is the concept of catastrophic forgetting (<a href="https://www.sciencedirect.com/science/article/pii/S0079742108605368">McCloskey &amp; Cohen, 1989</a>; <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.480.7627&amp;rep=rep1&amp;type=pdf">French, 1999</a>), which occurs if a model forgets the task it was originally trained on. In most settings, we only care about the performance on the target task, but this may differ depending on the application.</p><p>A guiding principle for updating the parameters of our model is to update them progressively from top-to-bottom in time, in intensity, or compared to a pretrained model:</p><p><strong>a) Progressively in time (freezing)</strong>  The main intuition is that training all layers at the same time on data of a different distribution and task may lead to instability and poor solutions. Instead, we train layers individually to give them time to adapt to the new task and data. This goes back to layer-wise training of early deep neural networks (<a href="https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">Hinton et al., 2006</a>; <a href="https://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf">Bengio et al., 2007</a>). Recent approaches (<a href="https://www.aclweb.org/anthology/D17-1169">Felbo et al., 2017</a>; <a href="https://arxiv.org/abs/1801.06146">Howard and Ruder, 2018</a>; <a href="https://arxiv.org/abs/1902.10547">Chronopoulou et al., 2019</a>) mostly vary in the combinations of layers that are trained together; all train all parameters jointly in the end. Unfreezing has not been investigated in detail for Transformer models.</p><p><strong>b) Progressively in intensity (lower learning rates)</strong>  We want to use lower learning rates to avoid overwriting useful information. Lower learning rates are particularly important in lower layers (as they capture more general information), early in training (as the model still needs to adapt to the target distribution), and late in training (when the model is close to convergence). To this end, we can use discriminative fine-tuning (<a href="https://aclweb.org/anthology/P18-1031">Howard and Ruder, 2018</a>), which decays the learning rate for each layer as can be seen below. In order to maintain lower learning rates early in training, a triangular learning rate schedule can be used, which is also known as learning rate warm-up in Transformers. <a href="https://arxiv.org/abs/1908.03265">Liu et al. (2019)</a> recently suggest that warm-up reduces variance in the early stage of training.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/08/discriminative_fine_tuning-1.png" class="kg-image"><figcaption>Discriminative fine-tuning (<a href="https://aclweb.org/anthology/P18-1031">Howard and Ruder, 2018</a>).</figcaption></figure><p><strong>c) Progressively vs. a pretrained model (regularization)</strong>  One way to minimize catastrophic forgetting is to encourage target model parameters to stay close to the parameters of the pretrained model using a regularization term (<a href="https://www.aclweb.org/anthology/K17-1029">Wiese et al., CoNLL 2017</a>, <a href="https://www.pnas.org/content/114/13/3521">Kirkpatrick et al., PNAS 2017</a>).</p><h2 id="trade-offs-and-practical-considerations">Trade-offs and practical considerations</h2><p>In general, the more parameters you need to train from scratch the slower your training will be. Feature extraction requires adding more parameters than fine-tuning (<a href="https://arxiv.org/abs/1903.05987">Peters et al., 2019</a>), so is typically slower to train. Feature extraction, however, is more space-efficient when a model needs to be adapted to many tasks as it only requires storing one copy of the pretrained model in memory. Adapters strike a balance by adding a small number of additional parameters per task.</p><p>In terms of performance, no adaptation method is clearly superior in every setting. If source and target tasks are dissimilar, feature extraction seems to be preferable (<a href="https://arxiv.org/abs/1903.05987">Peters et al., 2019</a>). Otherwise, feature extraction and fine-tuning often perform similar, though this depends on the budget available for hyper-parameter tuning (fine-tuning may often require a more extensive hyper-parameter search). Anecdotally, Transformers are easier to fine-tune (less sensitive to hyper-parameters) than LSTMs and may achieve better performance with fine-tuning.</p><p>However, large pretrained models (e.g. BERT-Large) are prone to degenerate performance when fine-tuned on tasks with small training sets. In practice, the observed behavior is often “on-off”: the model either works very well or does not work at all as can be seen in the figure below. Understanding the conditions and causes of this behavior is an open research question.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://ruder.io/content/images/2019/08/sentence_encoders_on_stilts_5k_examples.png" class="kg-image"><figcaption>Distribution of task scores across 20 random restarts for BERT (red) and BERT that was fine-tuned on MNLI (green) when fine-tuning on no more than 5k examples for each task (<a href="https://arxiv.org/abs/1811.01088">Phang et al., 2018</a>).</figcaption></figure><h2 id="getting-more-signal">Getting more signal</h2><p>The target task is often a low-resource task. We can often improve the performance of transfer learning by combining a diverse set of signals:</p><p><strong>Sequential adaptation</strong>  If related tasks are available, we can fine-tune our model first on a related task with more data before fine-tuning it on the target task. This<br>helps particularly for tasks with limited data and similar tasks (<a href="https://arxiv.org/abs/1811.01088v2">Phang et al., 2018</a>) and improves sample efficiency on the target task (<a href="https://arxiv.org/abs/1901.11373">Yogatama et al., 2019</a>).</p><p><strong>Multi-task fine-tuning</strong>  Alternatively, we can also fine-tune the model jointly on related tasks together with the target task. The related task can also be an unsupervised auxiliary task. Language modelling is a good choice for this and has been shown to help even without pretraining (<a href="https://arxiv.org/abs/1704.07156">Rei et al., 2017</a>). The task ratio can optionally be annealed to de-emphasize the auxiliary task towards the end of training (<a href="https://arxiv.org/abs/1902.10547">Chronopoulou et al., NAACL 2019</a>). Language model fine-tuning is used as a separate step in ULMFiT (<a href="https://aclweb.org/anthology/P18-1031">Howard and Ruder, 2018</a>). Recently, multi-task fine-tuning has led to improvements even with many target tasks (<a href="https://arxiv.org/abs/1901.11504">Liu et al., 2019</a>, <a href="https://www.aclweb.org/anthology/P19-1439">Wang et al., 2019</a>).</p><p><strong>Dataset slicing</strong>  Rather than fine-tuning with auxiliary tasks, we can use <a href="https://dawn.cs.stanford.edu/2019/03/22/glue/">auxiliary heads that are trained only on particular subsets of the data</a>. To this end, we would first analyze the errors of the model, use heuristics to automatically identify challenging subsets of the training data, and then train auxiliary heads jointly with main head.</p><p><strong>Semi-supervised learning</strong>  We can also use semi-supervised learning methods to make our model's predictions more consistent by perturbing unlabelled examples. The perturbation can be noise, masking (<a href="https://arxiv.org/abs/1809.08370">Clark et al., 2018</a>), or data augmentation, e.g. back-translation (<a href="https://arxiv.org/abs/1904.12848">Xie et al., 2019</a>).</p><p><strong>Ensembling</strong>  To improve performance the predictions of models fine-tuned with different hyper-parameters, fine-tuned with different pretrained models, or trained on different target tasks or dataset splits may be combined.</p><p><strong>Distilling</strong>  Finally, large models or ensembles of models may be distilled into a single, smaller model. The model can also be a lot simpler (Tang et al., 2019) or have a different inductive bias (<a href="https://www.aclweb.org/anthology/P19-1337">Kuncoro et al., 2019</a>). Multi-task fine-tuning can also be combined with distillation (<a href="https://arxiv.org/abs/1907.04829">Clark et al., 2019</a>).</p><h1 id="down-stream-applications">Down-stream applications</h1><p>Pretraining large-scale models is costly, not only in terms of computation but also in terms of the environmental impact (<a href="https://www.aclweb.org/anthology/P19-1355">Strubell et al., 2019</a>). Whenever possible, it's best to use open-source models. If you need to train your own models, please share your pretrained models with the community.</p><h3 id="frameworks-and-libraries">Frameworks and libraries</h3><p>For sharing and accessing pretrained models, different options are available:</p><p><strong>Hubs</strong>  Hubs are central repositories that provide a common API for accessing pretrained models. The two most common hubs are <a href="https://www.tensorflow.org/hub">TensorFlow Hub</a> and <a href="https://pytorch.org/hub">PyTorch Hub</a>. Hubs are generally simple to use; however, they act more like a black-box as the source code of the model cannot be easily accessed. In addition, modifying the internals of a pretrained model architecture can be difficult.</p><p><strong>Author released checkpoints</strong>  Checkpoint files generally contain all the weights of a pretrained model. In contrast to hub modules, the model graph still needs to be created and model weights need to be loaded separately. As such, checkpoint files are more difficult to use than hub modules, but provide you with full control over the model internals.</p><p><strong>Third-party libraries</strong>  Some third-party libraries like <a href="https://allennlp.org/">AllenNLP</a>, <a href="https://github.com/fastai/fastai">fast.ai</a>, and <a href="https://github.com/huggingface/pytorch-transformers">pytorch-transformers</a> provide easy access to pretrained models. Such libraries typically enable fast experimentation and cover many standard use cases for transfer learning.</p><p>For examples of how such models and libraries can be used for downstream tasks, have a look at the <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g569f436ced_0_30">code snippets</a> in the slides, the <a href="http://tiny.cc/NAACLTransferColab">Colaboratory notebook</a>, and the <a href="http://tiny.cc/NAACLTransferCode">code</a>.</p><h1 id="open-problems-and-future-directions">Open problems and future directions</h1><p>There are many open problems and interesting future research directions. Below is just an updated selection. For more pointers, have a look at <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g569f436ced_0_34">the slides</a>.</p><h3 id="shortcomings-of-pretrained-language-models">Shortcomings of pretrained language models</h3><p>Pretrained language models are still bad at fine-grained linguistic tasks (<a href="https://arxiv.org/abs/1903.08855">Liu et al., 2019</a>), hierarchical syntactic reasoning (<a href="https://www.aclweb.org/anthology/P19-1337">Kuncoro et al., 2019</a>), and common sense (when you actually make it difficult; <a href="https://arxiv.org/abs/1905.07830">Zellers et al., 2019</a>). They still fail at natural language generation, in particular maintaining long-term dependencies, relations, and coherence. They also tend to overfit to surface form information when fine-tuned and can still mostly be seen as ‘rapid surface learners’.</p><p>As we have noted above, particularly large models that are fine-tuned on small amounts of data are difficult to optimize and suffer from high variance. Current pretrained language models are also very large. Distillation and pruning are two ways to deal with this.</p><h3 id="pretraining-tasks">Pretraining tasks</h3><p>While the language modelling objective has shown to be effective empirically, it has its weaknesses. Lately, we have seen that bidirectional context and modelling contiguous word sequences is particularly important. Maybe most importantly, language modelling encourages a focus on syntax and word co-occurrences and only provides a weak signal for capturing semantics and long-term context. We can take inspiration from other forms of self-supervision. In addition, we can design specialized pretraining tasks that explicitly learn certain relationships (<a href="https://arxiv.org/abs/1907.10529">Joshi et al., 2019</a>, <a href="https://arxiv.org/abs/1907.12412">Sun et al., 2019</a>).</p><p>On the whole, it is difficult to learn certain types of information from raw text. Recent approaches incorporate structured knowledge (<a href="http://arxiv.org/abs/1905.07129">Zhang et al., 2019</a>; <a href="https://www.aclweb.org/anthology/P19-1598">Logan IV et al., 2019</a>) or leverage multiple modalities (<a href="https://arxiv.org/abs/1904.01766">Sun et al., 2019</a>; <a href="https://arxiv.org/abs/1908.02265">Lu et al., 2019</a>) as two potential ways to mitigate this problem.</p><h2 id="citation">Citation</h2><p> If you found this post helpful, consider citing <a href="https://www.aclweb.org/anthology/N19-5004">the tutorial</a> as:</p><pre><code>@inproceedings{ruder2019transfer,
  title={Transfer Learning in Natural Language Processing},
  author={Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials},
  pages={15--18},
  year={2019}
}</code></pre><h3 id="translations">Translations</h3><p>This article has been translated into the following languages:</p><ul><li><a href="https://www.infoq.cn/article/zD5QkcIzF9253friWPVd">Chinese</a> (by Sambodhi)</li><li><a href="https://www.ibidemgroup.com/edu/traduccion-aprendizaje-pnl/">Spanish</a> (by Ibidem Group)</li></ul>
                </div>

<h2 id="citation">Newsletter</h2>

<style>
input {
  color: black;
}
</style>

If you want to receive regular updates about advances in machine learning and natural language processing, subscribe to <a href="https://newsletter.ruder.io/">my newsletter</a> below. 

<div id="revue-embed">
  <form action="https://newsletter.ruder.io/add_subscriber" method="post" id="revue-form" name="revue-form" $
  <div class="revue-form-group">
    <label for="member_email">Email address: </label>
    <input class="revue-form-field" placeholder="Your email address" type="email" name="member[email]" id="$
  </div>
  <div class="revue-form-group">
    <label for="member_first_name">First name <span class="optional">(Optional)</span>:</label>
    <input class="revue-form-field" placeholder="First name " type="text" name="member[first_name]" id="memb$
  </div>
  <div class="revue-form-group">
    <label for="member_last_name">Last name <span class="optional">(Optional)</span>:</label>
    <input class="revue-form-field" placeholder="Last name" type="text" name="member[last_name]" id="member$
  </div>
  <div class="revue-form-actions">
    <input type="submit" value="Subscribe" name="member[subscribe]" id="member_submit">
  </div>
  </form>
</div>

            </section>



        </article>

    </div>
</main>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = '/state-of-transfer-learning-in-nlp/';  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "ghost-5bd1da94c414817bfe66d332"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://EXAMPLE.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                            

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
                <article class="read-next-card">
                    <header class="read-next-card-header">
                        <h3><span>More in</span> <a href="../tag/transfer-learning/index.html">transfer learning</a></h3>
                    </header>
                    <div class="read-next-card-content">
                        <ul>
                            <li>
                                <h4><a href="../nlp-benchmarking/index.html">Challenges and Opportunities in NLP Benchmarking</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2021-08-23">23 Aug 2021</time> –
                                        16 min read</p>
                                </div>
                            </li>
                            <li>
                                <h4><a href="../recent-advances-lm-fine-tuning/index.html">Recent Advances in Language Model Fine-tuning</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2021-02-24">24 Feb 2021</time> –
                                        13 min read</p>
                                </div>
                            </li>
                            <li>
                                <h4><a href="../research-highlights-2020/index.html">ML and NLP Research Highlights of 2020</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2021-01-19">19 Jan 2021</time> –
                                        15 min read</p>
                                </div>
                            </li>
                        </ul>
                    </div>
                    <footer class="read-next-card-footer">
                        <a href="../tag/transfer-learning/index.html">See all 19 posts
                            →</a>
                    </footer>
                </article>

                <article class="post-card post tag-cross-lingual tag-transfer-learning tag-natural-language-processing ">

    <a class="post-card-image-link" href="../unsupervised-cross-lingual-learning/index.html">
        <img class="post-card-image"
            srcset="../content/images/size/w300/2019/11/unsupervised_multilingual_overview.png 300w,
                   ../content/images/size/w600/2019/11/unsupervised_multilingual_overview.png 600w,
                  ../content/images/size/w1000/2019/11/unsupervised_multilingual_overview.png 1000w,
                 ../content/images/size/w2000/2019/11/unsupervised_multilingual_overview.png 2000w"
            sizes="(max-width: 1000px) 400px, 700px"
            src="../content/images/size/w600/2019/11/unsupervised_multilingual_overview.png"
            alt="Unsupervised Cross-lingual Representation Learning"
        />
    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="../unsupervised-cross-lingual-learning/index.html">

            <header class="post-card-header">
                    <div class="post-card-primary-tag">cross-lingual</div>
                <h2 class="post-card-title">Unsupervised Cross-lingual Representation Learning</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>This post expands on the ACL 2019 tutorial on Unsupervised Cross-lingual Representation Learning. It highlights key insights and takeaways and provides updates based on recent work, particularly unsupervised deep multilingual models.</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Sebastian Ruder
                    </div>
            
                    <a href="../author/sebastian/index.html" class="static-avatar">
                        <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/sebastian/index.html">Sebastian Ruder</a></span>
                <span class="post-card-byline-date"><time datetime="2019-10-26">26 Oct 2019</time> <span class="bull">&bull;</span> 20 min read</span>
            </div>
        </footer>

    </div>

</article>

                <article class="post-card post tag-events tag-natural-language-processing ">

    <a class="post-card-image-link" href="../eurnlp/index.html">
        <img class="post-card-image"
            srcset="../content/images/size/w300/2019/07/EurNLP-ban_2.jpg 300w,
                   ../content/images/size/w600/2019/07/EurNLP-ban_2.jpgg 600w,
                  ../content/images/size/w1000/2019/07/EurNLP-ban_2.jpg 1000w,
                 ../content/images/size/w2000/2019/07/EurNLP-ban_2.jpg 2000w"
            sizes="(max-width: 1000px) 400px, 700px"
            src="../content/images/size/w600/2019/07/EurNLP-ban_2.jpg"
            alt="EurNLP"
        />
    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="../eurnlp/index.html">

            <header class="post-card-header">
                    <div class="post-card-primary-tag">events</div>
                <h2 class="post-card-title">EurNLP</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>The first European NLP Summit (EurNLP) will take place in London on October 11, 2019. It is an opportunity to foster discussion and collaboration between researchers in and around Europe.</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Sebastian Ruder
                    </div>
            
                    <a href="../author/sebastian/index.html" class="static-avatar">
                        <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/sebastian/index.html">Sebastian Ruder</a></span>
                <span class="post-card-byline-date"><time datetime="2019-07-04">4 Jul 2019</time> <span class="bull">&bull;</span> 2 min read</span>
            </div>
        </footer>

    </div>

</article>
        </div>
    </div>
</aside>




        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://ruder.io">Sebastian Ruder</a> &copy; 2022</section>
                <nav class="site-footer-nav">
                    <a href="https://ruder.io">Latest Posts</a>
                    
                    <a href="https://twitter.com/seb_ruder" target="_blank" rel="noopener">Twitter</a>
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>


    <script
        src="https://code.jquery.com/jquery-3.4.1.min.js"
        integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
        crossorigin="anonymous">
    </script>
    <script src="../assets/built/casper.js?v=d60178fff0"></script>

    <script>
        // Parse the URL parameter
        function getParameterByName(name, url) {
            if (!url) url = window.location.href;
            name = name.replace(/[\[\]]/g, "\\$&");
            var regex = new RegExp("[?&]" + name + "(=([^&#]*)|&|#|$)"),
                results = regex.exec(url);
            if (!results) return null;
            if (!results[2]) return '';
            return decodeURIComponent(results[2].replace(/\+/g, " "));
        }

        // Give the parameter a variable name
        var action = getParameterByName('action');

        $(document).ready(function () {
            if (action == 'subscribe') {
                $('body').addClass("subscribe-success");
            }

            $('.subscribe-success-message .subscribe-close').click(function () {
                $('.subscribe-success-message').addClass('close');
            });

            // Reset form on opening subscrion overlay
            $('.subscribe-button').click(function() {
                $('.subscribe-overlay form').removeClass();
                $('.subscribe-email').val('');
            });
        });
    </script>

    <script>
    $(document).ready(function () {
        // FitVids - start
        var $postContent = $(".post-full-content");
        $postContent.fitVids();
        // FitVids - end

        // Replace nav with title on scroll - start
        Casper.stickyNavTitle({
            navSelector: '.site-nav-main',
            titleSelector: '.post-full-title',
            activeClass: 'nav-post-title-active'
        });
        // Replace nav with title on scroll - end

        // Hover on avatar
        var hoverTimeout;
        $('.author-list-item').hover(function () {
            var $this = $(this);

            clearTimeout(hoverTimeout);

            $('.author-card').removeClass('hovered');
            $(this).children('.author-card').addClass('hovered');

        }, function () {
            var $this = $(this);

            hoverTimeout = setTimeout(function () {
                $this.children('.author-card').removeClass('hovered');
            }, 800);
        });
    });
</script>


    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/zepto/1.1.6/zepto.min.js"></script>
<script>jQuery = Zepto</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism.min.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/plugins/line-numbers/prism-line-numbers.min.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism-coy.min.css" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/components/prism-python.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-60512592-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-60512592-1');
</script>

</body>
</html>

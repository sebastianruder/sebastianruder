<!DOCTYPE html>
<html lang="en">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>Requests for Research</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="stylesheet" type="text/css" href="../assets/built/screen.css?v=858352cb6c" />

    <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon" />
    <link rel="canonical" href="https://ruder.io/requests-for-research/" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <link rel="amphtml" href="https://ruder.io/requests-for-research/amp/" />
    
    <meta property="og:site_name" content="Sebastian Ruder" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Requests for Research" />
    <meta property="og:description" content="It can be hard to find compelling topics to work on and know what questions to ask when you are just starting as a researcher. This post aims to provide inspiration and ideas for research directions to junior researchers and those trying to get into research." />
    <meta property="og:url" content="https://ruder.io/requests-for-research/" />
    <meta property="og:image" content="https://ruder.io/content/images/2018/10/domain_randomization.png" />
    <meta property="article:published_time" content="2018-03-04T15:00:00.000Z" />
    <meta property="article:modified_time" content="2018-10-24T13:37:00.000Z" />
    <meta property="article:tag" content="transfer learning" />
    <meta property="article:tag" content="multi-task learning" />
    <meta property="article:tag" content="cross-lingual" />
    <meta property="article:tag" content="meta-learning" />
    <meta property="article:tag" content="domain adaptation" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Requests for Research" />
    <meta name="twitter:description" content="It can be hard to find compelling topics to work on and know what questions to ask when you are just starting as a researcher. This post aims to provide inspiration and ideas for research directions to junior researchers and those trying to get into research." />
    <meta name="twitter:url" content="https://ruder.io/requests-for-research/" />
    <meta name="twitter:image" content="https://ruder.io/content/images/2018/10/domain_randomization.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Sebastian Ruder" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="transfer learning, multi-task learning, cross-lingual, meta-learning, domain adaptation" />
    <meta name="twitter:site" content="@seb_ruder" />
    <meta property="og:image:width" content="1768" />
    <meta property="og:image:height" content="1518" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Sebastian Ruder",
        "url": "https://ruder.io/",
        "logo": {
            "@type": "ImageObject",
            "url": {
                "@type": "ImageObject",
                "url": "https://ruder.io/favicon.ico",
                "width": 48,
                "height": 48
            }
        }
    },
    "author": {
        "@type": "Person",
        "name": "Sebastian Ruder",
        "image": {
            "@type": "ImageObject",
            "url": "https://ruder.io/content/images/2019/02/new_profile_photo_square-1.jpg",
            "width": 2000,
            "height": 2000
        },
        "url": "https://ruder.io/author/sebastian/",
        "sameAs": []
    },
    "headline": "Requests for Research",
    "url": "https://ruder.io/requests-for-research/",
    "datePublished": "2018-03-04T15:00:00.000Z",
    "dateModified": "2018-10-24T13:37:00.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://ruder.io/content/images/2018/10/domain_randomization.png",
        "width": 1768,
        "height": 1518
    },
    "keywords": "transfer learning, multi-task learning, cross-lingual, meta-learning, domain adaptation",
    "description": "It can be hard to find compelling topics to work on and know what questions to ask when you are just starting as a researcher. This post aims to provide inspiration and ideas for research directions to junior researchers and those trying to get into research.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://ruder.io/"
    }
}
    </script>

    <meta name="generator" content="Ghost 3.11" />
    <link rel="alternate" type="application/rss+xml" title="Sebastian Ruder" href="https://ruder.io/rss/" />
    <script>
var profile_title = 'Sebastian Ruder';
</script>
<script>
var disqus_shortname = 'sebastianruder';
</script>
<script>
var profile_resume ='NLP PhD student';
</script>
<script>
var ga_id = 'UA-60512592-1';
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [["$", "$"], ["\\(", "\\)"]],
        processEscapes: true
    }
});
</script>

</head>
<body class="post-template tag-transfer-learning tag-multi-task-learning tag-cross-lingual tag-meta-learning tag-domain-adaptation">

    <div class="site-wrapper">

        

<header class="site-header">
    <div class="outer site-nav-main">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left-wrapper">
        <div class="site-nav-left">
                <a class="site-nav-logo" href="https://ruder.io">Sebastian Ruder</a>
            <div class="site-nav-content">
                    <ul class="nav" role="menu">
    <li class="nav-about" role="menuitem"><a href="https://ruder.io/about/">About</a></li>
    <li class="nav-tags" role="menuitem"><a href="https://ruder.io/tags/">Tags</a></li>
    <li class="nav-papers" role="menuitem"><a href="https://ruder.io/publications/">Papers</a></li>
    <li class="nav-talks" role="menuitem"><a href="https://ruder.io/talks/">Talks</a></li>
    <li class="nav-news" role="menuitem"><a href="https://ruder.io/news/">News</a></li>
    <li class="nav-faq" role="menuitem"><a href="https://ruder.io/faq/">FAQ</a></li>
    <li class="nav-sign-up-for-nlp-news" role="menuitem"><a href="https://ruder.io/nlp-news/">Sign up for NLP News</a></li>
    <li class="nav-nlp-progress" role="menuitem"><a href="https://nlpprogress.com/">NLP Progress</a></li>
    <li class="nav-media" role="menuitem"><a href="https://ruder.io/media/">Media</a></li>
    <li class="nav-contact" role="menuitem"><a href="https://ruder.io/contact/">Contact</a></li>
</ul>

                    <span class="nav-post-title dash">Requests for Research</span>
            </div>
        </div>
    </div>
    <div class="site-nav-right">
            <div class="social-links">
                    <a class="social-link social-link-tw" href="https://twitter.com/seb_ruder" title="Twitter" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
</a>
            </div>
                <a class="rss-button" href="https://ruder.io/rss/index.rss" title="RSS" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
</a>

    </div>
</nav>
    </div>
</div></header>


<main id="site-main" class="site-main outer">
    <div class="inner">

        <article class="post-full post tag-transfer-learning tag-multi-task-learning tag-cross-lingual tag-meta-learning tag-domain-adaptation ">

            <header class="post-full-header">

                <section class="post-full-tags">
                    <a href="../tag/transfer-learning/index.html">transfer learning</a>
                </section>

                <h1 class="post-full-title">Requests for Research</h1>

                <p class="post-full-custom-excerpt">It can be hard to find compelling topics to work on and know what questions to ask when you are just starting as a researcher. This post aims to provide inspiration and ideas for research directions to junior researchers and those trying to get into research.</p>

                <div class="post-full-byline">

                    <section class="post-full-byline-content">

                        <ul class="author-list">
                            <li class="author-list-item">

                                <div class="author-card">
                                    <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                                    <div class="author-info">
                                        <h2>Sebastian Ruder</h2>
                                        <p>Read <a href="../author/sebastian/index.html">more posts</a> by this author.</p>
                                    </div>
                                </div>

                                <a href="../author/sebastian/index.html" class="author-avatar">
                                    <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                                </a>

                            </li>
                        </ul>

                        <section class="post-full-byline-meta">
                            <h4 class="author-name"><a href="../author/sebastian/index.html">Sebastian Ruder</a></h4>
                            <div class="byline-meta-content">
                                <time class="byline-meta-date" datetime="2018-03-04">4 Mar 2018</time>
                                <span class="byline-reading-time"><span class="bull">&bull;</span> 13 min read</span>
                            </div>
                        </section>

                    </section>


                </div>
            </header>

            <figure class="post-full-image">
                <img
                    srcset="../content/images/size/w300/2018/10/domain_randomization.png 300w,
                           ../content/images/size/w600/2018/10/domain_randomization.png 600w,
                          ../content/images/size/w1000/2018/10/domain_randomization.png 1000w,
                         ../content/images/size/w2000/2018/10/domain_randomization.png 2000w"
                    sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px"
                    src="../content/images/size/w2000/2018/10/domain_randomization.png"
                    alt="Requests for Research"
                />
            </figure>

            <section class="post-full-content">
                <div class="post-content">
                    <!--kg-card-begin: markdown--><p>This post aims to provide inspiration and ideas for research directions to junior researchers and those trying to get into research.</p>
<p>Table of contents:</p>
<ul>
<li><a href="index.html#taskindependentdataaugmentationfornlp">Task-independent data augmentation for NLP</a></li>
<li><a href="index.html#fewshotlearningfornlp">Few-shot learning for NLP</a></li>
<li><a href="index.html#transferlearningfornlp">Transfer learning for NLP</a></li>
<li><a href="index.html#multitasklearning">Multi-task learning</a></li>
<li><a href="index.html#crosslinguallearning">Cross-lingual learning</a></li>
<li><a href="index.html#taskindependentarchitectureimprovements">Task-independent architecture improvements</a></li>
</ul>
<p>It can be hard to find compelling topics to work on and know what questions are interesting to ask when you are just starting as a researcher in a new field. Machine learning research in particular moves so fast these days that it is difficult to find an opening.</p>
<p>This post aims to provide inspiration and ideas for research directions to junior researchers and those trying to get into research. It gathers a collection of research topics that are interesting to me, with a focus on NLP and transfer learning. As such, they might obviously not be of interest to everyone. If you are interested in Reinforcement Learning, OpenAI provides a <a href="https://blog.openai.com/requests-for-research-2/">selection of interesting RL-focused research topics</a>. In case you'd like to collaborate with others or are interested in a broader range of topics, have a look at the <a href="https://ai-on.org/">Artificial Intelligence Open Network</a>.</p>
<p>Most of these topics are not thoroughly thought out yet; in many cases, the general description is quite vague and subjective and many directions are possible. In addition, most of these are <em>not</em> low-hanging fruit, so serious effort is necessary to come up with a solution. I am happy to provide feedback with regard to any of these, but will not have time to provide more detailed guidance unless you have a working proof-of-concept. I will update this post periodically with new research directions and advances in already listed ones. Note that this collection does not attempt to review the extensive literature but only aims to give a glimpse of a topic; consequently, the references won't be comprehensive.</p>
<p>I hope that this collection will pique your interest and serve as inspiration for your own research agenda.</p>
<h2 id="taskindependentdataaugmentationfornlp">Task-independent data augmentation for NLP</h2>
<p>Data augmentation aims to create additional training data by producing variations of existing training examples through transformations, which can mirror those encountered in the real world. In Computer Vision (CV), common augmentation techniques are <a href="https://www.coursera.org/learn/convolutional-neural-networks/lecture/AYzbX/data-augmentation">mirroring, random cropping, shearing, etc</a>. Data augmentation is super useful in CV. For instance, it has been used to great effect in AlexNet (Krizhevsky et al., 2012) <sup class="footnote-ref"><a href="index.html#fn1" id="fnref1">[1]</a></sup> to combat overfitting and in most state-of-the-art models since. In addition, data augmentation makes intuitive sense as it makes the training data more diverse and should thus increase a model's generalization ability.</p>
<p>However, in NLP, data augmentation is not widely used. In my mind, this is for two reasons:</p>
<ol>
<li>Data in NLP is discrete. This prevents us from applying simple transformations directly to the input data. Most recently proposed augmentation methods in CV focus on such transformations, e.g. domain randomization (Tobin et al., 2017) <sup class="footnote-ref"><a href="index.html#fn2" id="fnref2">[2]</a></sup>.</li>
<li>Small perturbations may change the meaning. Deleting a negation may change a sentence's sentiment, while modifying a word in a paragraph might inadvertently change the answer to a question about that paragraph. This is not the case in CV where perturbing individual pixels does not change whether an image is a cat or dog and even stark changes such as interpolation of different images can be useful (Zhang et al., 2017) <sup class="footnote-ref"><a href="index.html#fn3" id="fnref3">[3]</a></sup>.</li>
</ol>
<p>Existing approaches that I am aware of are either rule-based (Li et al., 2017) <sup class="footnote-ref"><a href="index.html#fn4" id="fnref4">[4]</a></sup> or task-specific, e.g. for parsing (Wang and Eisner, 2016) <sup class="footnote-ref"><a href="index.html#fn5" id="fnref5">[5]</a></sup> or zero-pronoun resolution (Liu et al., 2017) <sup class="footnote-ref"><a href="index.html#fn6" id="fnref6">[6]</a></sup>. Xie et al. (2017) <sup class="footnote-ref"><a href="index.html#fn7" id="fnref7">[7]</a></sup> replace words with samples from different distributions for language modelling and Machine Translation. Recent work focuses on creating adversarial examples either by replacing words or characters (Samanta and Mehta, 2017; Ebrahimi et al., 2017) <sup class="footnote-ref"><a href="index.html#fn8" id="fnref8">[8]</a></sup> <sup class="footnote-ref"><a href="index.html#fn9" id="fnref9">[9]</a></sup>, concatenation (Jia and Liang, 2017) <sup class="footnote-ref"><a href="index.html#fn10" id="fnref10">[10]</a></sup>, or adding adversarial perturbations (Yasunaga et al., 2017) <sup class="footnote-ref"><a href="index.html#fn11" id="fnref11">[11]</a></sup>. An adversarial setup is also used by Li et al. (2017) <sup class="footnote-ref"><a href="index.html#fn12" id="fnref12">[12]</a></sup> who train a system to produce sequences that are indistinguishable from human-generated dialogue utterances.</p>
<p>Back-translation (Sennrich et al., 2015; Sennrich et al., 2016) <sup class="footnote-ref"><a href="index.html#fn13" id="fnref13">[13]</a></sup> <sup class="footnote-ref"><a href="index.html#fn14" id="fnref14">[14]</a></sup> is a common data augmentation method in Machine Translation (MT) that allows us to incorporate monolingual training data. For instance, when training a EN\(\rightarrow\)FR system, monolingual French text is translated to English using an FR\(\rightarrow\)EN system; the synthetic parallel data can then be used for training. Back-translation can also be used for paraphrasing (Mallinson et al., 2017) <sup class="footnote-ref"><a href="index.html#fn15" id="fnref15">[15]</a></sup>. Paraphrasing has been used for data augmentation for QA (Dong et al., 2017) <sup class="footnote-ref"><a href="index.html#fn16" id="fnref16">[16]</a></sup>, but I am not aware of its use for other tasks.</p>
<p>Another method that is close to paraphrasing is generating sentences from a continuous space using a variational autoencoder (Bowman et al., 2016; Guu et al., 2017) <sup class="footnote-ref"><a href="index.html#fn17" id="fnref17">[17]</a></sup> <sup class="footnote-ref"><a href="index.html#fn18" id="fnref18">[18]</a></sup>. If the representations are disentangled as in (Hu et al., 2017) <sup class="footnote-ref"><a href="index.html#fn19" id="fnref19">[19]</a></sup>, then we are also not too far from style transfer (Shen et al., 2017) <sup class="footnote-ref"><a href="index.html#fn20" id="fnref20">[20]</a></sup>.</p>
<p>There are a few research directions that would be interesting to pursue:</p>
<ol>
<li><strong>Evaluation study:</strong> Evaluate a range of existing data augmentation methods as well as techniques that have not been widely used for augmentation such as paraphrasing and style transfer on a diverse range of tasks including text classification and sequence labelling. Identify what types of data augmentation are robust across task and which are task-specific. This could be packaged as a software library to make future benchmarking easier (think <a href="https://github.com/tensorflow/cleverhans">CleverHans</a> for NLP).</li>
<li><strong>Data augmentation with style transfer:</strong> Investigate if style transfer can be used to modify various attributes of training examples for more robust learning.</li>
<li><strong>Learn the augmentation:</strong> Similar to Dong et al. (2017) we could learn either to paraphrase or to generate transformations for a particular task.</li>
<li><strong>Learn a word embedding space for data augmentation:</strong> A typical word embedding space clusters synonyms and antonyms together; using nearest neighbours in this space for replacement is thus infeasible. Inspired by recent work (Mrkšić et al., 2017) <sup class="footnote-ref"><a href="index.html#fn21" id="fnref21">[21]</a></sup>, we could specialize the word embedding space to make it more suitable for data augmentation.</li>
<li><strong>Adversarial data augmentation:</strong> Related to recent work in interpretability (Ribeiro et al., 2016) <sup class="footnote-ref"><a href="index.html#fn22" id="fnref22">[22]</a></sup>, we could change the most salient words in an example, i.e. those that a model depends on for a prediction. This still requires a semantics-preserving replacement method, however.</li>
</ol>
<h2 id="fewshotlearningfornlp">Few-shot learning for NLP</h2>
<p>Zero-shot, one-shot and few-shot learning are one of the most interesting recent research directions IMO. Following the key insight from Vinyals et al. (2016) <sup class="footnote-ref"><a href="index.html#fn23" id="fnref23">[23]</a></sup> that a few-shot learning model should be explicitly trained to perform few-shot learning, we have seen several recent advances (Ravi and Larochelle, 2017; Snell et al., 2017) <sup class="footnote-ref"><a href="index.html#fn24" id="fnref24">[24]</a></sup> <sup class="footnote-ref"><a href="index.html#fn25" id="fnref25">[25]</a></sup>.</p>
<p>Learning from few labeled samples is one of the hardest problems IMO and one of the core capabilities that separates the current generation of ML models from more generally applicable systems. Zero-shot learning has only been investigated in the context of <a href="http://ruder.io/word-embeddings-2017/index.html#oovhandling">learning word embeddings for unknown words</a> AFAIK. Dataless classification (Song and Roth, 2014; Song et al., 2016) <sup class="footnote-ref"><a href="index.html#fn26" id="fnref26">[26]</a></sup> <sup class="footnote-ref"><a href="index.html#fn27" id="fnref27">[27]</a></sup> is an interesting related direction that embeds labels and documents in a joint space, but requires interpretable labels with good descriptions.</p>
<p>Potential research directions are the following:</p>
<ol>
<li><strong>Standardized benchmarks:</strong> Create standardized benchmarks for few-shot learning for NLP. Vinyals et al. (2016) introduce a one-shot language modelling task for the Penn Treebank. The task, while useful, is dwarfed by the extensive evaluation on CV benchmarks and has not seen much use AFAIK. A few-shot learning benchmark for NLP should contain a large number of classes and provide a standardized split for reproducibility. Good candidate tasks would be topic classification or fine-grained entity recognition.</li>
<li><strong>Evaluation study</strong>: After creating such a benchmark, the next step would be to evaluate how well existing few-shot learning models from CV perform for NLP.</li>
<li><strong>Novel methods for NLP</strong>: Given a dataset for benchmarking and an empirical evaluation study, we could then start developing novel methods that can perform few-shot learning for NLP.</li>
</ol>
<h2 id="transferlearningfornlp">Transfer learning for NLP</h2>
<p>Transfer learning has had a large impact on computer vision (CV) and has greatly lowered the entry threshold for people wanting to apply CV algorithms to their own problems. CV practicioners are no longer required to perform extensive feature-engineering for every new task, but can simply fine-tune a model pretrained on a large dataset with a small number of examples.</p>
<p>In NLP, however, we have so far only been pretraining the first layer of our models via pretrained embeddings. Recent approaches (Peters et al., 2017, 2018) <sup class="footnote-ref"><a href="index.html#fn28" id="fnref28">[28]</a></sup> <sup class="footnote-ref"><a href="index.html#fn29" id="fnref29">[29]</a></sup> add pretrained language model embedddings, but these still require custom architectures for every task. In my opinion, in order to unlock the true potential of transfer learning for NLP, we need to pretrain the entire model and fine-tune it on the target task, akin to fine-tuning ImageNet models. Language modelling, for instance, is a great task for pretraining and could be to NLP what ImageNet classification is to CV (Howard and Ruder, 2018) <sup class="footnote-ref"><a href="index.html#fn30" id="fnref30">[30]</a></sup>.</p>
<p>Here are some potential research directions in this context:</p>
<ol>
<li><strong>Identify useful pretraining tasks:</strong> The choice of the pretraining task is very important as even fine-tuning a model on a related task might only provide limited success (Mou et al., 2016) <sup class="footnote-ref"><a href="index.html#fn31" id="fnref31">[31]</a></sup>. Other tasks such as those explored in recent work on learning general-purpose sentence embeddings (Conneau et al., 2017; Subramanian et al., 2018; Nie et al., 2017) <sup class="footnote-ref"><a href="index.html#fn32" id="fnref32">[32]</a></sup> <sup class="footnote-ref"><a href="index.html#fn33" id="fnref33">[33]</a></sup> <sup class="footnote-ref"><a href="index.html#fn34" id="fnref34">[34]</a></sup> might be complementary to language model pretraining or suitable for other target tasks.</li>
<li><strong>Fine-tuning of complex architectures:</strong> Pretraining is most useful when a model can be applied to many target tasks. However, it is still unclear how to pretrain more complex architectures, such as those used for pairwise classification tasks (Augenstein et al., 2018) or reasoning tasks such as QA or reading comprehension.</li>
</ol>
<h2 id="multitasklearning">Multi-task learning</h2>
<p>Multi-task learning (MTL) has become more commonly used in NLP. See <a href="http://ruder.io/multi-task/">here</a> for a general overview of multi-task learning and <a href="http://ruder.io/multi-task-learning-nlp/">here</a> for MTL objectives for NLP. However, there is still much we don't understand about multi-task learning in general.</p>
<p>The main questions regarding MTL give rise to many interesting research directions:</p>
<ol>
<li><strong>Identify effective auxiliary tasks:</strong> One of the main questions is which tasks are useful for multi-task learning. Label entropy has been shown to be a predictor of MTL success (Alonso and Plank, 2017) <sup class="footnote-ref"><a href="index.html#fn35" id="fnref35">[35]</a></sup>, but this does not tell the whole story. In recent work (Augenstein et al., 2018) <sup class="footnote-ref"><a href="index.html#fn36" id="fnref36">[36]</a></sup>, we have found that auxiliary tasks with more data and more fine-grained labels are more useful. It would be useful if future MTL papers would not only propose a new model or auxiliary task, but also try to understand why a certain auxiliary task might be better than another closely related one.</li>
<li><strong>Alternatives to hard parameter sharing:</strong> Hard parameter sharing is still the default modus operandi for MTL, but places a strong constraint on the model to compress knowledge pertaining to different tasks with the same parameters, which often makes learning difficult. We need better ways of doing MTL that are easy to use and work reliably across many tasks. Recently proposed methods such as cross-stitch units (Misra et al., 2017; Ruder et al., 2017) <sup class="footnote-ref"><a href="index.html#fn37" id="fnref37">[37]</a></sup> <sup class="footnote-ref"><a href="index.html#fn38" id="fnref38">[38]</a></sup> and a label embedding layer (Augenstein et al., 2018) are promising steps in this direction.</li>
<li><strong>Artificial auxiliary tasks:</strong> The best auxiliary tasks are those, which are tailored to the target task and do not require any additional data. I have outlined a list of potential <em>artificial</em> auxiliary tasks <a href="http://ruder.io/multi-task-learning-nlp/">here</a>. However, it is not clear which of these work reliably across a number of diverse tasks or what variations or task-specific modifications are useful.</li>
</ol>
<h2 id="crosslinguallearning">Cross-lingual learning</h2>
<p>Creating models that perform well across languages and that can transfer knowledge from resource-rich to resource-poor languages is one of the most important research directions IMO. There has been much progress in learning cross-lingual representations that project different languages into a shared embedding space. Refer to Ruder et al. (2017) <sup class="footnote-ref"><a href="index.html#fn39" id="fnref39">[39]</a></sup> for a survey.</p>
<p>Cross-lingual representations are commonly evaluated either intrinsically on similarity benchmarks or extrinsically on downstream tasks, such as text classification. While recent methods have advanced the state-of-the-art for many of these settings, we do not have a good understanding of the tasks or languages for which these methods fail and how to mitigate these failures in a task-independent manner, e.g. by injecting task-specific constraints (Mrkšić et al., 2017).</p>
<h2 id="taskindependentarchitectureimprovements">Task-independent architecture improvements</h2>
<p>Novel architectures that outperform the current state-of-the-art and are tailored to specific tasks are regularly introduced, superseding the previous architecture. I have outlined <a href="http://ruder.io/deep-learning-nlp-best-practices/">best practices for different NLP tasks</a> before, but without comparing such architectures on different tasks, it is often hard to gain insights from specialized architectures and tell which components would also be useful in other settings.</p>
<p>A particularly promising recent model is the Transformer (Vaswani et al., 2017) <sup class="footnote-ref"><a href="index.html#fn40" id="fnref40">[40]</a></sup>. While the complete model might not be appropriate for every task, components such as multi-head attention or position-based encoding could be building blocks that are generally useful for many NLP tasks.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I hope you've found this collection of research directions useful. If you have suggestions on how to tackle some of these problems or ideas for related research topics, feel free to comment below.</p>
<p>Cover image is from Tobin et al. (2017).</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105). <a href="index.html#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., &amp; Abbeel, P. (2017). Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World. arXiv Preprint arXiv:1703.06907. Retrieved from <a href="http://arxiv.org/abs/1703.06907">http://arxiv.org/abs/1703.06907</a> <a href="index.html#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Zhang, H., Cisse, M., Dauphin, Y. N., &amp; Lopez-Paz, D. (2017). mixup: Beyond Empirical Risk Minimization, 1–11. Retrieved from <a href="http://arxiv.org/abs/1710.09412">http://arxiv.org/abs/1710.09412</a> <a href="index.html#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>Li, Y., Cohn, T., &amp; Baldwin, T. (2017). Robust Training under Linguistic Adversity. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (Vol. 2, pp. 21–27). <a href="index.html#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>Wang, D., &amp; Eisner, J. (2016). The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages. Tacl, 4, 491–505. Retrieved from <a href="https://www.transacl.org/ojs/index.php/tacl/article/viewFile/917/212%0Ahttps://transacl.org/ojs/index.php/tacl/article/view/917">https://www.transacl.org/ojs/index.php/tacl/article/viewFile/917/212
https://transacl.org/ojs/index.php/tacl/article/view/917</a> <a href="index.html#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>Liu, T., Cui, Y., Yin, Q., Zhang, W., Wang, S., &amp; Hu, G. (2017). Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (pp. 102–111). <a href="index.html#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p>Xie, Z., Wang, S. I., Li, J., Levy, D., Nie, A., Jurafsky, D., &amp; Ng, A. Y. (2017). Data Noising as Smoothing in Neural Network Language Models. In Proceedings of ICLR 2017. <a href="index.html#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p>Samanta, S., &amp; Mehta, S. (2017). Towards Crafting Text Adversarial Samples. arXiv preprint arXiv:1707.02812. <a href="index.html#fnref8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p>Ebrahimi, J., Rao, A., Lowd, D., &amp; Dou, D. (2017). HotFlip: White-Box Adversarial Examples for NLP. Retrieved from <a href="http://arxiv.org/abs/1712.06751">http://arxiv.org/abs/1712.06751</a> <a href="index.html#fnref9" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p>Jia, R., &amp; Liang, P. (2017). Adversarial Examples for Evaluating Reading Comprehension Systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. <a href="index.html#fnref10" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p>Yasunaga, M., Kasai, J., &amp; Radev, D. (2017). Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In Proceedings of NAACL 2018. Retrieved from <a href="http://arxiv.org/abs/1711.04903">http://arxiv.org/abs/1711.04903</a> <a href="index.html#fnref11" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p>Li, J., Monroe, W., Shi, T., Ritter, A., &amp; Jurafsky, D. (2017). Adversarial Learning for Neural Dialogue Generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Retrieved from <a href="http://arxiv.org/abs/1701.06547">http://arxiv.org/abs/1701.06547</a> <a href="index.html#fnref12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p>Sennrich, R., Haddow, B., &amp; Birch, A. (2015). Improving neural machine translation models with monolingual data. arXiv preprint arXiv:1511.06709. <a href="index.html#fnref13" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn14" class="footnote-item"><p>Sennrich, R., Haddow, B., &amp; Birch, A. (2016). Edinburgh neural machine translation systems for wmt 16. arXiv preprint arXiv:1606.02891. <a href="index.html#fnref14" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn15" class="footnote-item"><p>Mallinson, J., Sennrich, R., &amp; Lapata, M. (2017). Paraphrasing revisited with neural machine translation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers (Vol. 1, pp. 881-893). <a href="index.html#fnref15" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn16" class="footnote-item"><p>Dong, L., Mallinson, J., Reddy, S., &amp; Lapata, M. (2017). Learning to Paraphrase for Question Answering. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. <a href="index.html#fnref16" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn17" class="footnote-item"><p>Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R., &amp; Bengio, S. (2016). Generating Sentences from a Continuous Space. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL). Retrieved from <a href="http://arxiv.org/abs/1511.06349">http://arxiv.org/abs/1511.06349</a> <a href="index.html#fnref17" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn18" class="footnote-item"><p>Guu, K., Hashimoto, T. B., Oren, Y., &amp; Liang, P. (2017). Generating Sentences by Editing Prototypes. <a href="index.html#fnref18" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn19" class="footnote-item"><p>Hu, Z., Yang, Z., Liang, X., Salakhutdinov, R., &amp; Xing, E. P. (2017). Toward Controlled Generation of Text. In Proceedings of the 34th International Conference on Machine Learning. <a href="index.html#fnref19" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn20" class="footnote-item"><p>Shen, T., Lei, T., Barzilay, R., &amp; Jaakkola, T. (2017). Style Transfer from Non-Parallel Text by Cross-Alignment. In Advances in Neural Information Processing Systems. Retrieved from <a href="http://arxiv.org/abs/1705.09655">http://arxiv.org/abs/1705.09655</a> <a href="index.html#fnref20" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn21" class="footnote-item"><p>Mrkšić, N., Vulić, I., Séaghdha, D. Ó., Leviant, I., Reichart, R., Gašić, M., … Young, S. (2017). Semantic Specialisation of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints. TACL. Retrieved from <a href="http://arxiv.org/abs/1706.00374">http://arxiv.org/abs/1706.00374</a> <a href="index.html#fnref21" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn22" class="footnote-item"><p>Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016, August). Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1135-1144). ACM. <a href="index.html#fnref22" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn23" class="footnote-item"><p>Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., &amp; Wierstra, D. (2016). Matching Networks for One Shot Learning. NIPS 2016. Retrieved from <a href="http://arxiv.org/abs/1606.04080">http://arxiv.org/abs/1606.04080</a> <a href="index.html#fnref23" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn24" class="footnote-item"><p>Ravi, S., &amp; Larochelle, H. (2017). Optimization as a Model for Few-Shot Learning. In ICLR 2017. <a href="index.html#fnref24" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn25" class="footnote-item"><p>Snell, J., Swersky, K., &amp; Zemel, R. S. (2017). Prototypical Networks for Few-shot Learning. In Advances in Neural Information Processing Systems. <a href="index.html#fnref25" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn26" class="footnote-item"><p>Song, Y., &amp; Roth, D. (2014). On dataless hierarchical text classification. Proceedings of AAAI, 1579–1585. Retrieved from <a href="http://cogcomp.cs.illinois.edu/papers/SongSoRo14.pdf">http://cogcomp.cs.illinois.edu/papers/SongSoRo14.pdf</a> <a href="index.html#fnref26" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn27" class="footnote-item"><p>Song, Y., Upadhyay, S., Peng, H., &amp; Roth, D. (2016). Cross-Lingual Dataless Classification for Many Languages. Ijcai, 2901–2907. <a href="index.html#fnref27" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn28" class="footnote-item"><p>Peters, M. E., Ammar, W., Bhagavatula, C., &amp; Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017). <a href="index.html#fnref28" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn29" class="footnote-item"><p>Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp; Zettlemoyer, L. (2018). Deep contextualized word representations. Proceedings of NAACL. <a href="index.html#fnref29" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn30" class="footnote-item"><p>Howard, J., &amp; Ruder, S. (2018). Fine-tuned Language Models for Text Classification. arXiv preprint arXiv:1801.06146. <a href="index.html#fnref30" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn31" class="footnote-item"><p>Mou, L., Meng, Z., Yan, R., Li, G., Xu, Y., Zhang, L., &amp; Jin, Z. (2016). How Transferable are Neural Networks in NLP Applications? Proceedings of 2016 Conference on Empirical Methods in Natural Language Processing. <a href="index.html#fnref31" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn32" class="footnote-item"><p>Conneau, A., Kiela, D., Schwenk, H., Barrault, L., &amp; Bordes, A. (2017). Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. <a href="index.html#fnref32" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn33" class="footnote-item"><p>Subramanian, S., Trischler, A., Bengio, Y., &amp; Pal, C. J. (2018). Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning. In Proceedings of ICLR 2018. <a href="index.html#fnref33" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn34" class="footnote-item"><p>Nie, A., Bennett, E. D., &amp; Goodman, N. D. (2017). DisSent: Sentence Representation Learning from Explicit Discourse Relations. arXiv Preprint arXiv:1710.04334. Retrieved from <a href="http://arxiv.org/abs/1710.04334">http://arxiv.org/abs/1710.04334</a> <a href="index.html#fnref34" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn35" class="footnote-item"><p>Alonso, H. M., &amp; Plank, B. (2017). When is multitask learning effective? Multitask learning for semantic sequence prediction under varying data conditions. In EACL. Retrieved from <a href="http://arxiv.org/abs/1612.02251">http://arxiv.org/abs/1612.02251</a> <a href="index.html#fnref35" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn36" class="footnote-item"><p>Augenstein, I., Ruder, S., &amp; Søgaard, A. (2018). Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces. In Proceedings of NAACL 2018. <a href="index.html#fnref36" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn37" class="footnote-item"><p>Misra, I., Shrivastava, A., Gupta, A., &amp; Hebert, M. (2016). Cross-stitch Networks for Multi-task Learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. <a href="http://doi.org/10.1109/CVPR.2016.433">http://doi.org/10.1109/CVPR.2016.433</a> <a href="index.html#fnref37" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn38" class="footnote-item"><p>Ruder, S., Bingel, J., Augenstein, I., &amp; Søgaard, A. (2017). Sluice networks: Learning what to share between loosely related tasks. arXiv preprint arXiv:1705.08142. <a href="index.html#fnref38" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn39" class="footnote-item"><p>Ruder, S., Vulić, I., &amp; Søgaard, A. (2017). A Survey of Cross-lingual Word Embedding Models. arXiv Preprint arXiv:1706.04902. Retrieved from <a href="http://arxiv.org/abs/1706.04902">http://arxiv.org/abs/1706.04902</a> <a href="index.html#fnref39" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn40" class="footnote-item"><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention Is All You Need. In Advances in Neural Information Processing Systems. <a href="index.html#fnref40" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
<!--kg-card-end: markdown-->
                </div>
            </section>



        </article>

    </div>
</main>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = '/requests-for-research/';  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "ghost-50"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://EXAMPLE.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                            

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
                <article class="read-next-card">
                    <header class="read-next-card-header">
                        <h3><span>More in</span> <a href="../tag/transfer-learning/index.html">transfer learning</a></h3>
                    </header>
                    <div class="read-next-card-content">
                        <ul>
                            <li>
                                <h4><a href="../recent-advances-lm-fine-tuning/index.html">Recent Advances in Language Model Fine-tuning</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2021-02-24">24 Feb 2021</time> –
                                        13 min read</p>
                                </div>
                            </li>
                            <li>
                                <h4><a href="../research-highlights-2020/index.html">ML and NLP Research Highlights of 2020</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2021-01-19">19 Jan 2021</time> –
                                        15 min read</p>
                                </div>
                            </li>
                            <li>
                                <h4><a href="../research-highlights-2019/index.html">10 ML &amp; NLP Research Highlights of 2019</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2020-01-06">6 Jan 2020</time> –
                                        12 min read</p>
                                </div>
                            </li>
                        </ul>
                    </div>
                    <footer class="read-next-card-footer">
                        <a href="../tag/transfer-learning/index.html">See all 18 posts
                            →</a>
                    </footer>
                </article>

                <article class="post-card post tag-tensorflow tag-natural-language-processing ">

    <a class="post-card-image-link" href="../text-classification-tensorflow-estimators/index.html">
        <img class="post-card-image"
            srcset="../content/images/size/w300/2018/10/estimators_loss.png 300w,
                   ../content/images/size/w600/2018/10/estimators_loss.png 600w,
                  ../content/images/size/w1000/2018/10/estimators_loss.png 1000w,
                 ../content/images/size/w2000/2018/10/estimators_loss.png 2000w"
            sizes="(max-width: 1000px) 400px, 700px"
            src="../content/images/size/w600/2018/10/estimators_loss.png"
            alt="Text Classification with TensorFlow Estimators"
        />
    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="../text-classification-tensorflow-estimators/index.html">

            <header class="post-card-header">
                    <div class="post-card-primary-tag">tensorflow</div>
                <h2 class="post-card-title">Text Classification with TensorFlow Estimators</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>This post is a tutorial that shows how to use Tensorflow Estimators for text classification. It covers loading data using Datasets, using pre-canned estimators as baselines, word embeddings, and building custom estimators, among others.</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Sebastian Ruder
                    </div>
            
                    <a href="../author/sebastian/index.html" class="static-avatar">
                        <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/sebastian/index.html">Sebastian Ruder</a></span>
                <span class="post-card-byline-date"><time datetime="2018-04-16">16 Apr 2018</time> <span class="bull">&bull;</span> 13 min read</span>
            </div>
        </footer>

    </div>

</article>

                <article class="post-card post tag-optimization ">

    <a class="post-card-image-link" href="../deep-learning-optimization-2017/index.html">
        <img class="post-card-image"
            srcset="../content/images/size/w300/2017/12/snapshot_ensembles.png 300w,
                   ../content/images/size/w600/2017/12/snapshot_ensembles.png 600w,
                  ../content/images/size/w1000/2017/12/snapshot_ensembles.png 1000w,
                 ../content/images/size/w2000/2017/12/snapshot_ensembles.png 2000w"
            sizes="(max-width: 1000px) 400px, 700px"
            src="../content/images/size/w600/2017/12/snapshot_ensembles.png"
            alt="Optimization for Deep Learning Highlights in 2017"
        />
    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="../deep-learning-optimization-2017/index.html">

            <header class="post-card-header">
                    <div class="post-card-primary-tag">optimization</div>
                <h2 class="post-card-title">Optimization for Deep Learning Highlights in 2017</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>Different gradient descent optimization algorithms have been proposed in recent years but Adam is still most commonly used. This post discusses the most exciting highlights and most promising recent approaches that may shape the way we will optimize our models in the future.</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Sebastian Ruder
                    </div>
            
                    <a href="../author/sebastian/index.html" class="static-avatar">
                        <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/sebastian/index.html">Sebastian Ruder</a></span>
                <span class="post-card-byline-date"><time datetime="2017-12-03">3 Dec 2017</time> <span class="bull">&bull;</span> 15 min read</span>
            </div>
        </footer>

    </div>

</article>
        </div>
    </div>
</aside>




        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://ruder.io">Sebastian Ruder</a> &copy; 2021</section>
                <nav class="site-footer-nav">
                    <a href="https://ruder.io">Latest Posts</a>
                    
                    <a href="https://twitter.com/seb_ruder" target="_blank" rel="noopener">Twitter</a>
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>


    <script
        src="https://code.jquery.com/jquery-3.4.1.min.js"
        integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
        crossorigin="anonymous">
    </script>
    <script src="../assets/built/casper.js?v=858352cb6c"></script>

    <script>
        // Parse the URL parameter
        function getParameterByName(name, url) {
            if (!url) url = window.location.href;
            name = name.replace(/[\[\]]/g, "\\$&");
            var regex = new RegExp("[?&]" + name + "(=([^&#]*)|&|#|$)"),
                results = regex.exec(url);
            if (!results) return null;
            if (!results[2]) return '';
            return decodeURIComponent(results[2].replace(/\+/g, " "));
        }

        // Give the parameter a variable name
        var action = getParameterByName('action');

        $(document).ready(function () {
            if (action == 'subscribe') {
                $('body').addClass("subscribe-success");
            }

            $('.subscribe-success-message .subscribe-close').click(function () {
                $('.subscribe-success-message').addClass('close');
            });

            // Reset form on opening subscrion overlay
            $('.subscribe-button').click(function() {
                $('.subscribe-overlay form').removeClass();
                $('.subscribe-email').val('');
            });
        });
    </script>

    <script>
    $(document).ready(function () {
        // FitVids - start
        var $postContent = $(".post-full-content");
        $postContent.fitVids();
        // FitVids - end

        // Replace nav with title on scroll - start
        Casper.stickyNavTitle({
            navSelector: '.site-nav-main',
            titleSelector: '.post-full-title',
            activeClass: 'nav-post-title-active'
        });
        // Replace nav with title on scroll - end

        // Hover on avatar
        var hoverTimeout;
        $('.author-list-item').hover(function () {
            var $this = $(this);

            clearTimeout(hoverTimeout);

            $('.author-card').removeClass('hovered');
            $(this).children('.author-card').addClass('hovered');

        }, function () {
            var $this = $(this);

            hoverTimeout = setTimeout(function () {
                $this.children('.author-card').removeClass('hovered');
            }, 800);
        });
    });
</script>


    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/zepto/1.1.6/zepto.min.js"></script>
<script>jQuery = Zepto</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism.min.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/plugins/line-numbers/prism-line-numbers.min.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism-coy.min.css" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/components/prism-python.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-60512592-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-60512592-1');
</script>

</body>
</html>

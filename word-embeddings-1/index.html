<!DOCTYPE html>
<html lang="en">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>On word embeddings - Part 1</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="stylesheet" type="text/css" href="../assets/built/screen.css?v=8587ecf4f6" />

    <meta name="description" content="The first post in a series about word embeddings. This post presents word embedding models in the context of language modeling and past research." />
    <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon" />
    <link rel="canonical" href="https://ruder.io/word-embeddings-1/" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <link rel="amphtml" href="https://ruder.io/word-embeddings-1/amp/" />
    
    <meta property="og:site_name" content="Sebastian Ruder" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="On word embeddings - Part 1" />
    <meta property="og:description" content="Word embeddings popularized by word2vec are pervasive in current NLP applications. The history of word embeddings, however, goes back a lot further. This post explores the history of word embeddings in the context of language modelling." />
    <meta property="og:url" content="https://ruder.io/word-embeddings-1/" />
    <meta property="og:image" content="https://ruder.io/content/images/2016/04/word_embeddings_colah.png" />
    <meta property="article:published_time" content="2016-04-11T14:00:00.000Z" />
    <meta property="article:modified_time" content="2019-10-06T10:14:18.000Z" />
    <meta property="article:tag" content="word embeddings" />
    <meta property="article:tag" content="natural language processing" />
    <meta property="article:tag" content="language models" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="On word embeddings - Part 1" />
    <meta name="twitter:description" content="Word embeddings popularized by word2vec are pervasive in current NLP applications. The history of word embeddings, however, goes back a lot further. This post explores the history of word embeddings in the context of language modelling." />
    <meta name="twitter:url" content="https://ruder.io/word-embeddings-1/" />
    <meta name="twitter:image" content="https://ruder.io/content/images/2016/04/word_embeddings_colah.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Sebastian Ruder" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="word embeddings, natural language processing, language models" />
    <meta name="twitter:site" content="@seb_ruder" />
    <meta property="og:image:width" content="993" />
    <meta property="og:image:height" content="813" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Sebastian Ruder",
        "url": "https://ruder.io/",
        "logo": {
            "@type": "ImageObject",
            "url": {
                "@type": "ImageObject",
                "url": "https://ruder.io/favicon.ico",
                "width": 48,
                "height": 48
            }
        }
    },
    "author": {
        "@type": "Person",
        "name": "Sebastian Ruder",
        "image": {
            "@type": "ImageObject",
            "url": "https://ruder.io/content/images/2019/02/new_profile_photo_square-1.jpg",
            "width": 2000,
            "height": 2000
        },
        "url": "https://ruder.io/author/sebastian/",
        "sameAs": []
    },
    "headline": "On word embeddings - Part 1",
    "url": "https://ruder.io/word-embeddings-1/",
    "datePublished": "2016-04-11T14:00:00.000Z",
    "dateModified": "2019-10-06T10:14:18.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://ruder.io/content/images/2016/04/word_embeddings_colah.png",
        "width": 993,
        "height": 813
    },
    "keywords": "word embeddings, natural language processing, language models",
    "description": "Word embeddings popularized by word2vec are pervasive in current NLP applications. The history of word embeddings, however, goes back a lot further. This post explores the history of word embeddings in the context of language modelling.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://ruder.io/"
    }
}
    </script>

    <meta name="generator" content="Ghost 3.11" />
    <link rel="alternate" type="application/rss+xml" title="Sebastian Ruder" href="https://ruder.io/rss/" />
    <script>
var profile_title = 'Sebastian Ruder';
</script>
<script>
var disqus_shortname = 'sebastianruder';
</script>
<script>
var profile_resume ='NLP PhD student';
</script>
<script>
var ga_id = 'UA-60512592-1';
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [["$", "$"], ["\\(", "\\)"]],
        processEscapes: true
    }
});
</script>

</head>
<body class="post-template tag-word-embeddings tag-natural-language-processing tag-language-models">

    <div class="site-wrapper">

        

<header class="site-header">
    <div class="outer site-nav-main">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left-wrapper">
        <div class="site-nav-left">
                <a class="site-nav-logo" href="https://ruder.io">Sebastian Ruder</a>
            <div class="site-nav-content">
                    <ul class="nav" role="menu">
    <li class="nav-about" role="menuitem"><a href="https://ruder.io/about/">About</a></li>
    <li class="nav-tags" role="menuitem"><a href="https://ruder.io/tags/">Tags</a></li>
    <li class="nav-papers" role="menuitem"><a href="https://ruder.io/publications/">Papers</a></li>
    <li class="nav-talks" role="menuitem"><a href="https://ruder.io/talks/">Talks</a></li>
    <li class="nav-news" role="menuitem"><a href="https://ruder.io/news/">News</a></li>
    <li class="nav-faq" role="menuitem"><a href="https://ruder.io/faq/">FAQ</a></li>
    <li class="nav-sign-up-for-nlp-news" role="menuitem"><a href="https://ruder.io/nlp-news/">Sign up for NLP News</a></li>
    <li class="nav-nlp-progress" role="menuitem"><a href="https://nlpprogress.com/">NLP Progress</a></li>
    <li class="nav-contact" role="menuitem"><a href="https://ruder.io/contact/">Contact</a></li>
</ul>

                    <span class="nav-post-title dash">On word embeddings - Part 1</span>
            </div>
        </div>
    </div>
    <div class="site-nav-right">
            <div class="social-links">
                    <a class="social-link social-link-tw" href="https://twitter.com/seb_ruder" title="Twitter" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
</a>
            </div>
                <a class="rss-button" href="https://ruder.io/rss/index.rss" title="RSS" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
</a>

    </div>
</nav>
    </div>
</div></header>


<main id="site-main" class="site-main outer">
    <div class="inner">

        <article class="post-full post tag-word-embeddings tag-natural-language-processing tag-language-models ">

            <header class="post-full-header">

                <section class="post-full-tags">
                    <a href="../tag/word-embeddings/index.html">word embeddings</a>
                </section>

                <h1 class="post-full-title">On word embeddings - Part 1</h1>

                <p class="post-full-custom-excerpt">Word embeddings popularized by word2vec are pervasive in current NLP applications. The history of word embeddings, however, goes back a lot further. This post explores the history of word embeddings in the context of language modelling.</p>

                <div class="post-full-byline">

                    <section class="post-full-byline-content">

                        <ul class="author-list">
                            <li class="author-list-item">

                                <div class="author-card">
                                    <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                                    <div class="author-info">
                                        <h2>Sebastian Ruder</h2>
                                        <p>Read <a href="../author/sebastian/index.html">more posts</a> by this author.</p>
                                    </div>
                                </div>

                                <a href="../author/sebastian/index.html" class="author-avatar">
                                    <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                                </a>

                            </li>
                        </ul>

                        <section class="post-full-byline-meta">
                            <h4 class="author-name"><a href="../author/sebastian/index.html">Sebastian Ruder</a></h4>
                            <div class="byline-meta-content">
                                <time class="byline-meta-date" datetime="2016-04-11">11 Apr 2016</time>
                                <span class="byline-reading-time"><span class="bull">&bull;</span> 15 min read</span>
                            </div>
                        </section>

                    </section>


                </div>
            </header>

            <figure class="post-full-image">
                <img
                    srcset="../content/images/size/w300/2016/04/word_embeddings_colah.png 300w,
                           ../content/images/size/w600/2016/04/word_embeddings_colah.png 600w,
                          ../content/images/size/w1000/2016/04/word_embeddings_colah.png 1000w,
                         ../content/images/size/w2000/2016/04/word_embeddings_colah.png 2000w"
                    sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px"
                    src="../content/images/size/w2000/2016/04/word_embeddings_colah.png"
                    alt="On word embeddings - Part 1"
                />
            </figure>

            <section class="post-full-content">
                <div class="post-content">
                    <!--kg-card-begin: markdown--><p>This post presents the most well-known models for learning word embeddings based on language modelling.</p>
<p>Table of contents:</p>
<ul>
<li><a href="index.html#abriefhistoryofwordembeddings">A brief history of word embeddings</a></li>
<li><a href="index.html#wordembeddingmodels">Word embedding models</a>
<ul>
<li><a href="index.html#anoteonlanguagemodelling">A note on language modelling</a></li>
<li><a href="index.html#classicneurallanguagemodel">Classic neural language model</a></li>
<li><a href="index.html#cwmodel">C&amp;W model</a></li>
<li><a href="index.html#word2vec">Word2Vec</a>
<ul>
<li><a href="index.html#continuousbagofwordscbow">CBOW</a></li>
<li><a href="index.html#skipgram">Skip-gram</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Unsupervisedly learned word embeddings have been exceptionally successful in many NLP tasks and are frequently seen as something akin to a <em>silver bullet</em>. In fact, in many NLP architectures, they have almost completely replaced traditional distributional features such as Brown clusters and LSA features.</p>
<p>Proceedings of last year's <a href="https://aclweb.org/anthology/P/P15/">ACL</a> and <a href="https://aclweb.org/anthology/D/D15/">EMNLP</a> conferences have been dominated by word embeddings, with some people musing that <em>Embedding Methods in Natural Language Processing</em> was a more fitting name for EMNLP. This year's ACL features not <a href="https://sites.google.com/site/repl4nlp2016/">one</a> but <a href="https://sites.google.com/site/repevalacl16/">two</a> workshops on word embeddings.</p>
<p>Semantic relations between word embeddings seem nothing short of magical to the uninitiated and Deep Learning NLP talks frequently prelude with the notorious \(king - man + woman \approx queen \) slide, while <a href="http://cacm.acm.org/magazines/2016/3/198856-deep-or-shallow-nlp-is-breaking-out/fulltext">a recent article</a> in <em>Communications of the ACM</em> hails word embeddings as the primary reason for NLP's breakout.</p>
<p>This post will be the first in a series that aims to give an extensive overview of word embeddings showcasing why this hype may or may not be warranted. In the course of this review, we will try to connect the disperse literature on word embedding models, highlighting many models, applications and interesting features of word embeddings, with a focus on multilingual embedding models and word embedding evaluation tasks in later posts.<br>
This first post lays the foundations by presenting current word embeddings based on language modelling. While many of these models have been discussed at length, we hope that investigating and discussing their merits in the context of past and current research will provide new insights.</p>
<p>A brief note on nomenclature: In the following we will use the currently prevalent term <em>word embeddings</em> to refer to dense representations of words in a low-dimensional vector space. Interchangeable terms are <em>word vectors</em> and <em>distributed representations</em>. We will particularly focus on <em>neural word embeddings</em>, i.e. word embeddings learned by a neural network.</p>
<h1 id="abriefhistoryofwordembeddings">A brief history of word embeddings</h1>
<p>Since the 1990s, vector space models have been used in distributional semantics. During this time, many models for estimating continuous representations of words have been developed, including Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). Have a look at <a href="https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/">this blog post</a> for a more detailed overview of distributional semantics history in the context of word embeddings.</p>
<p>Bengio et al. refer to word embeddings as <em>distributed representations of words</em> in 2003 and train them in a neural language model jointly with the model's parameters. First to show the utility of pre-trained word embeddings were arguably Collobert and Weston in 2008. Their landmark paper <em>A unified architecture for natural language processing</em> not only establishes word embeddings as a useful tool for downstream tasks, but also introduces a neural network architecture that forms the foundation for many current approaches. However, the eventual popularization of word embeddings can be attributed to Mikolov et al. in 2013 who created word2vec, a toolkit that allows the seamless training and use of pre-trained embeddings. In 2014, Pennington et al. released GloVe, a competitive set of pre-trained word embeddings, signalling that word embeddings had reached the main stream.</p>
<p>Word embeddings are one of the few currently successful applications of unsupervised learning. Their main benefit arguably is that they don't require expensive annotation, but can be derived from large unannotated corpora that are readily available. Pre-trained embeddings can then be used in downstream tasks that use small amounts of labeled data.</p>
<h1 id="wordembeddingmodels">Word embedding models</h1>
<p>Naturally, every feed-forward neural network that takes words from a vocabulary as input and embeds them as vectors into a lower dimensional space, which it then fine-tunes through back-propagation, necessarily yields word embeddings as the weights of the first layer, which is usually referred to as <em>Embedding Layer</em>.</p>
<p>The main difference between such a network that produces word embeddings as a by-product and a method such as word2vec whose explicit goal is the generation of word embeddings is its computational complexity. Generating word embeddings with a very deep architecture is simply too computationally expensive for a large vocabulary. This is the main reason why it took until 2013 for word embeddings to explode onto the NLP stage; computational complexity is a key trade-off for word embedding models and will be a recurring theme in our review.</p>
<p>Another difference is the training objective: word2vec and GloVe are geared towards producing word embeddings that encode general semantic relationships, which are beneficial to many downstream tasks; notably, word embeddings trained this way won't be helpful in tasks that do not rely on these kind of relationships. In contrast, regular neural networks typically produce task-specific embeddings that are only of limited use elsewhere. Note that a task that relies on semantically coherent representations such as language modelling will produce similar embeddings to word embedding models, which we will investigate in the next chapter.<br>
As a side-note, word2vec and Glove might be said to be to NLP what <a href="https://github.com/BVLC/caffe/wiki/Model-Zoo">VGGNet</a> is to vision, i.e. a common weight initialisation that provides generally helpful features without the need for lengthy training.</p>
<p>To facilitate comparison between models, we assume the following notational standards: We assume a training corpus containing a sequence of \(T\) training words \(w_1, w_2, w_3, \cdots, w_T\) that belong to a vocabulary \(V\) whose size is \(|V|\). Our models generally consider a context of \( n \) words. We associate every word with an input embedding \( v_w \) (the eponymous word embedding in the Embedding Layer) with \(d\) dimensions and an output embedding \( v'_w \) (another word representation whose role will soon become clearer). We finally optimize an objective function \(J_\theta\) with regard to our model parameters \(\theta\) and our model outputs some score \(f_\theta(x)\) for every input \( x \).</p>
<h2 id="anoteonlanguagemodelling">A note on language modelling</h2>
<p>Word embedding models are quite closely intertwined with language models. The quality of language models is measured based on their ability to learn a probability distribution over words in \( V \). In fact, many state-of-the-art word embedding models try to predict the next word in a sequence to some extent. Additionally, word embedding models are often evaluated using <a href="https://en.wikipedia.org/wiki/Perplexity">perplexity</a>, a cross-entropy based measure borrowed from language modelling.</p>
<p>Before we get into the gritty details of word embedding models, let us briefly talk about some language modelling fundamentals.</p>
<p>Language models generally try to compute the probability of a word \(w_t\) given its \(n - 1\) previous words, i.e. \(p(w_t \: | \: w_{t-1} , \cdots w_{t-n+1})\). By applying the chain rule together with the Markov assumption, we can approximate the probability of a whole sentence or document by the product of the probabilities of each word given its \(n\) previous words:</p>
<p>\(p(w_1 , \cdots , w_T) = \prod\limits_i p(w_i \: | \: w_{i-1} , \cdots , w_{i-n+1}) \).</p>
<p>In n-gram based language models, we can calculate a word's probability based on the frequencies of its constituent n-grams:<br>
\( p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1}) = \dfrac{count(w_{t-n+1}, \cdots , w_{t-1},w_t)}{count({w_{t-n+1}, \cdots , w_{t-1}})}\).</p>
<p>Setting \(n = 2\) yields bigram probabilities, while \(n = 5\) together with Kneser-Ney smoothing leads to smoothed 5-gram models that have been found to be a strong baseline for language modelling. For more details, you can refer to <a href="https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf">these slides</a> from Stanford.</p>
<p>In neural networks, we achieve the same objective using the well-known softmax layer:</p>
<p>\(p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1}) = \dfrac{\text{exp}({h^\top v'_{w_t}})}{\sum_{w_i \in V} \text{exp}({h^\top v'_{w_i}})} \).</p>
<p>The inner product \( h^\top v'_{w_t} \) computes the (unnormalized) log-probability of word \( w_t \), which we normalize by the sum of the log-probabilities of all words in \( V \). \(h\) is the output vector of the penultimate network layer (the hidden layer in the feed-forward network in Figure 1), while \(v'_w\) is the output embedding of word \(w \), i.e. its representation in the weight matrix of the softmax layer. Note that even though \(v'_w\) represents the word \(w\), it is learned separately from the input word embedding \(v_w\), as the multiplications both vectors are involved in differ (\(v_w\) is multiplied with an index vector, \(v'_w\) with \(h\)).</p>
<figure>
      <img src="https://ruder.io/content/images/2016/04/nn_language_model-1.jpg" style="width: 80%" title="SGD without momentum">
<figcaption>Figure 1: A neural language model (Bengio et al., 2006)</figcaption>
</figure>
<p>Note that we need to calculate the probability of every word \( w \) at the output layer of the neural network. To do this efficiently, we perform a matrix multiplication between \(h\) and a weight matrix whose rows consist of \(v'_w\) of all words \(w\) in \(V\). We then feed the resulting vector, which is often referred to as a logit, i.e. the output of a previous layer that is not a probability, with \(d = |V|\) into the softmax, while the softmax layer &quot;squashes&quot; the vector to a probability distribution over the words in \(V\).</p>
<p>Note that the softmax layer (in contrast to the previous n-gram calculations) only implicitly takes into account \(n\) previous words: LSTMs, which are typically used for neural language models, encode these in their state \(h\), while Bengio's neural language model, which we will see in the next chapter, feeds the previous \(n\) words through a feed-forward layer.</p>
<p>Keep this softmax layer in mind, as many of the subsequent word embedding models will use it in some fashion.</p>
<p>Using this softmax layer, the model tries to maximize the probability of predicting the correct word at every timestep \( t \). The whole model thus tries to maximize the averaged log probability of the whole corpus:</p>
<p>\(J_\theta = \frac{1}{T} \text{log} \space p(w_1 , \cdots , w_T)\).</p>
<p>Analogously, through application of the chain rule, it is usually trained to maximize the average of the log probabilities of all words in the corpus given their previous \( n \) words:</p>
<p>\(J_\theta = \frac{1}{T}\sum\limits_{t=1}^T\ \text{log} \space p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1})\).</p>
<p>To sample words from the language model at test time, we can either greedily choose the word with the highest probability \(p(w_t \: | \: w_{t-1} \cdots w_{t-n+1})\) at every time step \( t \) or use beam search. We can do this for instance to generate arbitrary text sequences as in <a href="https://github.com/karpathy/char-rnn">Karpathy's Char-RNN</a> or as part of a sequence prediction task, where an LSTM is used as the decoder.</p>
<h2 id="classicneurallanguagemodel">Classic neural language model</h2>
<p>The classic neural language model proposed by  Bengio et al. <sup class="footnote-ref"><a href="index.html#fn1" id="fnref1">[1]</a></sup> in 2003 consists of a one-hidden layer feed-forward neural network that predicts the next word in a sequence as in Figure 2.</p>
<figure>
      <img src="https://ruder.io/content/images/2016/04/bengio_language_model.png" style="width: 70%" title="Language model by Bengio et al.">
<figcaption>Figure 2: Classic neural language model (Bengio et al., 2003)</figcaption>
</figure>
<p>Their model maximizes what we've described above as the prototypical neural language model objective (we omit the regularization term for simplicity):</p>
<p>\(J_\theta = \frac{1}{T}\sum\limits_{t=1}^T\ \text{log} \space f(w_t , w_{t-1} , \cdots , w_{t-n+1})\).</p>
<p>\( f(w_t , w_{t-1} , \cdots , w_{t-n+1}) \) is the output of the model, i.e. the probability \( p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1}) \) as computed by the softmax, where \(n \) is the number of previous words fed into the model.</p>
<p>Bengio et al. are one of the first to introduce what we now refer to as a word embedding, a real-valued word feature vector in \(\mathbb{R}\). Their architecture forms very much the prototype upon which current approaches have gradually improved. The general building blocks of their model, however, are still found in all current neural language and word embedding models. These are:</p>
<ol>
<li><strong>Embedding Layer</strong>: a layer that generates word embeddings by multiplying an index vector with a word embedding matrix;</li>
<li><strong>Intermediate Layer(s)</strong>: one or more layers that produce an intermediate representation of the input, e.g. a fully-connected layer that applies a non-linearity to the concatenation of word embeddings of \(n\) previous words;</li>
<li><strong>Softmax Layer</strong>: the final layer that produces a probability distribution over words in \(V\).</li>
</ol>
<p>Additionally, Bengio et al. identify two issues that lie at the heart of current state-of-the-art-models:</p>
<ul>
<li>They remark that <strong>2.</strong> can be replaced by an LSTM, which is used by state-of-the-art neural language models <sup class="footnote-ref"><a href="index.html#fn2" id="fnref2">[2]</a></sup> <sup class="footnote-ref"><a href="index.html#fn3" id="fnref3">[3]</a></sup>.</li>
<li>They identify the final softmax layer (more precisely: the normalization term) as the network's main bottleneck, as the cost of computing the softmax is proportional to the number of words in \(V\), which is typically on the order of hundreds of thousands or millions.</li>
</ul>
<p>Finding ways to mitigate the computational cost associated with computing the softmax over a large vocabulary <sup class="footnote-ref"><a href="index.html#fn4" id="fnref4">[4]</a></sup> is thus one of the key challenges both in neural language models as well as in word embedding models.</p>
<h2 id="cwmodel">C&amp;W model</h2>
<p>After Bengio et al.'s first steps in neural language models, research in word embeddings stagnated as computing power and algorithms did not yet allow the training of a large vocabulary.</p>
<p>Collobert and Weston <sup class="footnote-ref"><a href="index.html#fn5" id="fnref5">[5]</a></sup> (thus C&amp;W) showcase in 2008 that word embeddings trained on a sufficiently large dataset carry syntactic and semantic meaning and improve performance on downstream tasks. They elaborate upon this in their 2011 paper <sup class="footnote-ref"><a href="index.html#fn6" id="fnref6">[6]</a></sup>.</p>
<p>Their solution to avoid computing the expensive softmax is to use a different objective function: Instead of the cross-entropy criterion of Bengio et al., which maximizes the probability of the next word given the previous words, Collobert and Weston train a network to output a higher score \(f_\theta\) for a correct word sequence (a probable word sequence in Bengio's model) than for an incorrect one. For this purpose, they use a pairwise ranking criterion, which looks like this:</p>
<p>\(J_\theta\ = \sum\limits_{x \in X} \sum\limits_{w \in V} \text{max} \lbrace 0, 1 - f_\theta(x) + f_\theta(x^{(w)}) \rbrace \).</p>
<p>They sample correct windows \(x\) containing \(n\) words from the set of all possible windows \(X\) in their corpus. For each window \(x\), they then produce a corrupted, incorrect version \(x^{(w)}\) by replacing \(x\)'s centre word with another word \(w\) from \(V\). By minimizing the objective, the model will now learn to assign a score for the correct window that is higher than the score for the incorrect window by at least a margin of \(1\). Their model architecture, depicted in Figure 3 without the ranking objective, is analogous to Bengio et al.'s model.</p>
<figure>
      <img src="https://ruder.io/content/images/2016/09/nlp_almost_from_scratch_window_approach.png" style="width: 50%" title="C&W model">
<figcaption>Figure 3: The C&W model without ranking objective (Collobert et al., 2011)</figcaption>
</figure>
<p>The resulting language model produces embeddings that already possess many of the relations word embeddings have become known for, e.g. countries are clustered close together and syntactically similar words occupy similar locations in the vector space. While their ranking objective eliminates the complexity of the softmax, they keep the intermediate fully-connected hidden layer (<strong>2.</strong>) of Bengio et al. around (the <strong>HardTanh</strong> layer in Figure 3), which constitutes another source of expensive computation. Partially due to this, their full model trains for seven weeks in total with \(|V| = 130000\).</p>
<h2 id="word2vec">Word2Vec</h2>
<p>Let us now introduce arguably the most popular word embedding model, the model that launched a thousand word embedding papers: word2vec, the subject of two papers by Mikolov et al. in 2013. As word embeddings are a key building block of deep learning models for NLP, word2vec is often assumed to belong to the same group. Technically however, word2vec is not be considered to be part of deep learning, as its architecture is neither deep nor uses non-linearities (in contrast to Bengio's model and the C&amp;W model).</p>
<p>In their first paper <sup class="footnote-ref"><a href="index.html#fn7" id="fnref7">[7]</a></sup>, Mikolov et al. propose two architectures for learning word embeddings that are computationally less expensive than previous models. In their second paper <sup class="footnote-ref"><a href="index.html#fn8" id="fnref8">[8]</a></sup>, they improve upon these models by employing additional strategies to enhance training speed and accuracy.<br>
These architectures offer two main benefits over the C&amp;W model and Bengio's language model:</p>
<ul>
<li>They do away with the expensive hidden layer.</li>
<li>They enable the language model to take additional context into account.</li>
</ul>
<p>As we will later show, the success of their model is not only due to these changes, but especially due to certain training strategies.</p>
<p>In the following, we will look at both of these architectures:</p>
<h3 id="continuousbagofwordscbow">Continuous bag-of-words (CBOW)</h3>
<p>While a language model is only able to look at the past words for its predictions, as it is evaluated on its ability to predict each next word in the corpus, a model that just aims to generate accurate word embeddings does not suffer from this restriction. Mikolov et al. thus use both the \(n\) words before and after the target word \( w_t \) to predict it as depicted in Figure 4. They call this continuous bag-of-words (CBOW), as it uses continuous representations whose order is of no importance.</p>
<figure>
      <img src="https://ruder.io/content/images/2016/02/cbow.png" style="width: 50%" title="Continuous bag-of-words">
<figcaption>Figure 4: Continuous bag-of-words (Mikolov et al., 2013)</figcaption>
</figure>
<p>The objective function of CBOW in turn is only slightly different than the language model one:</p>
<p>\(J_\theta = \frac{1}{T}\sum\limits_{t=1}^T\ \text{log} \space p(w_t \: | \: w_{t-n} , \cdots , w_{t-1}, w_{t+1}, \cdots , w_{t+n})\).</p>
<p>Instead of feeding \( n \) previous words into the model, the model receives a window of \( n \) words around the target word \( w_t \) at each time step \( t \).</p>
<h3 id="skipgram">Skip-gram</h3>
<p>While CBOW can be seen as a precognitive language model, skip-gram turns the language model objective on its head: Instead of using the surrounding words to predict the centre word as with CBOW, skip-gram uses the centre word to predict the surrounding words as can be seen in Figure 5.</p>
<figure>
      <img src="https://ruder.io/content/images/2016/02/skip-gram.png" style="width: 50%" title="Skip-gram">
<figcaption>Figure 5: Skip-gram (Mikolov et al., 2013)</figcaption>
</figure>
<p>The skip-gram objective thus sums the log probabilities of the surrounding \( n \) words  to the left and to the right of the target word \( w_t \) to produce the following objective:</p>
<p>\(J_\theta = \frac{1}{T}\sum\limits_{t=1}^T\ \sum\limits_{-n \leq j \leq n , \neq 0} \text{log} \space p(w_{t+j} \: | \: w_t)\).</p>
<p>To gain a better intuition of how the skip-gram model computes \( p(w_{t+j} \: | \: w_t) \), let's recall the definition of our softmax:</p>
<p>\(p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1}) = \dfrac{\text{exp}({h^\top v'_{w_t}})}{\sum_{w_i \in V} \text{exp}({h^\top v'_{w_i}})} \).</p>
<p>Instead of computing the probability of the target word \( w_t \) given its previous words, we calculate the probability of the surrounding word \( w_{t+j} \) given \( w_t \). We can thus simply replace these variables in the equation:</p>
<p>\(p(w_{t+j} \: | \: w_t ) = \dfrac{\text{exp}({h^\top v'_{w_{t+j}}})}{\sum_{w_i \in V} \text{exp}({h^\top v'_{w_i}})} \).</p>
<p>As the skip-gram architecture does not contain a hidden layer that produces an intermediate state vector \(h\), \(h\) is simply the word embedding \(v_{w_t}\) of the input word \(w_t\). This also makes it clearer why we want to have different representations for input embeddings \(v_w\) and output embeddings \(v'_w\), as we would otherwise multiply the word embedding by itself. Replacing \(h \) with \(v_{w_t}\) yields:</p>
<p>\(p(w_{t+j} \: | \: w_t ) = \dfrac{\text{exp}({v^\top_{w_t} v'_{w_{t+j}}})}{\sum_{w_i \in V} \text{exp}({v^\top_{w_t} v'_{w_i}})} \).</p>
<p>Note that the notation in Mikolov's paper differs slightly from ours, as they denote the centre word with \( w_I \) and the surrounding words with \( w_O \). If we replace \( w_t \) with \( w_I \), \( w_{t+j} \) with \( w_O \), and swap the vectors in the inner product due to its commutativity, we arrive at the softmax notation in their paper:</p>
<p>\(p(w_O|w_I) = \dfrac{\text{exp}(v'^\top_{w_O} v_{w_I})}{\sum^V_{w=1}\text{exp}(v'^\top_{w} v_{w_I})}\).</p>
<p>In the next post, we will discuss different ways to approximate the expensive softmax as well as key training decisions that account for much of skip-gram's success. We will also introduce GloVe <sup class="footnote-ref"><a href="index.html#fn9" id="fnref9">[9]</a></sup>, a word embedding model based on matrix factorisation and discuss the link between word embeddings and methods from distributional semantics.</p>
<p>Did I miss anything? <strong>Let me know in the comments below.</strong></p>
<h1 id="otherblogpostsonwordembeddings">Other blog posts on word embeddings</h1>
<p>If you want to learn more about word embeddings, these other blog posts on word embeddings are also available:</p>
<ul>
<li><a href="http://ruder.io/word-embeddings-softmax/index.html">On word embeddings - Part 2: Approximating the softmax</a></li>
<li><a href="http://ruder.io/secret-word2vec/index.html">On word embeddings - Part 3: The secret ingredients of word2vec</a></li>
<li><a href="http://ruder.io/cross-lingual-embeddings/index.html">Unofficial Part 4: A survey of cross-lingual embedding models</a></li>
<li><a href="http://ruder.io/word-embeddings-2017/index.html">Unofficial Part 5: Word embeddings in 2017 -  Trends and future directions</a></li>
</ul>
<h1 id="translations">Translations</h1>
<p>This blog post has been translated into the following languages:</p>
<ul>
<li><a href="http://geek.csdn.net/news/detail/111466">Chinese</a></li>
</ul>
<p>Credit for the post image goes to <a href="http://colah.github.io/">Christopher Olah</a>.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Bengio, Y., Ducharme, R., Vincent, P., &amp; Janvin, C. (2003). A Neural Probabilistic Language Model. The Journal of Machine Learning Research, 3, 1137–1155. Retrieved from <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf</a> <a href="index.html#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Kim, Y., Jernite, Y., Sontag, D., &amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from <a href="http://arxiv.org/abs/1508.06615">http://arxiv.org/abs/1508.06615</a> <a href="index.html#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. Retrieved from <a href="http://arxiv.org/abs/1602.02410">http://arxiv.org/abs/1602.02410</a> <a href="index.html#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>Chen, W., Grangier, D., &amp; Auli, M. (2015). Strategies for Training Large Vocabulary Neural Language Models, 12. Retrieved from <a href="http://arxiv.org/abs/1512.04906">http://arxiv.org/abs/1512.04906</a> <a href="index.html#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>Collobert, R., &amp; Weston, J. (2008). A unified architecture for natural language processing. Proceedings of the 25th International Conference on Machine Learning - ICML ’08, 20(1), 160–167. <a href="http://doi.org/10.1145/1390156.1390177">http://doi.org/10.1145/1390156.1390177</a> <a href="index.html#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &amp; Kuksa, P. (2011). Natural Language Processing (almost) from Scratch. Journal of Machine Learning Research, 12 (Aug), 2493–2537. Retrieved from <a href="http://arxiv.org/abs/1103.0398">http://arxiv.org/abs/1103.0398</a> <a href="index.html#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p>Mikolov, T., Corrado, G., Chen, K., &amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. <a href="index.html#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS, 1–9. <a href="index.html#fnref8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p>Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543. <a href="http://doi.org/10.3115/v1/D14-1162">http://doi.org/10.3115/v1/D14-1162</a> <a href="index.html#fnref9" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
<!--kg-card-end: markdown-->
                </div>
            </section>



        </article>

    </div>
</main>

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
                <article class="read-next-card">
                    <header class="read-next-card-header">
                        <h3><span>More in</span> <a href="../tag/word-embeddings/index.html">word embeddings</a></h3>
                    </header>
                    <div class="read-next-card-content">
                        <ul>
                            <li>
                                <h4><a href="../aaai-2019-highlights/index.html">AAAI 2019 Highlights: Dialogue, reproducibility, and more</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2019-02-07">7 Feb 2019</time> –
                                        11 min read</p>
                                </div>
                            </li>
                            <li>
                                <h4><a href="../emnlp-2018-highlights/index.html">EMNLP 2018 Highlights: Inductive bias, cross-lingual learning, and more</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2018-11-06">6 Nov 2018</time> –
                                        11 min read</p>
                                </div>
                            </li>
                            <li>
                                <h4><a href="../a-review-of-the-recent-history-of-nlp/index.html">A Review of the Neural History of Natural Language Processing</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2018-10-01">1 Oct 2018</time> –
                                        29 min read</p>
                                </div>
                            </li>
                        </ul>
                    </div>
                    <footer class="read-next-card-footer">
                        <a href="../tag/word-embeddings/index.html">See all 9 posts
                            →</a>
                    </footer>
                </article>

                <article class="post-card post tag-word-embeddings tag-natural-language-processing tag-language-models ">

    <a class="post-card-image-link" href="../word-embeddings-softmax/index.html">
        <img class="post-card-image"
            srcset="../content/images/size/w300/2016/06/softmax_classifier.png 300w,
                   ../content/images/size/w600/2016/06/softmax_classifier.png 600w,
                  ../content/images/size/w1000/2016/06/softmax_classifier.png 1000w,
                 ../content/images/size/w2000/2016/06/softmax_classifier.png 2000w"
            sizes="(max-width: 1000px) 400px, 700px"
            src="../content/images/size/w600/2016/06/softmax_classifier.png"
            alt="On word embeddings - Part 2: Approximating the Softmax"
        />
    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="../word-embeddings-softmax/index.html">

            <header class="post-card-header">
                    <div class="post-card-primary-tag">word embeddings</div>
                <h2 class="post-card-title">On word embeddings - Part 2: Approximating the Softmax</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>The softmax layer is a core part of many current neural network architectures. When the number of output classes is very large, such as in the case of language modelling, computing the softmax becomes very expensive. This post explores approximations to make the computation more efficient.</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Sebastian Ruder
                    </div>
            
                    <a href="../author/sebastian/index.html" class="static-avatar">
                        <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/sebastian/index.html">Sebastian Ruder</a></span>
                <span class="post-card-byline-date"><time datetime="2016-06-13">13 Jun 2016</time> <span class="bull">&bull;</span> 33 min read</span>
            </div>
        </footer>

    </div>

</article>

                <article class="post-card post tag-optimization ">

    <a class="post-card-image-link" href="../optimizing-gradient-descent/index.html">
        <img class="post-card-image"
            srcset="../content/images/size/w300/2016/09/loss_function_image_tumblr.png 300w,
                   ../content/images/size/w600/2016/09/loss_function_image_tumblr.png 600w,
                  ../content/images/size/w1000/2016/09/loss_function_image_tumblr.png 1000w,
                 ../content/images/size/w2000/2016/09/loss_function_image_tumblr.png 2000w"
            sizes="(max-width: 1000px) 400px, 700px"
            src="../content/images/size/w600/2016/09/loss_function_image_tumblr.png"
            alt="An overview of gradient descent optimization algorithms"
        />
    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="../optimizing-gradient-descent/index.html">

            <header class="post-card-header">
                    <div class="post-card-primary-tag">optimization</div>
                <h2 class="post-card-title">An overview of gradient descent optimization algorithms</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>Gradient descent is the preferred way to optimize neural networks and many other machine learning algorithms but is often used as a black box. This post explores how many of the most popular gradient-based optimization algorithms such as Momentum, Adagrad, and Adam actually work.</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Sebastian Ruder
                    </div>
            
                    <a href="../author/sebastian/index.html" class="static-avatar">
                        <img class="author-profile-image" src="../content/images/size/w100/2019/02/new_profile_photo_square-1.jpg" alt="Sebastian Ruder" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/sebastian/index.html">Sebastian Ruder</a></span>
                <span class="post-card-byline-date"><time datetime="2016-01-19">19 Jan 2016</time> <span class="bull">&bull;</span> 28 min read</span>
            </div>
        </footer>

    </div>

</article>
        </div>
    </div>
</aside>




        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://ruder.io">Sebastian Ruder</a> &copy; 2020</section>
                <nav class="site-footer-nav">
                    <a href="https://ruder.io">Latest Posts</a>
                    
                    <a href="https://twitter.com/seb_ruder" target="_blank" rel="noopener">Twitter</a>
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>


    <script
        src="https://code.jquery.com/jquery-3.4.1.min.js"
        integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
        crossorigin="anonymous">
    </script>
    <script src="../assets/built/casper.js?v=8587ecf4f6"></script>

    <script>
        // Parse the URL parameter
        function getParameterByName(name, url) {
            if (!url) url = window.location.href;
            name = name.replace(/[\[\]]/g, "\\$&");
            var regex = new RegExp("[?&]" + name + "(=([^&#]*)|&|#|$)"),
                results = regex.exec(url);
            if (!results) return null;
            if (!results[2]) return '';
            return decodeURIComponent(results[2].replace(/\+/g, " "));
        }

        // Give the parameter a variable name
        var action = getParameterByName('action');

        $(document).ready(function () {
            if (action == 'subscribe') {
                $('body').addClass("subscribe-success");
            }

            $('.subscribe-success-message .subscribe-close').click(function () {
                $('.subscribe-success-message').addClass('close');
            });

            // Reset form on opening subscrion overlay
            $('.subscribe-button').click(function() {
                $('.subscribe-overlay form').removeClass();
                $('.subscribe-email').val('');
            });
        });
    </script>

    <script>
    $(document).ready(function () {
        // FitVids - start
        var $postContent = $(".post-full-content");
        $postContent.fitVids();
        // FitVids - end

        // Replace nav with title on scroll - start
        Casper.stickyNavTitle({
            navSelector: '.site-nav-main',
            titleSelector: '.post-full-title',
            activeClass: 'nav-post-title-active'
        });
        // Replace nav with title on scroll - end

        // Hover on avatar
        var hoverTimeout;
        $('.author-list-item').hover(function () {
            var $this = $(this);

            clearTimeout(hoverTimeout);

            $('.author-card').removeClass('hovered');
            $(this).children('.author-card').addClass('hovered');

        }, function () {
            var $this = $(this);

            hoverTimeout = setTimeout(function () {
                $this.children('.author-card').removeClass('hovered');
            }, 800);
        });
    });
</script>


    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/zepto/1.1.6/zepto.min.js"></script>
<script>jQuery = Zepto</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism.min.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/plugins/line-numbers/prism-line-numbers.min.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism-coy.min.css" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/components/prism-python.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-60512592-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-60512592-1');
</script>

</body>
</html>

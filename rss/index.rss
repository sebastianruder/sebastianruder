<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Sebastian Ruder]]></title><description><![CDATA[I'm a research scientist at Google. I blog about natural language processing and machine learning.]]></description><link>http://ruder.io/</link><image><url>http://ruder.io/favicon.png</url><title>Sebastian Ruder</title><link>http://ruder.io/</link></image><generator>Ghost 3.11</generator><lastBuildDate>Mon, 14 Nov 2022 20:20:40 GMT</lastBuildDate><atom:link href="http://ruder.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[The State of Multilingual AI]]></title><description><![CDATA[This post takes a closer look at the state of multilingual AI. How multilingual are current models in NLP, computer vision, and speech? What are the main recent contributions in this area? What challenges remain and how we can we address them?]]></description><link>http://ruder.io/state-of-multilingual-ai/</link><guid isPermaLink="false">632ec478b681766fe285d3c0</guid><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Sat, 24 Sep 2022 10:47:44 GMT</pubDate><media:content url="http://ruder.io/content/images/2022/11/language_models_non-english.png" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="http://ruder.io/content/images/2022/11/language_models_non-english.png" alt="The State of Multilingual AI"><p>Models that allow interaction via natural language have become ubiquitious. Research models such as BERT and T5 have become much more accessible while the latest generation of language and multi-modal models are demonstrating increasingly powerful capabilities. At the same time, a <a href="https://www.forbes.com/sites/robtoews/2022/03/27/a-wave-of-billion-dollar-language-ai-startups-is-coming/?sh=3517fd212b14">wave of</a> <a href="https://techcrunch.com/2022/07/28/a-gold-rush-of-nlp-startups-is-about-to-arrive-heres-why">NLP startups</a> has started to put this technology to practical use.</p>
<p>While such language technology may be hugely impactful, recent models have mostly focused on English and a handful of other languages with large amounts of resources. Developing models that work for more languages is important in order to offset the existing <a href="https://labs.theguardian.com/digital-language-divide/">language divide</a> and to ensure that speakers of non-English languages are not left behind, among <a href="https://ruder.io/nlp-beyond-english/">many other reasons</a>.</p>
<p>This post takes a closer look at how the AI community is faring in this endeavour. I will be focusing on topics related to natural language processing (NLP) and African languages as these are the domains I am most familiar with. I've tried to cover as many contributions as possible but undoubtedly missed relevant work. Feel free to leave a comment or reach out with a pointer to anything I missed.</p>
<p>This post is partially based on a <a href="https://drive.google.com/file/d/1T8aGnKxO7vRclhjfeDKEiARmsor5Cvsa/view?usp=sharing">keynote</a> I gave at the <a href="https://deeplearningindaba.com/2022/">Deep Learning Indaba 2022</a>. It covers the following high-level topics:</p>
<ul>
<li><a href="#statusquo">Status Quo</a></li>
<li><a href="#recentprogress">Recent Progress</a></li>
<li><a href="#challengesandopportunities">Challenges and Opportunities</a></li>
</ul>
<h2 id="statusquo">Status Quo</h2>
<p>There are around 7,000 languages spoken around the world. Around 400 languages have more than 1M speakers and around 1,200 languages have more than 100k <sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>. Bender <sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> highlighted the need for language independence in 2011. Reviewing papers published at ACL 2008, she found that 63% of all papers focused on English. For a recent study <sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>, we similarly reviewed papers from ACL 2021 and found that almost 70% of papers only evaluate on English. 10 years on, little thus seems to have changed.</p>
<p>Many languages in Africa, Asia, and the Americas that are spoken by tens of millions of people have received little research attention <sup class="footnote-ref"><a href="#fn1" id="fnref1:1">[1:1]</a></sup><sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>. Continents such as Africa with around 2,000 languages or individual countries such as Indonesia with around 700 languages are incredibly linguistically diverse and at the same time dramatically underserved by current research and technology.</p>
<p>Beyond individual languages, researchers with affiliations in countries where such languages are spoken are similarly under-represented in both ML and NLP communities. For instance, while we can observe a slight upward trend in the number of authors affiliated with African universities publishing at top machine learning (ML) and NLP venues, this increase pales compared to the <a href="https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report-_Chapter-1.pdf">thousands of authors</a> from other regions publishing in such venues every year.</p>
<figure>
      <img src="http://ruder.io/content/images/2022/10/representation_of_african_researchers.png" style="width: 100%" title="Representation of African NLP Researchers in top ML and NLP venues" alt="The State of Multilingual AI">
<figcaption>Representation of African NLP Researchers in top ML and NLP venues. *: does not consider African authors working abroad. Data is based on: <a href="https://github.com/marekrei/ml_nlp_paper_data">ml_nlp_paper_data</a> by Marek Rei. NLP venues: ACL, CL, COLING, CoNLL, EACL, EMNLP, NAACL, TACL; ML venues: AAAI, ICLR, ICML, NeurIPS.</figcaption>
</figure>
<p>Current state-of-the-art models in many ML domains are mainly based on two ingredients: 1) large, scalable architectures (often based on the Transformer <sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>) and 2) <a href="https://tinyurl.com/NAACLTransfer">transfer learning</a><sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>. Given the general nature of these models, they can be applied to various types of data including images <sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>, video <sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>, and audio <sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup>. Some of the most successful models in recent NLP are BERT <sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup>, RoBERTa <sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup>, BART <sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup>, T5 <sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup>, and DeBERTa <sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup>, which have been trained on billions of tokens of online text using variants of masked language modeling in English. In speech, wav2vec 2.0 <sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup> has been pre-trained on large amounts of unlabeled speech.</p>
<p><strong>Multilingual models</strong>   These models have multilingual analogues—in NLP, models such as <a href="https://github.com/google-research/bert/blob/master/multilingual.md">mBERT</a>, RemBERT <sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup>, XLM-RoBERTa <sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup>, mBART <sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup>, mT5 <sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup>, and mDeBERTa <sup class="footnote-ref"><a href="#fn14" id="fnref14:1">[14:1]</a></sup>—that were trained in a similar fashion, predicting randomly masked tokens on data of around 100 languages. Compared to their monolingual counterparts, these multilingual models require a much larger vocabulary to represent tokens in many languages.</p>
<p>A number of factors has been found to be important for learning robust multilingual representations, including shared tokens <sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup>, subword fertility <sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup>, and word embedding alignment <sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup>. In speech, models such as XSLR <sup class="footnote-ref"><a href="#fn23" id="fnref23">[23]</a></sup> and UniSpeech <sup class="footnote-ref"><a href="#fn24" id="fnref24">[24]</a></sup> are pre-trained on large amounts of unlabeled data in 53 and 60 languages respectively.</p>
<p><strong>The curse of multilinguality</strong>   Why do these models only cover up to 100 languages? One reason is the 'curse of multilinguality' <sup class="footnote-ref"><a href="#fn17" id="fnref17:1">[17:1]</a></sup>. Similar to models that are trained on many tasks, the more languages a model is pre-trained on, the less model capacity is available to learn representations for each language. Increasing the size of a model ameliorates this to some extent, enabling the model to dedicate more capacity to each language <sup class="footnote-ref"><a href="#fn25" id="fnref25">[25]</a></sup>.</p>
<p><strong>Lack of pre-training data</strong>   Another limiting factor is the availability of text data on the web, which is skewed towards languages spoken in Western countries and with a large online footprint. The languages with the most online resources available for pre-training are typically prioritized, leading to an under-representation of languages with few resources due to this top-heavy selection. This is concerning as prior studies have shown that the amount of pre-training data in a language correlates with downstream performance for some tasks <sup class="footnote-ref"><a href="#fn26" id="fnref26">[26]</a></sup><sup class="footnote-ref"><a href="#fn27" id="fnref27">[27]</a></sup><sup class="footnote-ref"><a href="#fn28" id="fnref28">[28]</a></sup>. In particular, languages and scripts that were never seen during pre-training often lead to poor performance <sup class="footnote-ref"><a href="#fn29" id="fnref29">[29]</a></sup><sup class="footnote-ref"><a href="#fn30" id="fnref30">[30]</a></sup>.</p>
<figure>
      <img src="http://ruder.io/content/images/2022/10/language_coverage_wikipedia_commoncrawl.png" style="width: 100%" title="Representation of African NLP Researchers in top ML and NLP venues" alt="The State of Multilingual AI">
<figcaption>Amount of data in GiB (log-scale) for the 88 languages that appear in both Wikipedia and CommonCrawl (<a href="https://aclanthology.org/2020.acl-main.747/">Conneau et al., 2020</a>).</figcaption>
</figure>
<p><strong>Quality issues in existing multilingual resources</strong>   Even for languages where data is available, past work has shown that some commonly used multilingual resources have severe quality issues. Entity names in Wikidata are not in the native script for many under-represented languages while entity spans in WikiAnn <sup class="footnote-ref"><a href="#fn31" id="fnref31">[31]</a></sup>, a weakly supervised multilingual named entity recognition dataset based on Wikipedia, are often erroneous <sup class="footnote-ref"><a href="#fn32" id="fnref32">[32]</a></sup>.</p>
<p>Similarly, several automatically mined resources and automatically aligned corpora used for machine translation are problematic <sup class="footnote-ref"><a href="#fn33" id="fnref33">[33]</a></sup>. For instance, 44/65 audited languages in WikiMatrix <sup class="footnote-ref"><a href="#fn34" id="fnref34">[34]</a></sup> and 19/20 audited langages in CCAligned <sup class="footnote-ref"><a href="#fn35" id="fnref35">[35]</a></sup> contain less than 50% correct sentences. Overall, however, performance seems to be mostly constrained by the quantity rather than quality of data in under-represented languages <sup class="footnote-ref"><a href="#fn36" id="fnref36">[36]</a></sup>.</p>
<p><strong>Multilingual evaluation results</strong>   We can get a better picture of the state of the art by looking at the performance of recent models on a representative multilingual benchmark such as XTREME <sup class="footnote-ref"><a href="#fn26" id="fnref26:1">[26:1]</a></sup>—a multilingual counterpart to GLUE <sup class="footnote-ref"><a href="#fn37" id="fnref37">[37]</a></sup> and SuperGLUE <sup class="footnote-ref"><a href="#fn38" id="fnref38">[38]</a></sup>—which aggregates performance across 9 tasks and 40 languages. Starting with the first multilingual pre-trained models two and a half years ago, performance has improved steadily and is slowly approaching human-level performance on the benchmark.</p>
<figure>
      <img src="http://ruder.io/content/images/2022/11/progress_xtreme_nov.png" style="width: 100%" title="Representation of African NLP Researchers in top ML and NLP venues" alt="The State of Multilingual AI">
<figcaption>Performance of models on the XTREME leaderboard on 9 tasks and 40 languages.</figcaption>
</figure>
<p>However, looking at the average performance on such a benchmark obscures which languages a model was actually evaluated on. Beyond a few datasets with a large language coverage—Universal Dependencies <sup class="footnote-ref"><a href="#fn39" id="fnref39">[39]</a></sup>, WikiAnn <sup class="footnote-ref"><a href="#fn31" id="fnref31:1">[31:1]</a></sup>, and Tatoeba <sup class="footnote-ref"><a href="#fn40" id="fnref40">[40]</a></sup>—other tasks only cover few languages, and are again skewed towards languages with more resources. Most current benchmarks thus only provide a distorted view of the overall progress towards multilingual AI for the world's languages.</p>
<p>For a more accurate impression, we can look at the normalized state-of-the-art performance on different language technology tasks averaged across the world's languages either based on their speaker population (demographic utility) or equally (linguistic utility) <sup class="footnote-ref"><a href="#fn41" id="fnref41">[41]</a></sup>.</p>
<div>
<center>
<figure>
      <img src="http://ruder.io/content/images/2022/11/linguistic_demographic_utility.png" style="width: 100%" title="Linguistic and demographic utility" alt="The State of Multilingual AI">
<figcaption>Linguistic and demographic global utility metrics for a number of language technology tasks. The red curve corresponds to the sequence where first the language with the largest number of users is set to utility 1, then the second, and so on <a href="https://aclanthology.org/2022.acl-long.376/">(Blasi et al., 2022)</a>.</figcaption>
</figure>
</center>
</div>
<p>Most NLP tasks fare better when we average based on speaker population. Overall, however, we observe very low linguistic utility numbers, showing an unequal performance distribution across the world's languages. This conclusion, however, may be somewhat overly pessimistic as it only considers languages for which evaluation data is currently available. Cross-lingual performance prediction <sup class="footnote-ref"><a href="#fn42" id="fnref42">[42]</a></sup> could be used to estimate performance for a broader set of languages.</p>
<p><strong>Multilingual vs English-centric models</strong>   Let us now take a step back and look at recent large language models in NLP in general. We can plot recent models based on the fraction of non-English data they are pre-trained on. Based on this characterization, we can observe two distinct streams of research: 1) Multilingual models that are trained on multingual data in many languages and 2) English-centric models that are trained on mostly English data.</p>
<figure>
      <img src="http://ruder.io/content/images/2022/11/language_models_over_time_opt_bloom.png" style="width: 100%" title="Representation of African NLP Researchers in top ML and NLP venues" alt="The State of Multilingual AI">
<figcaption>The largest recent models are not becoming significantly more multilingual. Figure adapted from Noah Constant.</figcaption>
</figure>
<p>The latter form the foundation for the mainstream of NLP research and while these models have been getting larger, they have not been getting much more multilingual. An exception is BLOOM <sup class="footnote-ref"><a href="#fn43" id="fnref43">[43]</a></sup>, the largest multilingual open-source model to date. Some of these large models have demonstrated surprising multilingual capabilities. For instance, GPT-3 <sup class="footnote-ref"><a href="#fn44" id="fnref44">[44]</a></sup> and PaLM <sup class="footnote-ref"><a href="#fn45" id="fnref45">[45]</a></sup> can translate text between languages with large amounts of data. While they have been shown to be able to perform multilingual few-shot learning <sup class="footnote-ref"><a href="#fn46" id="fnref46">[46]</a></sup><sup class="footnote-ref"><a href="#fn47" id="fnref47">[47]</a></sup><sup class="footnote-ref"><a href="#fn48" id="fnref48">[48]</a></sup>, models perform best when prompts or input data are translated to English. They also perform poorly when translating between non-English language pairs or into languages with limited data. While PaLM is able to summarize non-English text into English, it struggles when generating text in other languages.</p>
<p>Similarly, recent speech models such as HuBERT <sup class="footnote-ref"><a href="#fn49" id="fnref49">[49]</a></sup> and WavLM <sup class="footnote-ref"><a href="#fn50" id="fnref50">[50]</a></sup> and recent large vision models that generate text based on an image such as Flamingo <sup class="footnote-ref"><a href="#fn51" id="fnref51">[51]</a></sup> or an image based on text such as DALL-E 2 <sup class="footnote-ref"><a href="#fn52" id="fnref52">[52]</a></sup>, Imagen <sup class="footnote-ref"><a href="#fn53" id="fnref53">[53]</a></sup>, and Parti <sup class="footnote-ref"><a href="#fn54" id="fnref54">[54]</a></sup> are English-centric. Exceptions are Whisper <sup class="footnote-ref"><a href="#fn55" id="fnref55">[55]</a></sup> and PaLI <sup class="footnote-ref"><a href="#fn56" id="fnref56">[56]</a></sup>, which are pre-trained on large amounts of weakly supervised data from the web for ASR and image captioning in 96 and 109 languages respectively. However, overall, for the latest generation of large models, multilinguality remains a side-effect rather than a key design criterion.</p>
<p><strong>User-facing technologies</strong>   With regard to user-facing technologies, keyboards and spell checkers such as <a href="https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin&amp;hl=en&amp;gl=US">Gboard</a> support more than 900+ languages but many languages still lack support or speakers may be unaware that a keyboard for their language is available <sup class="footnote-ref"><a href="#fn57" id="fnref57">[57]</a></sup>. Other user-facing technologies with broad language coverage are machine translation and automatic speech recognition (ASR). <a href="https://translate.google.com/">Google Translate</a> and <a href="https://cloud.google.com/speech-to-text">speech-to-text</a>, for instance, support 133 and more than 125 languages respectively as of the publishing of this post.</p>
<h2 id="recentprogress">Recent Progress</h2>
<p>Recent progress in this area can be categorized into two categories: 1) new groups, communities, support structures, and initiatives that have enabled broader work; and 2) high-level research contributions such as new datasets and models that allow others to build on them.</p>
<p><strong>Research communities</strong>   There are many languages with active existing research communities dedicated to them. These include languages with large speaker populations such as Japanese, Mandarin, Turkish, and Hindi as well as languages with fewer speakers such as Gaelic or Basque <sup class="footnote-ref"><a href="#fn1" id="fnref1:2">[1:2]</a></sup>. There have also been concerted efforts in the past to collect data for specific under-represented languages such as Inuktitut <sup class="footnote-ref"><a href="#fn58" id="fnref58">[58]</a></sup><sup class="footnote-ref"><a href="#fn59" id="fnref59">[59]</a></sup>.</p>
<p>In the last few years, various new communities have emerged specializing in under-represented languages or language families. These include groups focusing on linguistic regions such as <a href="https://www.masakhane.io/">Masakhane</a> for African languages, <a href="http://turing.iimas.unam.mx/americasnlp/">AmericasNLP</a> for native American languages, <a href="https://indonlp.github.io/">IndoNLP</a> for Indonesian languages, <a href="https://ghananlp.org/">GhanaNLP</a> and <a href="https://www.hausanlp.org/">HausaNLP</a>, among others. Events such as the <a href="https://deeplearningindaba.com/2022/">Deep Learning Indaba</a>, <a href="https://deeplearningindaba.com/2022/indabax/">IndabaX</a>, <a href="https://khipu.ai/">Khipu</a>, <a href="https://www.eeml.eu/home">EEML</a>, <a href="https://www.seamls.ai/">SEAMLS</a>, and <a href="https://lig-alps.imag.fr/">ALPS</a>, among many others and workshops such as <a href="https://africanlp.masakhane.io/">AfricaNLP</a> have enabled these communities to come together and complement longer-running events such as the <a href="https://sites.google.com/view/wanlp2022/">Arabic NLP</a>, <a href="https://computel-workshop.org/">ComputEL</a>, and <a href="https://sigtyp.github.io/">SIGTYP workshops</a>.</p>
<div>
<center>
<figure>
      <img src="http://ruder.io/content/images/2022/11/deep_learning_indaba_tunisia.jpeg" style="width: 80%" title="Deep Learning Indaba" alt="The State of Multilingual AI">
<figcaption>The Deep Learning Indaba 2022 in Tunesia.</figcaption>
</figure>
</center>
</div>
<p>At the same time, there are communities with broader focus areas such as <a href="https://mlcollective.org/">ML Collective</a> that have contributed to this space. One of the largest community-driven efforts in multilingual AI is <a href="https://bigscience.huggingface.co/">BigScience</a>, which has released BLOOM <sup class="footnote-ref"><a href="#fn43" id="fnref43:1">[43:1]</a></sup>. In many cases, projects in these communities have been participatory and highly collaborative <sup class="footnote-ref"><a href="#fn60" id="fnref60">[60]</a></sup><sup class="footnote-ref"><a href="#fn61" id="fnref61">[61]</a></sup>, lowering the barrier to doing research and involving members of the community at every stage of the process.</p>
<p>Other communities such as <a href="https://zindi.africa/">Zindi</a> or <a href="https://www.datasciencenigeria.org/">Data Science Nigeria</a> have focused on hosting competitions and providing training courses while new programs such as the <a href="https://aimsammi.org/">African Master's in Machine Intelligence</a> seek to educate the next generation of AI researchers.</p>
<p><strong>Initiatives</strong>   The Association for Computational Linguistics (ACL) has emphasized the importance of language diversity, with a <a href="https://www.2022.aclweb.org/post/acl-2022-theme-track-language-diversity-from-low-resource-to-endangered-languages">special theme track at the main ACL 2022 conference</a> on this topic. The ACL has also launched the <a href="https://www.2022.aclweb.org/dispecialinitiative">60-60 initiative</a>, which aims to make scientific content more accessible by creating a) translations of the entire <a href="https://aclanthology.org/">ACL anthology</a> into 60 languages; b) cross-lingual subtitling and dubbing for all plenary talks in 10 languages; and c) a comprehensive standardized scientific and NLP terminology list in 60 languages. The latter resource and <a href="https://www.masakhane.io/ongoing-projects/masakhane-mt-decolonise-science">glossaries for African languages</a> could help to facilitate the discussion of language technology in local languages.</p>
<p><strong>Datasets</strong>   On the research side, there has been a flurry of new datasets covering a host of applications, from unlabeled speech and text corpora <sup class="footnote-ref"><a href="#fn62" id="fnref62">[62]</a></sup>, to language identification <sup class="footnote-ref"><a href="#fn63" id="fnref63">[63]</a></sup>, text classification <sup class="footnote-ref"><a href="#fn64" id="fnref64">[64]</a></sup>, sentiment analysis <sup class="footnote-ref"><a href="#fn65" id="fnref65">[65]</a></sup>, <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YHXJSU">ASR</a>, named entity recognition <sup class="footnote-ref"><a href="#fn61" id="fnref61:1">[61:1]</a></sup>, question answering <sup class="footnote-ref"><a href="#fn66" id="fnref66">[66]</a></sup>, and summarization <sup class="footnote-ref"><a href="#fn67" id="fnref67">[67]</a></sup> in a range of under-represented languages. New benchmarks seek to assess models on a broad set of tasks in Romanian <sup class="footnote-ref"><a href="#fn68" id="fnref68">[68]</a></sup>, Korean <sup class="footnote-ref"><a href="#fn69" id="fnref69">[69]</a></sup>, and Turkish <sup class="footnote-ref"><a href="#fn70" id="fnref70">[70]</a></sup>, in geographically related languages such as Indonesian <sup class="footnote-ref"><a href="#fn71" id="fnref71">[71]</a></sup><sup class="footnote-ref"><a href="#fn72" id="fnref72">[72]</a></sup> or Indian languages <sup class="footnote-ref"><a href="#fn73" id="fnref73">[73]</a></sup>, and in different modalities such as speech <sup class="footnote-ref"><a href="#fn74" id="fnref74">[74]</a></sup> and image-grounded text <sup class="footnote-ref"><a href="#fn75" id="fnref75">[75]</a></sup>. The development of these datasets has been enabled by new funding structures and initiatives such as the <a href="https://lacunafund.org/">Lacuna Fund</a> and <a href="https://www.bmz-digital.global/en/overview-of-initiatives/fair-forward/">FAIR Forward</a> that have incentivized work in this area.</p>
<div>
<center>
<figure>
      <img src="http://ruder.io/content/images/2022/11/masakhaner_data.png" style="width: 100%" title="MasakhaNER data" alt="The State of Multilingual AI">
<figcaption>Named entity annotations in African languages in MasakhaNER. PER, LOC, and DATE entities are in purple, orange, and green respectively <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00416/107614/MasakhaNER-Named-Entity-Recognition-for-African">(Adelani et al., 2021)</a></figcaption>
</figure>
</center>
</div>
<p>Other existing corpora have grown in their language coverage with community involvement: The <a href="https://commonvoice.mozilla.org/en/datasets">Common Voice speech corpus</a> <sup class="footnote-ref"><a href="#fn76" id="fnref76">[76]</a></sup> now covers 100 languages while the latest release of <a href="https://universaldependencies.org/">Universal Dependencies</a> <sup class="footnote-ref"><a href="#fn39" id="fnref39:1">[39:1]</a></sup> includes 130 languages. Given the number of new datasets, there have been efforts to catalogue available datasets in <a href="https://lanfrica.com/">African</a> and <a href="https://indonlp.github.io/nusa-catalogue/">Indonesian languages</a>, <a href="https://arbml.github.io/masader/">Arabic</a>, and a diverse set of languages <sup class="footnote-ref"><a href="#fn77" id="fnref77">[77]</a></sup>.</p>
<p><strong>Models</strong>   New models developed in this area focus specifically on under-represented languages. There are text-based language models that focus on African languages such as AfriBERTa <sup class="footnote-ref"><a href="#fn78" id="fnref78">[78]</a></sup>, AfroXLM-R <sup class="footnote-ref"><a href="#fn79" id="fnref79">[79]</a></sup>, and KinyaBERT <sup class="footnote-ref"><a href="#fn80" id="fnref80">[80]</a></sup> and models for Indonesian languages such as IndoBERT <sup class="footnote-ref"><a href="#fn71" id="fnref71:1">[71:1]</a></sup> and IndoGPT <sup class="footnote-ref"><a href="#fn72" id="fnref72:1">[72:1]</a></sup>. For Indian languages, there are text-based models such as IndicBERT <sup class="footnote-ref"><a href="#fn73" id="fnref73:1">[73:1]</a></sup> and MuRIL <sup class="footnote-ref"><a href="#fn81" id="fnref81">[81]</a></sup> and speech models such as CLSRIL <sup class="footnote-ref"><a href="#fn82" id="fnref82">[82]</a></sup> and IndicWav2Vec <sup class="footnote-ref"><a href="#fn83" id="fnref83">[83]</a></sup>. Many of these approaches train a model on several related languages and are thus able to leverage positive transfer and to be much more efficient than larger multilingual models. See <sup class="footnote-ref"><a href="#fn84" id="fnref84">[84]</a></sup> and <sup class="footnote-ref"><a href="#fn85" id="fnref85">[85]</a></sup> for recent surveys of recent multilingual models in NLP and speech.</p>
<p><strong>Industry</strong>   In industry, startups have been developing new technology to serve local languages such as <a href="https://www.instadeep.com/">InstaDeep</a> developing a model for Tunisian Arabic <sup class="footnote-ref"><a href="#fn86" id="fnref86">[86]</a></sup>, <a href="https://nokwary.com/">Nokwary</a> enabling financial inclusion in Ghanaian languages, <a href="https://barefootlaw.org/">BarefootLaw</a> employing NLP technology to provide legal help in Uganda, and <a href="https://www.neuralspace.ai/">NeuralSpace</a> building speech and text APIs for a geographically diverse set of languages, among many others.</p>
<p>Similarly, large tech companies have expanded their ASR and <a href="https://blog.google/products/translate/24-new-languages/">machine translation offerings</a>. Both Google <sup class="footnote-ref"><a href="#fn87" id="fnref87">[87]</a></sup> and Meta <sup class="footnote-ref"><a href="#fn88" id="fnref88">[88]</a></sup> have described efforts on how to scale machine translation technology to the next thousand languages. At the heart of these efforts are a) mining high-quality monolingual data from the web based on improved language identification and filtering; b) training massively multilingual models on monolingual and parallel data; and c) extensive evaluation on newly collected datasets. These components are similarly important for building better ASR systems for under-represented languages <sup class="footnote-ref"><a href="#fn89" id="fnref89">[89]</a></sup>.</p>
<h2 id="challengesandopportunities">Challenges and Opportunities</h2>
<p>Given this recent progress, what are the remaining challenges and opportunities in this area?</p>
<h3 id="challenge1limiteddata">Challenge #1: Limited Data</h3>
<p>Arguably the biggest challenge in multilingual research is the limited amount of data available for most of the world's languages. Joshi et al. <sup class="footnote-ref"><a href="#fn90" id="fnref90">[90]</a></sup> categorized the languages of the world into six different categories based on the amount of labeled and unlabeled data available in them.</p>
<div>
<center>
<figure>
      <img src="http://ruder.io/content/images/2022/11/language_resource_distribution_joshi.png" style="width: 80%" title="Language resource distribution" alt="The State of Multilingual AI">
<figcaption>The distribution of resources in the world's languages. Labeled data is based on the <a href="https://catalog.ldc.upenn.edu/">LDC Catalog</a> and <a href="http://catalog.elra.info/en-us/">ELRA Map</a>. Unlabeled data is based on Wikipedia. The size of the gradient circle represents the number of languages in the class. The color spectrum represents the total speaker population size from low to high (<a href="https://aclanthology.org/2020.acl-main.560/">Joshi et al., 2020</a>).</figcaption>
</figure>
</center>
</div>
<p>88% of the world's languages are in resource group 0 with virtually no text data available to them while 5% of languages are in resource group 1 where there is very limited text data available.</p>
<h3 id="opportunity1realworlddata">Opportunity #1: Real-world Data</h3>
<p>How can we overcome this enormous discrepancy in the resource distribution across the world's languages? The creation of new data, particularly in languages with few annotators, is expensive. For this reason, many existing multilingual datasets such as XNLI <sup class="footnote-ref"><a href="#fn91" id="fnref91">[91]</a></sup>, XQuAD <sup class="footnote-ref"><a href="#fn92" id="fnref92">[92]</a></sup>, and XCOPA <sup class="footnote-ref"><a href="#fn93" id="fnref93">[93]</a></sup> are based on translations of established English datasets.</p>
<p>Such translation-based data, however, are problematic. Translated text in a language can be considered a dialect of that language, known as 'translationese', which differs from natural language <sup class="footnote-ref"><a href="#fn94" id="fnref94">[94]</a></sup>. Translation-based test sets may thus over-estimate the performance of models trained on similar data, which have learned to exploit translation artifacts <sup class="footnote-ref"><a href="#fn95" id="fnref95">[95]</a></sup>.</p>
<p><strong>Over-representation of Western concepts</strong>   Beyond these issues, translating an existing dataset inherits the biases of the original data. In particular, translated data differs from data that is naturally created by speakers of different languages. As existing datasets were mostly created by crowdworkers or researchers based in Western countries, they mostly reflect Western-centric concepts. For example, ImageNet <sup class="footnote-ref"><a href="#fn96" id="fnref96">[96]</a></sup>, one of the most influential datasets in ML, is based on English WordNet. As a result, it captures concepts that are overly English-specific and unknown in other cultures <sup class="footnote-ref"><a href="#fn97" id="fnref97">[97]</a></sup>. Similarly, Flickr30k <sup class="footnote-ref"><a href="#fn98" id="fnref98">[98]</a></sup> contains depictions of concepts that are mainly familiar to people from certain Western regions such as <a href="https://en.wikipedia.org/wiki/Tailgate_party">tailgating</a> in the US <sup class="footnote-ref"><a href="#fn99" id="fnref99">[99]</a></sup>.</p>
<div>
<center>
<figure>
      <img src="http://ruder.io/content/images/2022/11/tailgating_multi30k.png" style="width: 60%" title="Tailgating in Flickr30k" alt="The State of Multilingual AI">
    <figcaption>An image in Flickr30k <a href="https://aclanthology.org/Q14-1006/">(Young et al., 2014)</a>. Two American annotators but neither Dutch nor German workers identified the Denver Broncos jersey. Three out of five
American annotators described the activity in the image as <a href="https://en.wikipedia.org/wiki/Tailgate_party">tailgating</a>, a North-American pastime where people gather to enjoy an informal (often barbecue) meal on the parking lot outside a sports stadium <a href="https://aclanthology.org/W17-3503/">(van Miltenburg et al., 2017)</a>.</figcaption>
</figure>
</center>
</div>
<p>The commonsense reasoning dataset COPA <sup class="footnote-ref"><a href="#fn100" id="fnref100">[100]</a></sup> contains many referents that have no language-specific terms in some languages, e.g., bowling ball, hamburger, and lottery <sup class="footnote-ref"><a href="#fn93" id="fnref93:1">[93:1]</a></sup>. Most questions in current QA datasets ask about US or UK nationals <sup class="footnote-ref"><a href="#fn101" id="fnref101">[101]</a></sup> while many other datasets, particularly those based on Wikipedia, contain mainly entities from Europe, the US, and the Middle East <sup class="footnote-ref"><a href="#fn102" id="fnref102">[102]</a></sup>.</p>
<p><strong>Practical data</strong>   For new datasets, it is thus ever more important to create data that is informed by real-world usage. On the one hand, data should reflect the background of the speakers speaking the language. For example, MaRVL <sup class="footnote-ref"><a href="#fn103" id="fnref103">[103]</a></sup> is a multi-modal reasoning dataset that covers concepts representative of different cultures and languages.</p>
<div>
<center>
<figure>
      <img src="http://ruder.io/content/images/2022/11/marvl_swahili.png" style="width: 100%" title="MaRVL Swahili example" alt="The State of Multilingual AI">
    <figcaption>A Swahili example in MaRVL depicting the concept <i>leso</i> ("handkerchief"). Caption: <i>Picha moja ina watu kadhaa waliovaa leso na picha nyingine ina leso bila watu.</i> ("One picture contains several people wearing handkerchiefs and another picture has a handkerchief without people."). Label: FALSE <a href="https://aclanthology.org/2021.emnlp-main.818">(Liu et al., 2021)</a>.</figcaption>
</figure>
</center>
</div>
<p>Given the increasing maturity of language technology, it is important to collect data that is relevant for real-world applications and that may have a positive impact on speakers of under-represented languages. Such applications include the development of assistive language technology for humanitarian crises, health, education, legal, and finance. Languages that may benefit from such technology are standardised languages and contact languages, including creoles and regional language varieties <sup class="footnote-ref"><a href="#fn104" id="fnref104">[104]</a></sup>.</p>
<p>Creating real-world datasets has the potential to ground research and enables it to have a larger impact. It also reduces the distribution shift between research and practical scenarios and makes it more likely that models developed on academic datasets will be useful in production.</p>
<p>Beyond the creation of the training or evaluation data, the development of a language model requires the involvement of a large number of stakeholders, many of whom are often not explicitly acknowledged. Many of the components in this process are under-performing and often not available in many languages.</p>
<figure>
      <img src="http://ruder.io/content/images/2022/11/language_model_development_cycle.png" style="width: 100%" title="Language model development cycle" alt="The State of Multilingual AI">
<figcaption>The development cycle of a language model. Model creation relies on data created by multiple stakeholders. (Credit: Clara Rivera; adapted from <a href="https://aclanthology.org/2020.findings-emnlp.195/">∀ et al., 2020</a>).</figcaption>
</figure>
<p>This starts at the beginning of data creation where online platforms and keyboards may not support certain languages <sup class="footnote-ref"><a href="#fn57" id="fnref57:1">[57:1]</a></sup>, dictionaries do not cover certain languages and language ID does not perform well in those languages <sup class="footnote-ref"><a href="#fn105" id="fnref105">[105]</a></sup>. In many languages, the connections between different stakeholders are also missing and it is difficult to find original content or to identify qualified annotators. The fact that text on the web is difficult to find for some languages does not mean, however, that these languages are resource-poor or that data for these languages does not exist.</p>
<p><strong>Multi-modal data</strong>   Many languages around the world are more commonly spoken than written. We can overcome the reliance (and lack of) text data by focusing on information from multi-modal data sources such as radio broadcasts and online videos as well as combining information from multiple modalities. Recent speech-and-text models <sup class="footnote-ref"><a href="#fn106" id="fnref106">[106]</a></sup><sup class="footnote-ref"><a href="#fn107" id="fnref107">[107]</a></sup> achieve strong improvements on speech tasks such as ASR, speech translation, and text-to-speech. They still perform more poorly, however, on text-only tasks due to a lack of capacity <sup class="footnote-ref"><a href="#fn108" id="fnref108">[108]</a></sup>. There is a lot of potential to leverage multi-modal data as well as to investigate the linguistic characteristics of different languages and their interplay in text and speech <sup class="footnote-ref"><a href="#fn109" id="fnref109">[109]</a></sup>.</p>
<div>
<center>
<figure>
      <img src="http://ruder.io/content/images/2022/11/mslam.png" style="width: 100%" title="Multi-modal pre-training in mSLAM" alt="The State of Multilingual AI">
    <figcaption>Multilingual speech-text pre-training in mSLAM. A model is jointly pre-trained on unlabeled and labeled text and speech datasets using a set of different modality-specific losses <a href="https://arxiv.org/abs/2202.01374">(Bapna et al., 2022)</a>.</figcaption>
</figure>
</center>
</div>
<p>Beyond multi-modal information, data may also be available in formats that are locked to current models such as in handwritten documents and non-digitized books, among others. Technologies such as optical character recognition (OCR) <sup class="footnote-ref"><a href="#fn110" id="fnref110">[110]</a></sup> and new datasets such as the Bloom Library <sup class="footnote-ref"><a href="#fn111" id="fnref111">[111]</a></sup> will help us make such untapped data sources more accessible. There are also resources that have so far been used relatively little despite their large language coverage such as the Bible, which covers around 1,600 languages <sup class="footnote-ref"><a href="#fn112" id="fnref112">[112]</a></sup> and lexicons, which cover around 5,700 languages <sup class="footnote-ref"><a href="#fn113" id="fnref113">[113]</a></sup>. Other data sources may be readily available but have so far gone unused or unnoticed. Recent examples of such 'fortuitous data' <sup class="footnote-ref"><a href="#fn114" id="fnref114">[114]</a></sup> include HTML and web page structure <sup class="footnote-ref"><a href="#fn115" id="fnref115">[115]</a></sup><sup class="footnote-ref"><a href="#fn116" id="fnref116">[116]</a></sup>, among others.</p>
<p>Given the generalization ability of pre-trained language models, benchmarks have been increasingly moving towards evaluation in low-resource settings. When creating new datasets, large test sets with sufficient statistical power <sup class="footnote-ref"><a href="#fn117" id="fnref117">[117]</a></sup> should thus be prioritized. In addition, languages for annotation can be prioritized based on the expected gain in utility <sup class="footnote-ref"><a href="#fn41" id="fnref41:1">[41:1]</a></sup> and reduction in inequality <sup class="footnote-ref"><a href="#fn118" id="fnref118">[118]</a></sup>.</p>
<p>Finally, there are challenges for responsible AI when collecting data and developing technology for under-represented languages, including data governance, safety, privacy, and participation. Addressing these challenges requires answering questions such as: How are appropriate usage and ownership of the data and technology guaranteed <sup class="footnote-ref"><a href="#fn119" id="fnref119">[119]</a></sup>? Are there methods in place to detect and filter sensitive and biased data and detect bias in models? How is privacy preserved during data collection and usage? How can the data and technology development be made participatory <sup class="footnote-ref"><a href="#fn120" id="fnref120">[120]</a></sup>?</p>
<h3 id="challenge2limitedcompute">Challenge #2: Limited Compute</h3>
<p>Under-represented language applications face constraints that go beyond the lack of data. Mobile data, compute, and other computational resources may often be expensive or unavailable. GPU servers, for instance, are scarce even in top universities in many countries <sup class="footnote-ref"><a href="#fn4" id="fnref4:1">[4:1]</a></sup> while the cost of mobile data is higher in countries where under-represented languages are spoken <sup class="footnote-ref"><a href="#fn121" id="fnref121">[121]</a></sup>.</p>
<div>
<center>
<figure>
      <img src="http://ruder.io/content/images/2022/11/cost_mobile_data.png" style="width: 60%" title="Cost of mobile data" alt="The State of Multilingual AI">
<figcaption>Cost of mobile data by country for the resource groups by <a href="https://aclanthology.org/2020.acl-main.560/">Joshi et al. (2020)</a> <a href="https://aclanthology.org/2021.findings-emnlp.282/">(Ahia et al., 2021)</a>.</figcaption>
</figure>
</center>
</div>
<h3 id="opportunity2efficiency">Opportunity #2: Efficiency</h3>
<p>In order to make better use of limited compute, we must develop methods that are more efficient. For an overview of efficient Transformer architectures and efficient NLP methods in general refer to <sup class="footnote-ref"><a href="#fn122" id="fnref122">[122]</a></sup> and <sup class="footnote-ref"><a href="#fn123" id="fnref123">[123]</a></sup>. As pre-trained models are widely available, a promising direction is the adaptation of such models via parameter-efficient methods, which have been shown to be more effective than in-context learning <sup class="footnote-ref"><a href="#fn124" id="fnref124">[124]</a></sup>.</p>
<p>A common method are adapters <sup class="footnote-ref"><a href="#fn125" id="fnref125">[125]</a></sup><sup class="footnote-ref"><a href="#fn126" id="fnref126">[126]</a></sup>, small bottleneck layers that are inserted between a pre-trained model's weights. These parameter-efficient methods can be used to overcome the curse of multilinguality by enabling the allocation of additional language-specific capacity. They also enable the adaptation of a pre-trained multilingual model to languages that it has not been exposed to during pre-training <sup class="footnote-ref"><a href="#fn127" id="fnref127">[127]</a></sup><sup class="footnote-ref"><a href="#fn128" id="fnref128">[128]</a></sup>. As such adapter layers are separate from the remaining parameters of the model, they allow learning modular interactions between tasks and languages <sup class="footnote-ref"><a href="#fn129" id="fnref129">[129]</a></sup>.</p>
<div>
<center>
<figure>
      <img src="http://ruder.io/content/images/2022/11/language_adapter_layers.png" style="width: 80%" title="Multi-modal pre-training in mSLAM" alt="The State of Multilingual AI">
    <figcaption>Language-specific adapter layers learned via masked language modeling (MLM) on data of each language while the remaining parameters of the model are frozen <a href="https://aclanthology.org/2020.emnlp-main.617/">(Pfeiffer et al., 2020)</a>.</figcaption>
</figure>
</center>
</div>
<p>Adapters have been shown to improve robustness <sup class="footnote-ref"><a href="#fn130" id="fnref130">[130]</a></sup><sup class="footnote-ref"><a href="#fn131" id="fnref131">[131]</a></sup>, lead to increased sample efficiency compared to fine-tuning <sup class="footnote-ref"><a href="#fn132" id="fnref132">[132]</a></sup>, and outperform alternative parameter-efficient methods <sup class="footnote-ref"><a href="#fn133" id="fnref133">[133]</a></sup><sup class="footnote-ref"><a href="#fn134" id="fnref134">[134]</a></sup>. They allow for extensions such as incorporating hierarchical structure <sup class="footnote-ref"><a href="#fn135" id="fnref135">[135]</a></sup> or conditioning via hyper-networks <sup class="footnote-ref"><a href="#fn136" id="fnref136">[136]</a></sup><sup class="footnote-ref"><a href="#fn137" id="fnref137">[137]</a></sup>.</p>
<p>Cross-lingual parameter-efficient transfer learning is not restricted to adapters but can take other forms <sup class="footnote-ref"><a href="#fn138" id="fnref138">[138]</a></sup> such as sparse sub-networks <sup class="footnote-ref"><a href="#fn139" id="fnref139">[139]</a></sup>. Such methods have been applied to a diverse set of applications and domains, from machine translation <sup class="footnote-ref"><a href="#fn140" id="fnref140">[140]</a></sup><sup class="footnote-ref"><a href="#fn141" id="fnref141">[141]</a></sup> to ASR <sup class="footnote-ref"><a href="#fn142" id="fnref142">[142]</a></sup> and speech translation <sup class="footnote-ref"><a href="#fn143" id="fnref143">[143]</a></sup>.</p>
<h3 id="challenge3languagetypology">Challenge #3: Language Typology</h3>
<p>If we plot the typological features of the world's languages based on the <a href="https://wals.info/">World Atlas of Language Structures (WALS)</a> and project them into two dimensions using PCA, we get a density plot such as the one below. Marking the languages that are present in Universal Dependencies <sup class="footnote-ref"><a href="#fn39" id="fnref39:2">[39:2]</a></sup>, one of the most multilingual resources with red stars, we can observe that the languages for which data is available lie mostly in low-density regions of this plot. The distribution of languages in existing datasets is thus heavily skewed compared to the real-world distribution of languages and languages with available data are unrepresentative of most of the world's languages.</p>
<div>
<center>
<figure>
      <img src="http://ruder.io/content/images/2022/11/wals_density.png" style="width: 80%" title="Density of WALS typological features for the world's languages" alt="The State of Multilingual AI">
<figcaption>Density of WALS typological features of the world's languages. Red stars are languages in Universal Dependencies <a href="https://aclanthology.org/2021.findings-acl.106/">(Ponti et al. (2021)</a>.</figcaption>
</figure>
</center>
</div>
<p>Under-represented languages have many linguistic features that are not present in Western languages. A common linguistic feature is tone, which is present in around 80% of African languages <sup class="footnote-ref"><a href="#fn109" id="fnref109:1">[109:1]</a></sup> and can be lexical or gramatical. In Yorùbá, lexical tone distinguishes meaning, for instance, in the following words: <em>igbá</em> (&quot;calabash&quot;, &quot;basket&quot;), <em>igba</em> (&quot;200&quot;), <em>ìgbà</em> (&quot;time&quot;), <em>ìgbá</em> (&quot;garden egg&quot;), and <em>igbà</em> (&quot;rope&quot;). In Akan, grammatical tone distinguishes habitual and stative verbs such as for <em>Ama dá ha</em> (&quot;Ama sleeps here&quot;) and <em>Ama dà ha</em> (&quot;Ama is sleeping here&quot;). Tone is relatively unexplored in speech and NLP applications.</p>
<p>While the typological features of languages around the world are diverse, languages within a region often share linguistic features. For instance, African languages mainly belong to a few major language families.</p>
<div>
<center>
<figure>
      <img src="http://ruder.io/content/images/2022/11/map_african_languages.png" style="width: 60%" title="Map of African language families" alt="The State of Multilingual AI">
<figcaption>Map of African language families (Credit: <a href="https://en.wikipedia.org/wiki/Languages_of_Africa#/media/File:Map_of_African_language_families.svg">Wikipedia</a>).</figcaption>
</figure>
</center>
</div>
<h3 id="opportunity3specialization">Opportunity #3: Specialization</h3>
<p>Rich Sutton highlights a <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">bitter lesson</a> for the field of AI research:</p>
<blockquote>
<p>&quot;The great power of general purpose methods [...] that continue to scale with increased computation [...]. The two methods that seem to scale arbitrarily: search and learning.&quot;</p>
</blockquote>
<p>For most under-represented languages, computation and data, however, <em>are</em> limited. It is thus reasonable to incorporate (some amount of) knowledge into our language models to make them more useful for such languages.</p>
<p>This can take the form of biasing the tokenization process, which often produces poor segmentations for languages with a rich morphology or limited data. We can modify the algorithm to prefer tokens that are shared across many languages <sup class="footnote-ref"><a href="#fn144" id="fnref144">[144]</a></sup>, preserve tokens’ morphological structure <sup class="footnote-ref"><a href="#fn145" id="fnref145">[145]</a></sup>, or make the tokenization algorithm more robust to deal with erroneous segmentations <sup class="footnote-ref"><a href="#fn146" id="fnref146">[146]</a></sup>.</p>
<p>We can also exploit the fact that many under-represented languages belong to groups of similar languages. Models focusing on such groups can thus more easily share information across languages. While recent models focus mainly on related languages <sup class="footnote-ref"><a href="#fn73" id="fnref73:2">[73:2]</a></sup><sup class="footnote-ref"><a href="#fn81" id="fnref81:1">[81:1]</a></sup><sup class="footnote-ref"><a href="#fn82" id="fnref82:1">[82:1]</a></sup>, future models may also include language variants and dialects, which can benefit from positive transfer from related languages.</p>
<p>While principled variants of masking such as whole word masking <sup class="footnote-ref"><a href="#fn147" id="fnref147">[147]</a></sup> and PMI-masking <sup class="footnote-ref"><a href="#fn148" id="fnref148">[148]</a></sup> have been found useful in the past, new pre-training objectives that take linguistic characteristics such as rich morphology or tone into account may lead to more sample-efficient learning. Finally, the architeture of models can be adapted to incorporate information about morphology such as in the KinyaBERT model for Kinyarwanda <sup class="footnote-ref"><a href="#fn80" id="fnref80:1">[80:1]</a></sup>.</p>
<div>
<center>
<figure>
      <img src="http://ruder.io/content/images/2022/11/kinyabert_new.png" style="width: 100%" title="KinyaBERT" alt="The State of Multilingual AI">
<figcaption>The KinyaBERT model for Kinyarwanda. The morphological analyzer produces morphemes for every word. The model uses different embeddings for POS tags, stems, and affixes <a href="https://aclanthology.org/2022.acl-long.367/">(Nzeyimana & Rubungo, 2022)</a>.</figcaption>
</figure>
</center>
</div>
<h2 id="conclusion">Conclusion</h2>
<p>While there has been a tremendous amount of progress in recent multilingual AI, there is still a lot more to do. Most importantly, we should focus on creating data that reflects the real-world circumstances of language speakers and to develop language technology that serves the needs of speakers around the world. While there is momentum and increasing awareness that such work is important, it takes a village to develop equitable language technology for the world's languages. <em>Masakhane</em> (&quot;let us build together&quot; in isiZulu)!</p>
<div>
<center>
<figure>
      <img src="http://ruder.io/content/images/2022/11/masakhane_logo.jpeg" style="width: 80%" title="Masakhane logo" alt="The State of Multilingual AI">
    <figcaption>The <a href"https: www.masakhane.io ">Masakhane</a> logo.</figcaption>
</figure>
</center>
</div>
<h2 id="citation">Citation</h2>
<p>For attribution in academic contexts or books, please cite this work as:</p>
<pre><code>Sebastian Ruder, &quot;The State of Multilingual AI&quot;. http://ruder.io/state-of-multilingual-ai/, 2022.
</code></pre>
<p>BibTeX citation:</p>
<pre><code>@misc{ruder2022statemultilingualai,
author = {Ruder, Sebastian},
title = {{The State of Multilingual AI}},
year = {2022},
howpublished = {\url{http://ruder.io/state-of-multilingual-ai/}},
}
</code></pre>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>van Esch, D., Lucassen, T., Ruder, S., Caswell, I., &amp; Rivera, C. (2022). <a href="http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.538.pdf">Writing System and Speaker Metadata for 2,800+ Language Varieties. Proceedings of LREC 2022</a>, (June), 5035–5046. <a href="#fnref1" class="footnote-backref">↩︎</a> <a href="#fnref1:1" class="footnote-backref">↩︎</a> <a href="#fnref1:2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Bender, E. M. (2011). <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.360.2281&amp;rep=rep1&amp;type=pdf">On Achieving and Evaluating Language-Independence in NLP</a>. Linguistic Issues in Language Technology, 6(3), 1–26. <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Ruder, S., Vulić, I., &amp; Søgaard, A. (2022). <a href="https://aclanthology.org/2022.findings-acl.184">Square One Bias in NLP: Towards a Multi-Dimensional Exploration of the Research Manifold</a>. In Findings of the Association for Computational Linguistics: ACL 2022 (pp. 2340–2354). <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>Aji, A. F., Winata, G. I., Koto, F., Cahyawijaya, S., Romadhony, A., Mahendra, R., … Ruder, S. (2022). <a href="https://doi.org/10.18653/v1/2022.acl-long.500">One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia</a>. In Proceedings of ACL 2022 (pp. 7226–7249). <a href="#fnref4" class="footnote-backref">↩︎</a> <a href="#fnref4:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention Is All You Need</a>. In Proceedings of NIPS 2017. <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>Ruder, S., Peters, M., Swayamdipta, S., &amp; Wolf, T. (2019). <a href="https://aclanthology.org/N19-5004/">Transfer learning in natural language processing</a>. Proceedings of NAACL 2019, Tutorial Abstracts. <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … Houlsby, N. (2021). <a href="https://openreview.net/forum?id=YicbFdNTTy">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>. In Proceedings of ICLR 2021. <a href="#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p>Neimark, D., Bar, O., Zohar, M., &amp; Asselmann, D. (2021). Video Transformer Network. In Proceedings of the IEEE International Conference on Computer Vision (Vol. 2021-Octob, pp. 3156–3165). <a href="https://doi.org/10.1109/ICCVW54120.2021.00355">https://doi.org/10.1109/ICCVW54120.2021.00355</a> <a href="#fnref8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p>Baevski, A., Zhou, H., Mohamed, A., &amp; Auli, M. (2020). wav2vec 2.0: A framework for self-supervised learning of speech representations. In Advances in Neural Information Processing Systems. <a href="#fnref9" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p>Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL 2019. <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a> <a href="#fnref10" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., … Stoyanov, V. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv Preprint ArXiv:1907.11692. <a href="http://arxiv.org/abs/1907.11692">http://arxiv.org/abs/1907.11692</a> <a href="#fnref11" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p>Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., … Zettlemoyer, L. (2019). <a href="https://aclanthology.org/2020.acl-main.703/">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a>. In Proceedings of ACL 2019. <a href="#fnref12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., … Liu, P. J. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21. <a href="http://arxiv.org/abs/1910.10683">http://arxiv.org/abs/1910.10683</a> <a href="#fnref13" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn14" class="footnote-item"><p>He, P., Gao, J., &amp; Chen, W. (2021). <a href="https://arxiv.org/abs/2111.09543">DeBERTaV3: Improving DeBERTa using electra-style pre-training with gradient-disentangled embedding sharing</a>. arXiv preprint arXiv:2111.09543. <a href="#fnref14" class="footnote-backref">↩︎</a> <a href="#fnref14:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn15" class="footnote-item"><p>Baevski, A., Zhou, Y., Mohamed, A., &amp; Auli, M. (2020). <a href="https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf">wav2vec 2.0: A framework for self-supervised learning of speech representations</a>. Advances in Neural Information Processing Systems, 33, 12449-12460. <a href="#fnref15" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn16" class="footnote-item"><p>Chung, H. W., Févry, T., Tsai, H., Johnson, M., &amp; Ruder, S. (2021). Rethinking Embedding Coupling in Pre-trained Language Models. In Proceedings of ICLR 2021. <a href="#fnref16" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn17" class="footnote-item"><p>Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., … Stoyanov, V. (2020). Unsupervised Cross-lingual Representation Learning at Scale. In Proceedings of ACL 2020. <a href="http://arxiv.org/abs/1911.02116">http://arxiv.org/abs/1911.02116</a> <a href="#fnref17" class="footnote-backref">↩︎</a> <a href="#fnref17:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn18" class="footnote-item"><p>Liu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., ... &amp; Zettlemoyer, L. (2020). <a href="https://aclanthology.org/2020.tacl-1.47/">Multilingual denoising pre-training for neural machine translation</a>. Transactions of the Association for Computational Linguistics, 8, 726-742. <a href="#fnref18" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn19" class="footnote-item"><p>Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., … Raffel, C. (2021). mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of NAACL 2021. <a href="http://arxiv.org/abs/2010.11934">http://arxiv.org/abs/2010.11934</a> <a href="#fnref19" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn20" class="footnote-item"><p>Dufter, P., &amp; Schütze, H. (2020). <a href="https://aclanthology.org/2020.emnlp-main.358/">Identifying Elements Essential for BERT’s Multilinguality</a>. In Proceedings of EMNLP 2020. <a href="#fnref20" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn21" class="footnote-item"><p>Deshpande, A., Talukdar, P., &amp; Narasimhan, K. (2022). <a href="https://aclanthology.org/2022.naacl-main.264/">When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer</a>. In Proceedings of NAACL 2022. <a href="#fnref21" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn22" class="footnote-item"><p>Rust, P., Pfeiffer, J., Vulić, I., Ruder, S., &amp; Gurevych, I. (2021). <a href="https://aclanthology.org/2021.acl-long.243/">How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models</a>. In Proceedings of ACL 2021. <a href="#fnref22" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn23" class="footnote-item"><p>Conneau, A., Baevski, A., Collobert, R., Mohamed, A., &amp; Auli, M. (2021). <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/conneau21_interspeech.pdf">Unsupervised Cross-lingual Representation Learning for Speech Recognition</a>. In Proceedings of Interspeech 2021. <a href="#fnref23" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn24" class="footnote-item"><p>Wang, C., Wu, Y., Qian, Y., Kumatani, K., Liu, S., Wei, F., ... &amp; Huang, X. (2021, July). <a href="https://proceedings.mlr.press/v139/wang21y.html">Unispeech: Unified speech representation learning with labeled and unlabeled data</a>. In International Conference on Machine Learning (pp. 10937-10947). PMLR. <a href="#fnref24" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn25" class="footnote-item"><p>Goyal, N., Du, J., Ott, M., Anantharaman, G., &amp; Conneau, A. (2021). <a href="https://aclanthology.org/2021.repl4nlp-1.4/">Larger-Scale Transformers for Multilingual Masked Language Modeling</a>. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021). <a href="#fnref25" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn26" class="footnote-item"><p>Hu, J., Ruder, S., Siddhant, A., Neubig, G., Firat, O., &amp; Johnson, M. (2020). XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization. In Proceedings of ICML 2020. <a href="#fnref26" class="footnote-backref">↩︎</a> <a href="#fnref26:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn27" class="footnote-item"><p>Lauscher, A., Ravishankar, V., Vulić, I., &amp; Glavaš, G. (2020). <a href="https://aclanthology.org/2020.emnlp-main.363/">From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers</a>. In Proceedings of EMNLP 2020. <a href="#fnref27" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn28" class="footnote-item"><p>Ahuja, K., Kumar, S., Dandapat, S., &amp; Choudhury, M. (2022). <a href="https://aclanthology.org/2022.acl-long.374/">Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models</a>. In Proceedings of ACL 2022 (pp. 5454–5467). <a href="#fnref28" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn29" class="footnote-item"><p>Muller, B., Anastasopoulos, A., Sagot, B., &amp; Seddah, D. (2021). <a href="https://aclanthology.org/2021.naacl-main.38/">When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models</a>. In Proceedings of NAACL 2021. <a href="#fnref29" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn30" class="footnote-item"><p>Pfeiffer, J., Vulić, I., Gurevych, I., &amp; Ruder, S. (2021). <a href="https://aclanthology.org/2021.emnlp-main.800/">UNKs Everywhere: Adapting Multilingual Language Models to New Scripts</a>. In Proceedings of EMNLP 2021. <a href="#fnref30" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn31" class="footnote-item"><p>Pan, X., Zhang, B., May, J., Nothman, J., Knight, K., &amp; Ji, H. (2017). Cross-lingual name tagging and linking for 282 languages. In Proceedings of ACL 2017 (pp. 1946–1958). <a href="https://doi.org/10.18653/v1/P17-1178">https://doi.org/10.18653/v1/P17-1178</a> <a href="#fnref31" class="footnote-backref">↩︎</a> <a href="#fnref31:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn32" class="footnote-item"><p>Lignos, C., Holley, N., Palen-Michel, C., &amp; Sälevä, J. (2022). <a href="https://aclanthology.org/2022.findings-acl.44/">Toward More Meaningful Resources for Lower-resourced Languages</a>. In Findings of ACL 2022 (pp. 523–532). <a href="#fnref32" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn33" class="footnote-item"><p>Kreutzer, J., Caswell, I., Wang, L., Wahab, A., Van Esch, D., Ulzii-Orshikh, N., … Adeyemi, M. (2022). <a href="https://aclanthology.org/2022.tacl-1.4/">Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets</a>. In Proceedings of ACL 2022 (Vol. 10, pp. 50–72). <a href="#fnref33" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn34" class="footnote-item"><p>Schwenk, H., Chaudhary, V., Sun, S., Gong, H., &amp; Guzmán, F. (2019). <a href="https://arxiv.org/abs/1907.05791">WikiMatrix: Mining 135M Parallel Sentences</a>. arXiv preprint arXiv:1907.05791. <a href="#fnref34" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn35" class="footnote-item"><p>El-Kishky, A., Chaudhary, V., Guzmán, F., &amp; Koehn, P. (2020). <a href="http://aclanthology.lst.uni-saarland.de/2020.emnlp-main.480/">CCAligned: A massive collection of cross-lingual web-document Pairs</a>. In Proceedings of EMNLP 2020, 5960–5969. <a href="#fnref35" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn36" class="footnote-item"><p>Artetxe, M., Aldabe, I., Agerri, R., Perez-de-Viñaspre, O., &amp; Soroa, A. (2022). <a href="http://arxiv.org/abs/2203.08111">Does Corpus Quality Really Matter for Low-Resource Languages?</a> arXiv preprint arXiv:2203.08111. <a href="#fnref36" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn37" class="footnote-item"><p>Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., &amp; Bowman, S. R. (2019). GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In Proceedings of ICLR 2019. <a href="#fnref37" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn38" class="footnote-item"><p>Wang, A., Michael, J., Hill, F., Levy, O., &amp; Bowman, S. R. (2019). SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In Proceedings of NeurIPS 2019. <a href="#fnref38" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn39" class="footnote-item"><p>De Marneffe, M. C., Manning, C. D., Nivre, J., &amp; Zeman, D. (2021). Universal dependencies. Computational linguistics, 47(2), 255-308. <a href="#fnref39" class="footnote-backref">↩︎</a> <a href="#fnref39:1" class="footnote-backref">↩︎</a> <a href="#fnref39:2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn40" class="footnote-item"><p>Artetxe, M., &amp; Schwenk, H. (2019). Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond. Transactions of the ACL 2019. <a href="http://arxiv.org/abs/1812.10464">http://arxiv.org/abs/1812.10464</a> <a href="#fnref40" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn41" class="footnote-item"><p>Blasi, D., Anastasopoulos, A., &amp; Neubig, G. (2022). <a href="https://aclanthology.org/2022.acl-long.376/">Systematic Inequalities in Language Technology Performance across the World’s Languages</a>. In Proceedings of ACL 2022. <a href="#fnref41" class="footnote-backref">↩︎</a> <a href="#fnref41:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn42" class="footnote-item"><p>Ahuja, K., Kumar, S., Dandapat, S., &amp; Choudhury, M. (2022). <a href="https://aclanthology.org/2022.acl-long.374/">Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models</a>. In Proceedings of ACL 2022. <a href="#fnref42" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn43" class="footnote-item"><p>Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., ... &amp; Manica, M. (2022). <a href="https://arxiv.org/abs/2211.05100">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</a>. arXiv preprint arXiv:2211.05100. <a href="#fnref43" class="footnote-backref">↩︎</a> <a href="#fnref43:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn44" class="footnote-item"><p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., … Amodei, D. (2020). Language models are few-shot learners. In Proceedings of NeurIPS 2020. <a href="#fnref44" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn45" class="footnote-item"><p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., … Fiedel, N. (2022). <a href="http://arxiv.org/abs/2204.02311">PaLM: Scaling Language Modeling with Pathways</a>. <a href="#fnref45" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn46" class="footnote-item"><p>Winata, G. I., Madotto, A., Lin, Z., Liu, R., Yosinski, J., &amp; Fung, P. (2021). <a href="https://aclanthology.org/2021.mrl-1.1/">Language Models are Few-shot Multilingual Learners</a>. In Proceedings ofthe 1st Workshop on Multilingual Representation Learning. <a href="#fnref46" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn47" class="footnote-item"><p>Lin, X. V., Mihaylov, T., Artetxe, M., Wang, T., Chen, S., Simig, D., … Li, X. (2022). <a href="http://arxiv.org/abs/2112.10668">Few-shot Learning with Multilingual Language Models</a>. In Proceedings of EMNLP 2022. <a href="#fnref47" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn48" class="footnote-item"><p>Shi, F., Suzgun, M., Freitag, M., Wang, X., Srivats, S., Vosoughi, S., ... &amp; Wei, J. (2022). <a href="https://arxiv.org/abs/2210.03057">Language models are multilingual chain-of-thought reasoners</a>. arXiv preprint arXiv:2210.03057. <a href="#fnref48" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn49" class="footnote-item"><p>Hsu, W. N., Bolte, B., Tsai, Y. H. H., Lakhotia, K., Salakhutdinov, R., &amp; Mohamed, A. (2021). <a href="https://arxiv.org/abs/2106.07447">HuBERT: Self-supervised speech representation learning by masked prediction of hidden units</a>. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29, 3451-3460. <a href="#fnref49" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn50" class="footnote-item"><p>Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., ... &amp; Wei, F. (2022). <a href="https://arxiv.org/abs/2110.13900">WavLM: Large-scale self-supervised pre-training for full stack speech processing</a>. IEEE Journal of Selected Topics in Signal Processing, 16(6), 1505-1518. <a href="#fnref50" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn51" class="footnote-item"><p>Alayrac, J. B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., ... &amp; Simonyan, K. (2022). <a href="https://openreview.net/forum?id=EbMuimAbPbs">Flamingo: a visual language model for few-shot learning</a>. In Proceedings of NeurIPS 2022. <a href="#fnref51" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn52" class="footnote-item"><p>Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., &amp; Chen, M. (2022). <a href="https://arxiv.org/abs/2204.06125">Hierarchical text-conditional image generation with CLIP latents</a>. arXiv preprint arXiv:2204.06125. <a href="#fnref52" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn53" class="footnote-item"><p>Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., ... &amp; Norouzi, M. (2022). <a href="https://arxiv.org/abs/2205.11487">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</a>. arXiv preprint arXiv:2205.11487. <a href="#fnref53" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn54" class="footnote-item"><p>Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., ... &amp; Wu, Y. (2022). <a href="https://arxiv.org/abs/2206.10789">Scaling autoregressive models for content-rich text-to-image generation</a>. arXiv preprint arXiv:2206.10789. <a href="#fnref54" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn55" class="footnote-item"><p>Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., &amp; Sutskever, I. (2022). <a href="https://cdn.openai.com/papers/whisper.pdf">Robust speech recognition via large-scale weak supervision</a>. Tech. Rep., Technical report, OpenAI. <a href="#fnref55" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn56" class="footnote-item"><p>Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A. J., Padlewski, P., Salz, D., ... &amp; Soricut, R. (2022). <a href="https://arxiv.org/abs/2209.06794">PaLI: A jointly-scaled multilingual language-image model</a>. arXiv preprint arXiv:2209.06794. <a href="#fnref56" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn57" class="footnote-item"><p>van Esch, D., Sarbar, E., Lucassen, T., Brien, J. O., Breiner, T., Prasad, M., … Beaufays, F. (2019). <a href="https://arxiv.org/abs/1912.01218">Writing Across the World’s Languages: Deep Internationalization for Gboard, the Google Keyboard.</a> <a href="#fnref57" class="footnote-backref">↩︎</a> <a href="#fnref57:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn58" class="footnote-item"><p>Joanis, E., Knowles, R., Kuhn, R., Larkin, S., Littell, P., Lo, C. K., … Micher, J. (2020). <a href="https://aclanthology.org/2020.lrec-1.312/">The Nunavut Hansard Inuktitut-English Parallel Corpus 3.0 with Preliminary Machine Translation Results</a>. In LREC 2020 - 12th International Conference on Language Resources and Evaluation, Conference Proceedings (pp. 2562–2572). <a href="#fnref58" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn59" class="footnote-item"><p>Barrault, L., Biesialska, M., Costa-jussà, M. R., Federmann, C., Huck, M., Joanis, E., … Post, M. (2020). <a href="https://www.aclweb.org/anthology/2020.wmt-1.1">Findings of the 2020 Conference on Machine Translation (WMT20)</a>. In Proceedings ofthe 5th Conference on Machine Translation (WMT). <a href="#fnref59" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn60" class="footnote-item"><p>Nekoto, W., Marivate, V., Matsila, T., Fasubaa, T., Kolawole, T., Fagbohungbe, T., … Bashir, A. (2020). <a href="https://aclanthology.org/2020.findings-emnlp.195/">Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages</a>. In Findings of EMNLP 2020. <a href="#fnref60" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn61" class="footnote-item"><p>Adelani, D. I., Abbott, J., Neubig, G., Daniel, D., Kreutzer, J., Lignos, C., … Ogueji, K. (2021). <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00416/107614/MasakhaNER-Named-Entity-Recognition-for-African">MasakhaNER: Named Entity Recognition for African Languages</a>. Transactions of the ACL 2021. <a href="#fnref61" class="footnote-backref">↩︎</a> <a href="#fnref61:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn62" class="footnote-item"><p>Wanjawa, B., Wanzare, L., Indede, F., McOnyango, O., Ombui, E., &amp; Muchemi, L. (2022). <a href="https://arxiv.org/abs/2208.12081">Kencorpus: A Kenyan Language Corpus of Swahili, Dholuo and Luhya for Natural Language Processing Tasks</a>. arXiv preprint arXiv:2208.12081. <a href="#fnref62" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn63" class="footnote-item"><p>Adebara, I., Elmadany, A., Abdul-Mageed, M., &amp; Inciarte, A. A. (2022). <a href="https://arxiv.org/abs/2210.11744">AfroLID: A Neural Language Identification Tool for African Languages</a>. In Proceedings of EMNLP 2022. <a href="#fnref63" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn64" class="footnote-item"><p>Niyongabo, R. A., Qu, H., Kreutzer, J., &amp; Huang, L. (2020). <a href="https://aclanthology.org/2020.coling-main.480.pdf">KinNews and KirNews: Benchmarking Cross-Lingual Text Classification for Kinyarwanda and Kirundi</a>. In Proceedings of COLING 2020 (pp. 5507–5521). <a href="#fnref64" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn65" class="footnote-item"><p>Muhammad, S. H., Adelani, D. I., Ruder, S., Ahmad, I. S., Abdulmumin, I., Bello, B. S., … Brazdil, P. (2022). <a href="https://aclanthology.org/2022.lrec-1.63.pdf">NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis</a>. In Proceedings of LREC 2022 (pp. 590–602). <a href="#fnref65" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn66" class="footnote-item"><p>Wanjawa, B., Wanzare, L., Indede, F., McOnyango, O., Muchemi, L., &amp; Ombui, E. (2022). <a href="https://arxiv.org/abs/2205.02364">KenSwQuAD--A Question Answering Dataset for Swahili Low Resource Language</a>. arXiv preprint arXiv:2205.02364. <a href="#fnref66" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn67" class="footnote-item"><p>MBONU, C. E., Chukwuneke, C. I., Paul, R. U., Ezeani, I., &amp; Onyenwe, I. (2022, March). <a href="https://openreview.net/forum?id=rMUccG4LZq">IgboSum1500-Introducing the Igbo Text Summarization Dataset</a>. In 3rd Workshop on African Natural Language Processing. <a href="#fnref67" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn68" class="footnote-item"><p>Dumitrescu, S. D., Rebeja, P., Lorincz, B., Gaman, M., Avram, A., Ilie, M., ... &amp; Patraucean, V. (2021, June). <a href="https://openreview.net/forum?id=JH61CD7afTv">Liro: Benchmark and leaderboard for Romanian language tasks</a>. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1). <a href="#fnref68" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn69" class="footnote-item"><p>Park, S., Moon, J., Kim, S., Cho, W. I., Han, J., Park, J., ... &amp; Cho, K. (2021). <a href="https://arxiv.org/abs/2105.09680">KLUE: Korean language understanding evaluation</a>. arXiv preprint arXiv:2105.09680. <a href="#fnref69" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn70" class="footnote-item"><p>Safaya, A., Kurtuluş, E., Goktogan, A., &amp; Yuret, D. (2022). <a href="https://aclanthology.org/2022.findings-acl.69/">Mukayese: Turkish NLP Strikes Back</a>. In Findings of ACL 2022 (pp. 846–863). <a href="#fnref70" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn71" class="footnote-item"><p>Koto, F., Rahimi, A., Lau, J. H., &amp; Baldwin, T. (2020). <a href="https://aclanthology.org/2020.coling-main.66/">IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP</a>. In Proceedings of COLING 2020 (pp. 757–770). <a href="#fnref71" class="footnote-backref">↩︎</a> <a href="#fnref71:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn72" class="footnote-item"><p>Cahyawijaya, S., Winata, G. I., Wilie, B., Vincentio, K., Li, X., Kuncoro, A., … Fung, P. (2021). <a href="https://aclanthology.org/2021.emnlp-main.699/">IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation</a>. In Proceedings of EMNLP 2021 (pp. 8875–8898). <a href="#fnref72" class="footnote-backref">↩︎</a> <a href="#fnref72:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn73" class="footnote-item"><p>Kakwani, D., Kunchukuttan, A., Golla, S., Gokul, N. C., Bhattacharyya, A., Khapra, M. M., &amp; Kumar, P. (2020). <a href="https://aclanthology.org/2020.findings-emnlp.445/">IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian languages</a>. In Findings of the Association for Computational Linguistics: EMNLP 2020 (pp. 4948-4961). <a href="#fnref73" class="footnote-backref">↩︎</a> <a href="#fnref73:1" class="footnote-backref">↩︎</a> <a href="#fnref73:2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn74" class="footnote-item"><p>Conneau, A., Bapna, A., Zhang, Y., Ma, M., von Platen, P., Lozhkov, A., … Johnson, M. (2022). <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/conneau22_interspeech.pdf">XTREME-S: Evaluating Cross-lingual Speech Representations</a>. In Proceedings of Interspeech 2022. <a href="#fnref74" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn75" class="footnote-item"><p>Bugliarello, E., Liu, F., Pfeiffer, J., Reddy, S., Elliott, D., Ponti, E. M., &amp; Vulić, I. (2022). <a href="https://proceedings.mlr.press/v162/bugliarello22a/bugliarello22a.pdf">IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages</a>. In Proceedings of ICML 2022. <a href="#fnref75" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn76" class="footnote-item"><p>Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., … Weber, G. (2020). <a href="http://aclanthology.lst.uni-saarland.de/2020.lrec-1.520/">Common Voice: A massively-multilingual speech corpus</a>. In LREC 2020 - 12th International Conference on Language Resources and Evaluation, Conference Proceedings (pp. 4218–4222). <a href="#fnref76" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn77" class="footnote-item"><p>McMillan-Major, A., Alyafeai, Z., Biderman, S., Chen, K., De Toni, F., Dupont, G., ... &amp; Jernite, Y. (2022). <a href="https://arxiv.org/abs/2201.10066">Documenting geographically and contextually diverse data sources: The BigScience catalogue of language data and resources</a>. arXiv preprint arXiv:2201.10066. <a href="#fnref77" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn78" class="footnote-item"><p>Ogueji, K., Zhu, Y., &amp; Lin, J. (2021). <a href="https://aclanthology.org/2021.mrl-1.11/">Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-Resource Languages</a>. In Proceedings of Multilingual Representation Learning Workshop 2021. <a href="#fnref78" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn79" class="footnote-item"><p>Alabi, J. O., Adelani, D. I., Mosbach, M., &amp; Klakow, D. (2022). Multilingual Language Model Adaptive Fine-Tuning: A Study on African Languages. In Proceedings of COLING 2022. <a href="http://arxiv.org/abs/2204.06487">http://arxiv.org/abs/2204.06487</a> <a href="#fnref79" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn80" class="footnote-item"><p>Nzeyimana, A., &amp; Rubungo, A. N. (2022). <a href="https://aclanthology.org/2022.acl-long.367/">KinyaBERT: a Morphology-aware Kinyarwanda Language Model</a>. In Proceedings of ACL 2022. <a href="#fnref80" class="footnote-backref">↩︎</a> <a href="#fnref80:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn81" class="footnote-item"><p>Khanuja, S., Bansal, D., Mehtani, S., Khosla, S., Dey, A., Gopalan, B., ... &amp; Talukdar, P. (2021). <a href="https://arxiv.org/abs/2103.10730">MuRIL: Multilingual representations for indian languages</a>. arXiv preprint arXiv:2103.10730. <a href="#fnref81" class="footnote-backref">↩︎</a> <a href="#fnref81:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn82" class="footnote-item"><p>Gupta, A., Chadha, H. S., Shah, P., Chhimwal, N., Dhuriya, A., Gaur, R., &amp; Raghavan, V. (2021). <a href="https://arxiv.org/abs/2107.07402">CLSRIL-23: Cross lingual speech representations for indic languages</a>. arXiv preprint arXiv:2107.07402. <a href="#fnref82" class="footnote-backref">↩︎</a> <a href="#fnref82:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn83" class="footnote-item"><p>Javed, T., Doddapaneni, S., Raman, A., Bhogale, K. S., Ramesh, G., Kunchukuttan, A., ... &amp; Khapra, M. M. (2022, June). <a href="https://arxiv.org/abs/2111.03945">Towards building ASR systems for the next billion users</a>. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 36, No. 10, pp. 10813-10821). <a href="#fnref83" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn84" class="footnote-item"><p>Doddapaneni, S., Ramesh, G., Kunchukuttan, A., Kumar, P., &amp; Khapra, M. M. (2021). <a href="https://arxiv.org/abs/2107.00676">A primer on pretrained multilingual language models</a>. arXiv preprint arXiv:2107.00676. <a href="#fnref84" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn85" class="footnote-item"><p>Yadav, H., &amp; Sitaram, S. (2022). <a href="https://aclanthology.org/2022.lrec-1.542/">A Survey of Multilingual Models for Automatic Speech Recognition</a>. In Proceedings of LREC 2022. <a href="#fnref85" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn86" class="footnote-item"><p>Messaoudi, A., Cheikhrouhou, A., Haddad, H., Ferchichi, N., BenHajhmida, M., Korched, A., ... &amp; Kerkeni, A. (2022). <a href="https://arxiv.org/abs/2111.13138">Tunbert: Pretrained contextualized text representation for tunisian dialect</a>. In International Conference on Intelligent Systems and Pattern Recognition (pp. 278-290). <a href="#fnref86" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn87" class="footnote-item"><p>Bapna, A., Caswell, I., Kreutzer, J., Firat, O., van Esch, D., Siddhant, A., … Hughes, M. (2022). <a href="http://arxiv.org/abs/2205.03983">Building Machine Translation Systems for the Next Thousand Languages</a>. <a href="#fnref87" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn88" class="footnote-item"><p>Team, N., Costa-jussà, M. R., Cross, J., Çelebi, O., Elbayad, M., Heafield, K., … Wang, J. (2022). No Language Left Behind: Scaling Human-Centered Machine Translation. <a href="https://github.com/facebookresearch/fairseq/tree/nllb">https://github.com/facebookresearch/fairseq/tree/nllb</a> <a href="#fnref88" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn89" class="footnote-item"><p>Ritchie, S., Cheng, Y. C., Chen, M., Mathews, R., van Esch, D., Li, B., &amp; Sim, K. C. (2022). <a href="https://arxiv.org/abs/2208.03067">Large vocabulary speech recognition for languages of Africa: multilingual modeling and self-supervised learning</a>. arXiv preprint arXiv:2208.03067. <a href="#fnref89" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn90" class="footnote-item"><p>Joshi, P., Santy, S., Budhiraja, A., Bali, K., &amp; Choudhury, M. (2020). <a href="https://aclanthology.org/2020.acl-main.560/">The State and Fate of Linguistic Diversity and Inclusion in the NLP World</a>. In Proceedings of ACL 2020. <a href="#fnref90" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn91" class="footnote-item"><p>Conneau, A., Lample, G., Rinott, R., Williams, A., Bowman, S. R., Schwenk, H., &amp; Stoyanov, V. (2018). <a href="http://aclanthology.lst.uni-saarland.de/D18-1269/">XNLI: Evaluating Cross-lingual Sentence Representations</a>. In Proceedings of EMNLP 2018. <a href="#fnref91" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn92" class="footnote-item"><p>Artetxe, M., Ruder, S., &amp; Yogatama, D. (2020). <a href="https://aclanthology.org/2020.acl-main.421/">On the Cross-lingual Transferability of Monolingual Representations</a>. In Proceedings of ACL 2020. <a href="#fnref92" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn93" class="footnote-item"><p>Ponti, E. M., Glavaš, G., Majewska, O., Liu, Q., Vulić, I., &amp; Korhonen, A. (2020). <a href="https://aclanthology.org/2020.emnlp-main.185/">XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning</a>. In Proceedings of EMNLP 2020. <a href="#fnref93" class="footnote-backref">↩︎</a> <a href="#fnref93:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn94" class="footnote-item"><p>Volansky, V., Ordan, N., &amp; Wintner, S. (2015). <a href="http://cs.haifa.ac.il/~shuly/publications/vered.pdf">On the features of translationese</a>. Digital Scholarship in the Humanities, 30(1), 98-118. <a href="#fnref94" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn95" class="footnote-item"><p>Artetxe, M., Labaka, G., &amp; Agirre, E. (2020). <a href="https://aclanthology.org/2020.emnlp-main.618/">Translation Artifacts in Cross-lingual Transfer Learning</a>. In Proceedings of EMNLP 2020. <a href="#fnref95" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn96" class="footnote-item"><p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., … &amp; Fei-Fei, L. (2015). <a href="https://arxiv.org/abs/1409.0575">Imagenet large scale visual recognition challenge</a>. International journal of computer vision, 115(3), 211-252. <a href="#fnref96" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn97" class="footnote-item"><p>Liu, F., Bugliarello, E., Ponti, E. M., Reddy, S., Collier, N., &amp; Elliott, D. (2021). <a href="https://aclanthology.org/2021.emnlp-main.818/">Visually Grounded Reasoning across Languages and Cultures</a>. In Proceedings of EMNLP 2021. <a href="#fnref97" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn98" class="footnote-item"><p>Young, P., Lai, A., Hodosh, M., &amp; Hockenmaier, J. (2014). <a href="https://aclanthology.org/Q14-1006/">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</a>. Transactions of the Association for Computational Linguistics, 2, 67-78. <a href="#fnref98" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn99" class="footnote-item"><p>Van Miltenburg, E., Elliott, D., &amp; Vossen, P. (2017). <a href="https://doi.org/10.18653/v1/w17-3503">Cross-linguistic differences and similarities in image descriptions</a>. In INLG 2017 - 10th International Natural Language Generation Conference, Proceedings of the Conference (pp. 21–30). <a href="#fnref99" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn100" class="footnote-item"><p>Roemmele, M., Bejan, C. A., &amp; Gordon, A. S. (2011). <a href="https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF">Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning</a>. In AAAI spring symposium: logical formalizations of commonsense reasoning. <a href="#fnref100" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn101" class="footnote-item"><p>Gor, M., &amp; Webster, K. (2021). <a href="https://aclanthology.org/2021.emnlp-main.444/">Toward Deconfounding the Influence of Entity Demographics for Question Answering Accuracy</a>. In Proceedings of EMNLP 2021. <a href="#fnref101" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn102" class="footnote-item"><p>Faisal, F., Wang, Y., &amp; Anastasopoulos, A. (2022). <a href="https://aclanthology.org/2022.acl-long.239/">Dataset Geography: Mapping Language Data to Language Users</a>. In Proceedings of ACL 2022. <a href="#fnref102" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn103" class="footnote-item"><p>Liu, F., Bugliarello, E., Ponti, E. M., Reddy, S., Collier, N., &amp; Elliott, D. (2021). <a href="https://aclanthology.org/2021.emnlp-main.818/">Visually Grounded Reasoning across Languages and Cultures</a>. In Proceedings of EMNLP 2021. <a href="#fnref103" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn104" class="footnote-item"><p>Bird, S. (2022). <a href="https://aclanthology.org/2022.acl-long.539/">Local Languages, Third Spaces, and other High-Resource Scenarios</a>. In Proceedings of ACL 2022. <a href="#fnref104" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn105" class="footnote-item"><p>Caswell, I., Breiner, T., van Esch, D., &amp; Bapna, A. (2020). <a href="https://aclanthology.org/2020.coling-main.579/">Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus</a>. In Proceedings of COLING 2020 (pp. 6588–6608). <a href="#fnref105" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn106" class="footnote-item"><p>Chen, Z., Zhang, Y., Rosenberg, A., Ramabhadran, B., Moreno, P. J., Bapna, A., &amp; Zen, H. (2022). <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/chen22r_interspeech.pdf">MAESTRO: Matched Speech Text Representations through Modality Matching</a>. In Proceedings of Interspeech 2022. <a href="#fnref106" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn107" class="footnote-item"><p>Saeki, T., Zen, H., Chen, Z., Morioka, N., Wang, G., Zhang, Y., … Ramabhadran, B. (2022). <a href="https://arxiv.org/abs/2210.15447">Virtuoso: Massive Multilingual Speech-Text Joint Semi-Supervised Learning for Text-To-Speech</a>. arXiv preprint arXiv:2210.15447. <a href="#fnref107" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn108" class="footnote-item"><p>Bapna, A., Cherry, C., Zhang, Y., Jia, Y., Johnson, M., Cheng, Y., ... &amp; Conneau, A. (2022). <a href="https://arxiv.org/abs/2202.01374">mSLAM: Massively multilingual joint pre-training for speech and text</a>. arXiv preprint arXiv:2202.01374. <a href="#fnref108" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn109" class="footnote-item"><p>Adebara, I., &amp; Abdul-Mageed, M. (2022). <a href="https://aclanthology.org/2022.acl-long.265/">Towards Afrocentric NLP for African Languages: Where We Are and Where We Can Go</a>. In Proceedings of ACL 2022. <a href="#fnref109" class="footnote-backref">↩︎</a> <a href="#fnref109:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn110" class="footnote-item"><p>Rijhwani, S., Anastasopoulos, A., &amp; Neubig, G. (2020). <a href="https://aclanthology.org/2020.emnlp-main.478/">OCR Post Correction for Endangered Language Texts</a>. In Proceedings of EMNLP 2020. <a href="#fnref110" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn111" class="footnote-item"><p>Leong, C., Nemecek, J., Mansdorfer, J., Filighera, A., Owodunni, A., &amp; Whitenack, D. (2022). <a href="http://arxiv.org/abs/2210.14712">Bloom Library: Multimodal Datasets in 300+ Languages for a Variety of Downstream Tasks</a>. In Proceedings of EMNLP 2022. <a href="#fnref111" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn112" class="footnote-item"><p>Ebrahimi, A., &amp; Kann, K. (2021). <a href="https://aclanthology.org/2021.acl-long.351/">How to Adapt Your Pretrained Multilingual Model to 1600 Languages</a>. In Proceedings of ACL 2021. <a href="#fnref112" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn113" class="footnote-item"><p>Wang, X., Ruder, S., &amp; Neubig, G. (2022). <a href="https://aclanthology.org/2022.acl-long.61/">Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation</a>. In Proceedings of ACL 2022. <a href="#fnref113" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn114" class="footnote-item"><p>Plank, B. <a href="https://aclanthology.org/W16-3901/">Processing non-canonical or noisy text: fortuitous data to the rescue</a>. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT). <a href="#fnref114" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn115" class="footnote-item"><p>Aghajanyan, A., Okhonko, D., Lewis, M., Joshi, M., Xu, H., Ghosh, G., &amp; Zettlemoyer, L. (2021). <a href="https://arxiv.org/abs/2107.06955">HTLM: Hyper-text pre-training and prompting of language models</a>. arXiv preprint arXiv:2107.06955. <a href="#fnref115" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn116" class="footnote-item"><p>Varab, D., &amp; Schluter, N.. <a href="https://aclanthology.org/2021.emnlp-main.797/">MassiveSumm: a very large-scale, very multilingual, news summarisation dataset</a>. In Proceedings of EMNLP 2021. <a href="#fnref116" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn117" class="footnote-item"><p>Card, D., Henderson, P., Khandelwal, U., Jia, R., Mahowald, K., &amp; Jurafsky, D. (2020). <a href="http://aclanthology.lst.uni-saarland.de/2020.emnlp-main.745/">With Little Power Comes Great Responsibility</a>. In Proceedings of EMNLP 2020. <a href="#fnref117" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn118" class="footnote-item"><p>Khanuja, S., Ruder, S., &amp; Talukdar, P. (2022). Evaluating <a href="https://arxiv.org/abs/2205.12676">Inclusivity, Equity, and Accessibility of NLP Technology: A Case Study for Indian Languages</a>. arXiv preprint arXiv:2205.12676. <a href="#fnref118" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn119" class="footnote-item"><p>Abebe, R., Aruleba, K., Birhane, A., Kingsley, S., Obaido, G., Remy, S. L., &amp; Sadagopan, S. (2021). <a href="https://arxiv.org/abs/2103.01168">Narratives and Counternarratives on Data Sharing in Africa</a>. In Conference on Fairness, Accountability, and Transparency (FAccT ’21). Association for Computing Machinery. <a href="#fnref119" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn120" class="footnote-item"><p>Birhane, A., Isaac, W., Prabhakaran, V., Díaz, M., Elish, M. C., Gabriel, I., &amp; Mohamed, S. (2022). <a href="https://arxiv.org/abs/2209.07572">Power to the People? Opportunities and Challenges for Participatory AI</a>. Equity and Access in Algorithms, Mechanisms, and Optimization. <a href="#fnref120" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn121" class="footnote-item"><p>Ahia, O., Kreutzer, J., &amp; Hooker, S. (2021). <a href="https://aclanthology.org/2021.findings-emnlp.282/">The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation</a>. In Findings of EMNLP 2021. <a href="#fnref121" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn122" class="footnote-item"><p>Tay, Y., Dehghani, M., Bahri, D., &amp; Metzler, D. (2020). Efficient transformers: A survey. ACM Computing Surveys (CSUR). <a href="#fnref122" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn123" class="footnote-item"><p>Treviso, M., Ji, T., Lee, J. U., van Aken, B., Cao, Q., Ciosici, M. R., ... &amp; Schwartz, R. (2022). <a href="https://arxiv.org/abs/2209.00099">Efficient Methods for Natural Language Processing: A Survey</a>. arXiv preprint arXiv:2209.00099. <a href="#fnref123" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn124" class="footnote-item"><p>Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., &amp; Raffel, C. (2022). <a href="https://arxiv.org/abs/2205.05638">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning</a>. arXiv preprint arXiv:2205.05638. <a href="#fnref124" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn125" class="footnote-item"><p>Rebuffi, S. A., Bilen, H., &amp; Vedaldi, A. (2017). <a href="https://proceedings.neurips.cc/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf">Learning multiple visual domains with residual adapters</a>. Advances in Neural Information Processing Systems, 30. <a href="#fnref125" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn126" class="footnote-item"><p>Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., ... &amp; Gelly, S. (2019, May). <a href="http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf">Parameter-efficient transfer learning for NLP</a>. In International Conference on Machine Learning (pp. 2790-2799). PMLR. <a href="#fnref126" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn127" class="footnote-item"><p>Pfeiffer, J., Vulić, I., Gurevych, I., &amp; Ruder, S. (2021). <a href="https://aclanthology.org/2021.emnlp-main.800/">UNKs Everywhere: Adapting Multilingual Language Models to New Scripts</a>. In Proceedings of EMNLP 2021. <a href="#fnref127" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn128" class="footnote-item"><p>Pfeiffer, J., Goyal, N., Lin, X. V., Li, X., Cross, J., Riedel, S., &amp; Artetxe, M. (2022). <a href="https://aclanthology.org/2022.naacl-main.255/">Lifting the Curse of Multilinguality with Modular Transformers</a>. In Proceedings of NAACL 2022. <a href="#fnref128" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn129" class="footnote-item"><p>Pfeiffer, J., Vulić, I., Gurevych, I., &amp; Ruder, S. (2020). <a href="https://aclanthology.org/2020.emnlp-main.617/">MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer</a>. In Proceedings of EMNLP 2020. <a href="#fnref129" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn130" class="footnote-item"><p>He, R., Liu, L., Ye, H., Tan, Q., Ding, B., Cheng, L., … Si, L. (2021). <a href="https://aclanthology.org/2021.acl-long.172/">On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation</a>. In Proceedings of ACL 2021. <a href="#fnref130" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn131" class="footnote-item"><p>Han, W., Pang, B., &amp; Wu, Y. (2021). <a href="https://aclanthology.org/2021.acl-short.108/">Robust Transfer Learning with Pretrained Language Models through Adapters</a>. In Proceedings of ACL 2021. <a href="#fnref131" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn132" class="footnote-item"><p>Mahabadi, R. K., Ruder, S., Dehghani, M., &amp; Henderson, J. (2021). <a href="https://aclanthology.org/2021.acl-long.47/">Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks</a>. In Proceedings of ACL 2021. <a href="#fnref132" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn133" class="footnote-item"><p>Mahabadi, R. K., Henderson, J., &amp; Ruder, S. (2021). <a href="https://arxiv.org/abs/2106.04647">Compacter: Efficient Low-Rank Hypercomplex Adapter Layers</a>. In Proceedings of NeurIPS 2021. <a href="#fnref133" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn134" class="footnote-item"><p>Mahabadi, R. K., Zettlemoyer, L., Henderson, J., Saeidi, M., Mathias, L., Stoyanov, V., &amp; Yazdani, M. (2022). <a href="https://aclanthology.org/2022.acl-long.254/">PERFECT: Prompt-free and Efficient Few-shot Learning with Language Models</a>. In Proceedings of ACL 2022. <a href="#fnref134" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn135" class="footnote-item"><p>Chronopoulou, A., Peters, M. E., &amp; Dodge, J. (2022). <a href="https://aclanthology.org/2022.naacl-main.96/">Efficient Hierarchical Domain Adaptation for Pretrained Language Models</a>. In Proceedings of NAACL 2022. <a href="#fnref135" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn136" class="footnote-item"><p>Ansell, A., Ponti, E. M., Pfeiffer, J., Ruder, S., Glavaš, G., Vulić, I., &amp; Korhonen, A. (2021). <a href="https://aclanthology.org/2021.findings-emnlp.410/">MAD-G: Multilingual Adapter Generation for Efficient Cross-Lingual Transfer</a>. In Findings of EMNLP 2021. <a href="#fnref136" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn137" class="footnote-item"><p>Üstün, A., Bisazza, A., Bouma, G., van Noord, G., &amp; Ruder, S. (2022). <a href="https://arxiv.org/abs/2205.12148">Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer</a>. In Proceedings of EMNLP 2022. <a href="#fnref137" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn138" class="footnote-item"><p>He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., &amp; Neubig, G. (2022). <a href="https://openreview.net/pdf?id=0RDcd5Axok">Towards a Unified View of Parameter-Efficient Transfer Learning</a>. Proceedings of ICLR 2022. <a href="#fnref138" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn139" class="footnote-item"><p>Ansell, A., Ponti, E. M., Korhonen, A., &amp; Vulić, I. (2022). <a href="https://aclanthology.org/2022.acl-long.125/">Composable Sparse Fine-Tuning for Cross-Lingual Transfer</a>. In Proceedings of ACL 2022. <a href="#fnref139" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn140" class="footnote-item"><p>Bapna, A., &amp; Firat, O. (2019). <a href="https://aclanthology.org/D19-1165/">Simple, Scalable Adaptation for Neural Machine Translation</a>. In Proceedings of EMNLP 2019. <a href="#fnref140" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn141" class="footnote-item"><p>Üstün, A., Bérard, A., Besacier, L., &amp; Gallé, M. (2021). <a href="https://aclanthology.org/2021.emnlp-main.533/">Multilingual Unsupervised Neural Machine Translation with Denoising Adapters</a>. In Proceedings of EMNLP 2021. <a href="#fnref141" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn142" class="footnote-item"><p>Lu, Y., Huang, M., Qu, X., Wei, P., &amp; Ma, Z. (2022, May). <a href="https://arxiv.org/abs/2203.04583">Language adaptive cross-lingual speech representation learning with sparse sharing sub-networks</a>. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). <a href="#fnref142" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn143" class="footnote-item"><p>Le, H., Pino, J., Wang, C., Gu, J., Schwab, D., &amp; Besacier, L. (2021). <a href="https://aclanthology.org/2021.acl-short.103/">Lightweight Adapter Tuning for Multilingual Speech Translation</a>. In Proceedings of ACL 2021. <a href="#fnref143" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn144" class="footnote-item"><p>Patil, V., Talukdar, P., &amp; Sarawagi, S. (2022). <a href="https://aclanthology.org/2022.acl-long.18/">Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages</a>. In Proceedings of ACL 2022. <a href="#fnref144" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn145" class="footnote-item"><p>Hofmann, V., Schütze, H., &amp; Pierrehumbert, J. B. (2022). <a href="https://aclanthology.org/2022.acl-short.43/">An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers</a>. In Proceedings of ACL 2022. <a href="#fnref145" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn146" class="footnote-item"><p>Wang, X., Ruder, S., &amp; Neubig, G. (2021). <a href="https://aclanthology.org/2021.naacl-main.40/">Multi-view Subword Regularization</a>. In Proceedings of NAACL 2021. <a href="#fnref146" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn147" class="footnote-item"><p>Cui, Y., Che, W., Liu, T., Qin, B., &amp; Yang, Z. (2021). <a href="https://arxiv.org/abs/1906.08101">Pre-training with whole word masking for chinese bert</a>. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29, 3504-3514. <a href="#fnref147" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn148" class="footnote-item"><p>Levine, Y., Leyton-brown, K., Labs, A. I., &amp; Aviv, T. (2021). <a href="https://openreview.net/pdf?id=3Aoft6NWFej">PMI-Masking: Principled Masking of Correlated Spans</a>. In Proceedings of ICLR 2021. <a href="#fnref148" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
<!--kg-card-end: markdown--><p><br></p>]]></content:encoded></item><item><title><![CDATA[ACL 2022 Highlights]]></title><description><![CDATA[This post discusses my highlights of ACL 2022, including language diversity and multimodality, prompting, the next big ideas and keynotes, my favorite papers, and the hybrid conference experience.]]></description><link>http://ruder.io/acl2022/</link><guid isPermaLink="false">629dd7d20edbd45176b5a204</guid><category><![CDATA[events]]></category><category><![CDATA[natural language processing]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Mon, 06 Jun 2022 10:33:47 GMT</pubDate><media:content url="http://ruder.io/content/images/2022/06/acl_dublin_logo.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2022/06/acl_dublin_logo.png" alt="ACL 2022 Highlights"><p><a href="https://www.2022.aclweb.org/">ACL 2022</a> took place in Dublin from 22nd–27th May 2022. This was my first in-person conference since ACL 2019. This is also my first conference highlights post since <a href="https://ruder.io/naacl2019/">NAACL 2019</a>. With 1032 accepted papers (604 long, 97 short, 331 in Findings), this post can only offer a glimpse of the diverse research presented at the conference—biased towards my research interests.</p><p>Here are the themes that were most noticeable for me across the conference program:</p><ul><li><a href="#language-diversity-and-multimodality">Language Diversity and Multimodality</a></li><li><a href="#prompting">Prompting</a></li><li><a href="#next-big-ideas">Next Big Ideas</a></li><li><a href="#favorite-papers">Favorite Papers</a></li><li><a href="#the-dark-matter-of-language-and-intelligence">The Dark Matter of Language and Intelligence</a></li><li><a href="#hybrid-conference-experience">Hybrid Conference Experience</a></li></ul><p>Here are highlights of other conference attendees:</p><ul><li><a href="https://www.mubasharaakhtar.com/post/multimodality-at-acl-2022">Multimodality at ACL 2022</a> by Mubashara Akhtar</li><li><a href="https://jlibovicky.github.io/2022/06/02/Notes-from-ACL-2022.html">Notes from ACL 2022</a> by Jindřich Libovický</li><li><a href="https://yongzx.github.io/blog/articles/22/ACL22">My First In-Person NLP Conference</a> by Yong Zheng-Xin</li></ul><p>If you attended ACL, I encourage you to reflect on and write up your impressions of the conference (send me a message and I will link them here).</p><h2 id="language-diversity-and-multimodality">Language diversity and multimodality</h2><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2022/06/acl_2022_keynote_panel.png" class="kg-image" alt="ACL 2022 Highlights"><figcaption>Panelists and their spoken languages at the ACL 2022 keynote panel on supporting linguistic diversity.</figcaption></figure><p>ACL 2022 had a theme track on the topic of “Language Diversity: from Low-Resource to Endangered Languages”. Beyond the excellent papers in the track, language diversity also permeated other parts of the conference. Steven Bird hosted a panel on language diversity featuring researchers speaking and studying under-represented languages (<a href="https://docs.google.com/presentation/d/1GfrtGnw-bfWWk7jltYeDyVuY9QIlTbW9lqW9IlgHOHs">slides</a>). The panelists shared their experiences and discussed interlingual power dynamics, among other topics. They also made practical suggestions to encourage more work on such languages: creating data resources; establishing a conference track for work on low-resource and endangered languages; and encouraging researchers to apply their systems to low-resource language data. They also mentioned a positive development, that researchers are becoming more aware of the value of high-quality datasets. Overall, the panelists emphasised that working with such languages requires respect—towards the speakers, the culture, and the languages themselves.</p><p>Endangered languages were also the focus of the <a href="https://computel-workshop.org/computel-5/">Compute-EL workshop</a>. In the awards ceremony, the <a href="https://aclanthology.org/2022.acl-long.367/">best linguistic insight paper</a> proposed KinyaBERT, a pre-trained model for Kinyarwanda that leverages a morphological analyzer. The <a href="https://aclanthology.org/2022.acl-long.507/">best theme paper</a> developed speech synthesis models for three Canadian indigenous languages. The latter is an example of how multimodal approaches can benefit language diversity.</p><p>Other multimodal papers leveraged phone representations to improve NER performance in Swahili and Kinyarwanda (<a href="https://aclanthology.org/2022.acl-long.364/">Leong &amp; Whitenack</a>). For low-resource text-to-speech, <a href="https://aclanthology.org/2022.acl-long.472/">Lux &amp; Vu</a> employ articulatory features such as position (e.g., frontness of the tongue) and category (e.g., voicedness), which generalize better to unseen phonemes. Some work also explored new multimodal applications such as detecting fingerspelling in American sign language (<a href="https://aclanthology.org/2022.acl-long.119/">Shi et al.</a>) or translating songs for tonal languages (<a href="https://aclanthology.org/2022.findings-acl.60/">Guo et al.</a>).</p><p>The <a href="https://mml-workshop.github.io/">Multilingual Multimodal workshop</a> hosted a shared task on multilingual visually grounded reasoning on the <a href="https://marvl-challenge.github.io/">MaRVL dataset</a>. Seeing the emergence of such multilingual multimodal approaches is particularly encouraging as it is an improvement over the previous year’s ACL where multimodal approaches mainly dealt with English (based on an analysis of “multi-dimensional” NLP research we did for an <a href="https://aclanthology.org/2022.findings-acl.184/">ACL 2022 Findings paper</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2022/06/IMG_8540.JPG" class="kg-image" alt="ACL 2022 Highlights"><figcaption>My invited talk on scaling NLP systems to the next 1000 languages.</figcaption></figure><p>In an invited talk (<a href="https://drive.google.com/file/d/16YfD5Dh4PJwB-h_NOk4xLJYbgEL-tF7w/view">slides</a>), I emphasized multimodality in addition to three other challenges towards scaling NLP systems to the next 1,000 languages: computational efficiency, real-world evaluation, and language varieties. Multimodality is also at the heart of the <a href="https://www.2022.aclweb.org/dispecialinitiative">ACL 2022 D&amp;I Special Initiative “60-60 Globalization via localisation”</a> announced by Mona Diab. The initiative focuses on making Computational Linguistics (CL) research accessible in 60 languages and across all modalities, including text/speech/sign language translation, closed captioning, and dubbing. Another useful aspect of the initiative is the curation of the most common CL terms and their translation into 60 languages. The unavailability of accurate scientific expressions poses a barrier to entry in many languages (see <a href="https://www.masakhane.io/ongoing-projects/masakhane-mt-decolonise-science">this related Masakhane project</a> to decolonise science). The CL community is well positioned to advance the accessibility of scientific content and I am excited to see the progress of this grassroots initiative.</p><p>Under-represented languages typically have little text data available. Two tutorials focused on applying models to such low-resource settings. The <a href="https://github.com/diyiy/ACL2022_Limited_Data_Learning_Tutorial">tutorial on learning with limited text data</a> discussed data augmentation, semi-supervised learning, and applications to multilinguality while the tutorial on <a href="https://github.com/diyiy/ACL2022_Limited_Data_Learning_Tutorial">zero-shot and few-shot NLP with pre-trained language models</a> covered prompting, in-context learning, gradient-based LM task adaptation, among others.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2022/06/kinyabert.png" class="kg-image" alt="ACL 2022 Highlights"><figcaption>The KinyaBERT model architecture. The morphological analyzer produces morphemes for each word. Embeddings of POS tags (red), affix set (purple), affixes (green), and stems (yellow) are fed into a shallow transformer encoder. The output is then processed by a standard transformer encoder (<a href="https://aclanthology.org/2022.acl-long.367/">Nzeyimana &amp; Rubungo</a>).</figcaption></figure><p>How to optimally represent tokens across different languages is an open problem. The conference program featured several new approaches to overcome this challenge. KinyaBERT (<a href="https://aclanthology.org/2022.acl-long.367/">Nzeyimana &amp; Rubungo</a>) leveraged a morphological word segmentation approach. Similarly, <a href="https://aclanthology.org/2022.acl-short.43/">Hofmann et al.</a> propose a method that aims to preserve the morphological structure of words during tokenization. The algorithm tokenizes a word by determining its longest substring in the vocabulary and then recursing on the remaining string until a certain number of recursive calls.</p><p>Rather than choosing subwords that occur frequently in the multilingual pre-training data (which biases the model towards high-resource languages), <a href="https://aclanthology.org/2022.acl-long.18/">Patil et al.</a> propose a method that prefers subwords that are shared across multiple languages. Both CANINE (<a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00448/109284/Canine-Pre-training-an-Efficient-Tokenization-Free">Clark et al.</a>) and ByT5 (<a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00461/110049/ByT5-Towards-a-Token-Free-Future-with-Pre-trained">Xue et al.</a>) do away with tokenization completely and operate directly on bytes.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2022/06/cultural_nlp.png" class="kg-image" alt="ACL 2022 Highlights"><figcaption>Four dimensions along which cultures vary, and for which NLP can be culturally biased (<a href="https://aclanthology.org/2022.acl-long.482/">Hershcovich et al.</a>).</figcaption></figure><p>Languages naturally do not only differ in their linguistic form but also in their culture, which includes the shared knowledge, values, and goals of speakers, among other things. <a href="https://aclanthology.org/2022.acl-long.482/">Hershcovich et al.</a> provide a great overview of what is important for cross-cultural NLP. A particular form of cultural-specific knowledge relates to time and temporal expressions such as morning, which may refer to different hours in different languages (<a href="https://aclanthology.org/2022.findings-acl.224/">Schwartz</a>).</p><p>Here are some of my favorite papers, beyond the ones already mentioned above:</p><ul><li><a href="https://aclanthology.org/2022.acl-long.265/">Towards Afrocentric NLP for African Languages: Where We Are and Where We Can Go</a> (Adebara &amp; Abdul-Mageed). This paper discusses challenges of NLP for African languages and makes practical recommendations on how to address each. It highlights both linguistic phenomena (handling tone, vowel harmony, and serial verb constructions) and other challenges on the continent (low literacy, non-standardized orthographies, lack of language use in official contexts).</li><li><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00447/109285/Quality-at-a-Glance-An-Audit-of-Web-Crawled">Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets</a> (Kreutzer et al.). I <a href="https://newsletter.ruder.io/issues/qa-how-did-we-get-here-adapting-to-time-data-detectives-379447">wrote about this paper</a> when it first came out. It conducts a careful audit of large-scale multilingual datasets covering 70 languages and identifies many data quality issues that have previously gone unnoticed. It highlights that many low-resource language datasets have low quality and that some datasets are even completely mislabeled.</li><li><a href="https://aclanthology.org/2022.acl-long.374/">Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models</a> (Ahuja et al.). We would like to know how well a model does if we apply it to data in a new language, which can inform how many examples we need to annotate. This paper makes performance prediction more robust by jointly learning to predict performance across multiple tasks. This also enables an analysis of features that affect zero-shot transfer across all tasks.</li></ul><p>I had the chance to collaborate on a couple of papers in this space:</p><ul><li><a href="https://aclanthology.org/2022.acl-long.500/">One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia</a> (Aji et al.). We provide an overview of NLP challenges for Indonesia’s 700+ languages (Indonesia is the world’s second most linguistically diverse country). Among these are dialectal and style differences, code-mixing, and orthographic variation. We make practical recommendations such as documenting dialect, style, and register information in datasets.</li><li><a href="https://aclanthology.org/2022.acl-long.61/">Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation</a> (Wang et al.). We analyze different strategies using bilingual lexicons (which are available in around 5000 languages) to synthesize data for training models for extremely low-resource languages and how such data can be combined with existing data, if available. In particular, we find that this works much better than translation (as the performance of an NMT model for such languages is often poor).</li><li><a href="https://aclanthology.org/2022.findings-acl.184/">Square One Bias in NLP: Towards a Multi-Dimensional Exploration of the Research Manifold</a> (Ruder et al.). We identify the current prototypical NLP experiment (the “square one”) and assess the dimensions along which NLP papers make contributions that go beyond this prototype by annotating the 461 ACL 2021 oral papers. We find that almost 70% of papers evaluate only in English and almost 40% of papers only evaluate performance. Only 6.3% of papers evaluate the bias or fairness of a method and only 6.1% of papers are “multi-dimensional”, i.e., they make a contribution along two or more of our investigated dimensions.</li></ul><h2 id="prompting">Prompting</h2><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2022/06/open_prompt.png" class="kg-image" alt="ACL 2022 Highlights"><figcaption>The interplay of prompt-related components in <a href="https://github.com/thunlp/OpenPrompt">OpenPrompt</a>.</figcaption></figure><p>Prompting was another area that received a lot of attention. The <a href="https://aclanthology.org/2022.acl-demo.10/">best demo paper</a> was <a href="https://github.com/thunlp/OpenPrompt">OpenPrompt</a>, an open-source framework for learning with prompts that allows to easily define templates and verbalizers and to combine them with pre-trained models.</p><p>A common thread of research was to incorporate external knowledge into learning. <a href="https://aclanthology.org/2022.acl-long.158/">Hu et al.</a> propose to expand the verbalizer with words from a knowledge base. <a href="https://aclanthology.org/2022.acl-long.225/">Liu et al.</a> use an LM to generate relevant knowledge statements in a few-shot setting. A second LM then uses these to answer commonsense questions. We can also incorporate additional knowledge by modifying the training data, e.g., by inserting metadata strings (e.g., entity types and descriptions) after entities (<a href="https://aclanthology.org/2022.findings-acl.137/">Arora et al.</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2022/06/summarization_entity_chain.png" class="kg-image" alt="ACL 2022 Highlights"><figcaption>Fine-tuning a Transformer encoder-decoder to predict a target summary and entity chain (<a href="https://aclanthology.org/2021.tacl-1.88/">Narayan et al.</a>).</figcaption></figure><p>Other papers proposed prompts particularly suited for specific applications. <a href="https://aclanthology.org/2022.acl-short.94/">Reif et al.</a> propose to provide a model with examples of multiple styles for style transfer while <a href="https://aclanthology.org/2022.acl-short.36/">Tabasi et al.</a> compare independently obtained embeddings of [MASK] tokens using a similarity function for semantic similarity tasks. <a href="https://aclanthology.org/2021.tacl-1.88/">Narayan et al.</a> steer a summarization model by training it to predict a chain of entities before the target summary (e.g., “[ENTITYCHAIN] Frozen | Disney“). <a href="https://arxiv.org/abs/2103.00453">Schick et al.</a> prompt a model with questions containing an attribute (e.g., “Does the above text contain a threat?”) to diagnose if a model’s generated text is offensive. <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00468/110538/PADA-Example-based-Prompt-Learning-for-on-the-fly">Ben-David et al.</a> generate the domain name and domain-related features as a prompt for domain adaptation.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2022/06/clip_prompts.png" class="kg-image" alt="ACL 2022 Highlights"><figcaption>Generating prompts using T5 for image-text-alignment with CLIP (<a href="https://aclanthology.org/2022.acl-long.421/">Song et al.</a>).</figcaption></figure><p>Prompting in a multimodal setting also received some attention. <a href="https://aclanthology.org/2022.acl-long.197/">Jin et al.</a> analyze the effect of diverse prompts in a few-shot setting. <a href="https://aclanthology.org/2022.acl-long.421/">Song et al.</a> investigate prompting for vision-and-language few-shot learning with <a href="https://openai.com/blog/clip/">CLIP</a>. They generate prompts based on VQA questions using T5 and filter out impossible answers with the LM. Prompts are then paired with the target image and used to calculate image-text alignment scores with CLIP.</p><p>Finally, there were a few papers seeking to obtain a better understanding of prompting. <a href="https://aclanthology.org/2022.findings-acl.50/">Mishra et al.</a> explore different ways of reformulating instructions such as decomposing a complex task into several simpler tasks or itemizing instructions. <a href="https://aclanthology.org/2022.acl-long.556/">Lu et al.</a> analyze the sensitivity of models to the order of few-shot examples. As the best permutation cannot be identified without additional development data, they generate a synthetic dev set using the LM itself and determine the best example order via entropy.</p><p>The following papers on which I collaborated relate to few-shot learning:</p><ul><li><a href="https://aclanthology.org/2022.acl-long.38/">FewNLU: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding</a> (Zheng et al.). We introduce an evaluation framework to make few-shot evaluation more reliable, including a new data split strategy. We re-evaluate state-of-the-art few-shot learning methods under this framework. We observe that the absolute and relative performance of some methods was overestimated and that improvements of some methods decrease with a larger pre-trained model, among other things.</li><li><a href="https://aclanthology.org/2022.acl-long.521/">Memorisation versus Generalisation in Pre-trained Language Models</a> (Tänzer et al.). We study the memorisation and generalisation behaviour of state-of-the-art pre-trained models. We observe that current models are resistant even to high degrees of label noise and that training can be separated into three distinct phases. We also observe that pre-trained models forget drastically less than non-pre-trained models. Finally, we propose an extension to make models more resilient to low-frequency patterns.</li></ul><h2 id="next-big-ideas">Next Big Ideas</h2><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2022/06/next_big_ideas_panel.png" class="kg-image" alt="ACL 2022 Highlights"><figcaption>The next big ideas panel featuring (from left to right) Dan Roth, Heng Ji, Thamar Solorio, Mirella Lapata and Iryna Gurevych (moderator). Not pictured: Marco Baroni, Eduard Hovy, and Hang Li who participated virtually.</figcaption></figure><p>One of my favorite sessions of the conference was the <a href="https://www.2022.aclweb.org/invited-talks">Next Big Ideas session</a>, a new format pioneered by the conference organizers. The session featured senior researchers providing opinionated takes on important research directions.</p><p>Two themes stuck out for me during this session: structure and modularity. Researchers stressed the need for extracting and representing structured information such as relations, events, and narratives. They also emphasised the importance of putting thought into <em>how</em> these are represented—through human definitions and the design of appropriate schemas. Many topics required dealing with multiple interdependent tasks, whether for story understanding, reasoning, or schema learning. This will require multiple models or components interfacing with each other. If you want to learn more about modular approaches, we will be teaching a tutorial on modular and parameter-efficient fine-tuning for NLP models at EMNLP 2022. As a whole, these research proposals sketched a compelling vision of NLP models extracting, representing, and reasoning with complex knowledge in a structured, multi-agent manner.</p><p><a href="http://blender.cs.illinois.edu/hengji.html">Heng Ji</a> started off the session with a passionate plea for more structure in NLP models. She emphasized moving towards corpus-level IE (from current sentence-level and document-level IE) and noted the extraction of relations and structures from other types of text such as scientific articles and relation and event extraction for low-resource languages. In the multimodal setting, images and videos can be converted into visual tokens, organized into structures, and described with structured templates. Extracted structures can be further generalized into patterns and event schemas. We can represent structure by embedding it in pre-trained models, encoding it via a graph neural network or via global constraints.</p><p><a href="https://homepages.inf.ed.ac.uk/mlap/index.php?page=index">Mirella Lapata</a> discussed stories and why we should pay attention to them. Stories have shape, structure, and recurrent themes and are at the heart of NLU. They are also relevant for many practical applications such as question answering and summarization. To process stories, we need to do semi-supervised learning and train models that can process very long inputs and deal with multiple, interdependent tasks (such as modeling characters, events, temporality, etc). This requires modular models as well as including humans in the loop.</p><p><a href="https://www.cis.upenn.edu/~danroth/">Dan Roth</a> stressed the importance of reasoning for making decisions based on NLU. In light of the diverse set of reasoning processes, this requires multiple interdependent models and a planning process that determines what modules are relevant. We also need to be able to reason about time and other quantities. To this end, we need to be able to extract, contextualize, and scope relevant information and provide explanations for the reasoning process. To supervise models, we can use incidental supervision such as comparable texts (<a href="https://arxiv.org/abs/2005.12339">Roth, 2017</a>).</p><p><a href="http://solorio.uh.edu/">Thamar Solorio</a> discussed how to serve the half of the world’s population that is multilingual and frequently employs <a href="https://en.wikipedia.org/wiki/Code-switching">code-switching</a>. In contrast, current language technology mainly caters to monolingual speakers. Informal settings where code-switching is commonly used are becoming increasingly relevant such as in the context of chatbots, voice assistants, and social media. She noted challenges such as limited resources, “noise” in conversational data, and issues with transliterated data. We also need to identify relevant uses as code-switching is not relevant in all NLP scenarios. Ultimately, “we need language models that are representative of the actual ways in which people use language” (<a href="https://aclanthology.org/2022.acl-long.385/">Dingemanse &amp; Liesenfeld</a>). For more on code-switching, check out <a href="https://aclanthology.org/2021.acl-long.131/">this excellent ACL 2021 survey</a>.</p><p><a href="https://marcobaroni.org/">Marco Baroni</a> focused on modularity. He laid out a research vision where frozen pre-trained networks autonomously interact by interfacing with each other to solve new tasks together. He suggested that models should communicate through a learned interface protocol that is easily generalizable.</p><p><a href="https://www.cs.cmu.edu/~hovy/">Eduard Hovy</a> urged us to rediscover the need for representation and knowledge. When knowledge is rare or never appears in the training data such as implicit knowledge, it is not learned by our models. To fill these gaps, we need to define the set of human goals that we care about and schemas that capture what was not said or what is going to be said. This requires evolving schema learning to a set of inter-related schemas such as schemas of patient, epidemiologist, and pathogen in the context of a pandemic. Similarly, to capture roles of people in groups, we need human definitions and guidance. Overall, he encouraged the community to put thought into building topologies that can be learned by models.</p><p>Finally, <a href="https://scholar.google.com/citations?user=nTl5mSwAAAAJ&amp;hl=en">Hang Li</a> emphasized the need for symbolic reasoning. He suggested a neuro-symbolic architecture for NLU that combines analogical reasoning via a pre-trained model and logical reasoning via symbolic components.</p><p>In addition to the Next Big Ideas session, the conference also featured <a href="https://www.2022.aclweb.org/invited-talks">spotlight talks by early-career researchers</a>. I had the honor to speak next to amazing young researchers such as <a href="https://www.cs.utexas.edu/~eunsol/">Eunsol Choi</a>, <a href="https://faculty.cc.gatech.edu/~dyang888/">Diyi Yang</a>, <a href="https://rycolab.io/authors/ryan/">Ryan Cotterell</a>, and <a href="https://swabhs.com/">Swabha Swayamdipta</a>. I hope that future conferences will continue with these formats and experiment with others as they bring a fresh perspective and enable a broader view of research.</p><h2 id="favorite-papers">Favorite Papers</h2><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2022/06/tacl_a_00478_f001.png" class="kg-image" alt="ACL 2022 Highlights"><figcaption>Example narratives containing an idiom (top) or a simile (bottom), along with human-written plausible and implausible continuations (<a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00478/111221/It-s-not-Rocket-Science-Interpreting-Figurative">Chakrabarty et al.</a>).</figcaption></figure><p>Finally, here are some of my favorite papers on less mainstream topics. Quite a few of these are in <a href="https://transacl.org/index.php/tacl">TACL</a>, highlighting the usefulness of TACL as a venue for publishing nuanced research. Many of these also emphasize the human side of NLP:</p><ul><li><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00478/111221/It-s-not-Rocket-Science-Interpreting-Figurative">It’s not Rocket Science: Interpreting Figurative Language in Narratives</a> (Chakrabarty et al.). This paper focuses on understanding figurative language (idioms and similes). They evaluate whether models can interpret such figurative expressions by framing the task in an LM setting: they generate plausible and implausible continuations using crowd workers, which rely on the correct interpretation of the expression. They find that state-of-the-art models perform poorly on this task but performance can be improved by providing additional context or knowledge-based inferences (e.g., “The narrator sweats from nerves”, “Run is used for exercise”) as input.</li><li><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00444/109271/Word-Acquisition-in-Neural-Language-Models">Word Acquisition in Neural Language Models</a> (Chang &amp; Bergen). This paper investigates when individual words are acquired during training in neural models, compared to word acquisition in humans. They find that LMs rely far more on word frequency than children (who rely more on interaction and sensorimotor experience). Like children, LMs also exhibit slower learning of words in longer utterances. Early in training, models predict based on unigram token frequencies, later on bigram probabilities, and eventually converge to more nuanced predictions.</li><li><a href="https://aclanthology.org/2022.acl-long.601/">The Moral Debater: A Study on the Computational Generation of Morally Framed Arguments</a> (Alshomary et al.). This paper studies the automatic generation of morally framed arguments. The system takes a controversial topic (e.g., globalization), a stance (e.g., pro), and a set of morals (e.g., loyalty, authority, and purity) as input, retrieves and filters relevant texts based on the morals, and phrases an argument. They employ a Reddit dataset annotated with aspects (e.g., respect, obedience), which they map to morals. They evaluate the effectiveness of morally framed arguments in a user study with liberals and conservatives.</li><li><a href="https://aclanthology.org/2022.findings-acl.52/">Human Language Modeling</a> (Soni et al.). This paper extends the language modeling task with a dependence on a human state in which the text was generated. To this end, they process all utterances of a user sequentially and condition a Transformer’s self-attention on a recurrently computed user state. The model is pre-trained on Facebook posts and tweets with user information where it achieves a dramatic reduction in perplexity and improvements on downstream stance and sentiment tasks.</li><li><a href="https://aclanthology.org/2022.acl-long.257/">Inducing Positive Perspectives with Text Reframing</a> (Ziems et al.). This paper introduces the task of positive reframing, which neutralizes a negative point of view and generates a more positive perspective without contradicting the meaning (e.g., “I absolutely hate making decisions” → “It’ll become easier once I start to get used to it”). They create a new dataset for this task, annotated with different reframing strategies. Overall, the task is an interesting and challenging application of style transfer—and who is not in need of positive reframing sometimes?</li></ul><h2 id="the-dark-matter-of-language-and-intelligence">The Dark Matter of Language and Intelligence</h2><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2022/06/yejin_choi_keynote.png" class="kg-image" alt="ACL 2022 Highlights"><figcaption>Yejin Choi speculates on what ACL 2082 may be like, highlighting possible applications for contemporary research areas.</figcaption></figure><p>Yejin Choi gave an inspiring keynote. Among other things, it is the first talk I have seen that uses <a href="https://openai.com/dall-e-2/">DALL-E 2</a> for illustrating slides. She highlighted three important research areas in NLP by drawing analogies to physics: ambiguity, reasoning, and implicit information.</p><p>In modern physics, a greater understanding often leads to increased ambiguity (see, for instance, <a href="https://en.wikipedia.org/wiki/Schr%C3%B6dinger%27s_cat">Schrödinger’s cat</a> or the <a href="https://en.wikipedia.org/wiki/Wave%E2%80%93particle_duality">wave–particle duality</a>). Yejin similarly encouraged the ACL community to embrace ambiguity. In the past, there was pressure not to work on tasks that did not achieve high inter-annotator agreement; similarly, in traditional sentiment analysis, the neutral class is often discarded. Understanding cannot just be crammed into simple categories. Annotator opinions bias language models (<a href="https://arxiv.org/abs/2111.07997">Sap et al., 2021</a>) and ambiguous examples improve generalization (<a href="https://arxiv.org/abs/2009.10795">Swayamdipta et al., 2020</a>).</p><p>Similar in spirit to the notion of <a href="https://en.wikipedia.org/wiki/Spacetime">spacetime</a>, Yejin argued that language, knowledge, and reasoning are also not separate areas but exist on a continuum. Reasoning methods such as maieutic prompting (<a href="https://arxiv.org/abs/2205.11822">Jung et al., 2022</a>) allow us to investigate the continuum of a model’s knowledge by recursively generating explanations.</p><p>Finally, analogous to the central role of dark matter in modern physics, future research in NLP should focus on the “dark matter” of language, the unspoken rules of how the world works, which influence the way people use language. We should aspire to try to teach our models such as tacit rules, values, and objectives (<a href="https://arxiv.org/abs/2110.07574">Jiang et al., 2021</a>).</p><p>Yejin concluded her talk with a candid take of factors that led to her success: being humble, learning from others, taking risks; but also being lucky and working in an inclusive environment.</p><h2 id="hybrid-conference-experience">Hybrid conference experience</h2><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="http://ruder.io/content/images/2022/06/PXL_20220522_120522679.MP.jpg" width="4032" height="3024" alt="ACL 2022 Highlights"></div><div class="kg-gallery-image"><img src="http://ruder.io/content/images/2022/06/PXL_20220524_092733061.MP.jpg" width="4032" height="3024" alt="ACL 2022 Highlights"></div></div></div><figcaption>The Convention Centre Dublin, the venue of ACL 2022 (right) and the view of Samuel Beckett Bridge from the venue (right).&nbsp;</figcaption></figure><p>I personally really enjoyed the in-person conference experience. There was a <a href="https://www.2022.aclweb.org/covid-19-safety">strict mask wearing requirement</a>, which everyone adhered to and which did not otherwise impede the flow of the conference. The only issues were some technical problems that occurred during plenary and keynote talks.</p><p>On the other hand, I found it difficult to reconcile the in-person with the virtual conference experience. Virtual poster sessions overlapped with breakfast or dinner, which made attending them difficult. From what I have heard, many virtual poster sessions were almost empty. It seems we need to rethink how to conduct virtual poster sessions in a hybrid setting. As an alternative, it may be more effective to create asynchronous per-poster chat rooms in <a href="https://rocket.chat/">rocket.chat</a> or a similar platform, with the ability to set up impromptu video calls for deeper conversations.</p><p>The experience was better for oral presentations and workshops, which had a reasonable number of virtual participants. I particularly appreciate being able to rewatch the recordings of keynotes and other invited talks.</p><p>Overall, we still have some way to go to create a great experience for virtual participants. However, it was a blast being able to interact with people in-person again. Thanks to all the organizers, volunteers, and to the community for putting together a great conference!</p>]]></content:encoded></item><item><title><![CDATA[ML and NLP Research Highlights of 2021]]></title><description><![CDATA[This post summarizes progress across multiple impactful areas in ML and NLP in 2021.]]></description><link>http://ruder.io/ml-highlights-2021/</link><guid isPermaLink="false">61ced3718761d5340fe1429d</guid><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Mon, 24 Jan 2022 18:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2022/01/prompt_survey_header.png" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="http://ruder.io/content/images/2022/01/prompt_survey_header.png" alt="ML and NLP Research Highlights of 2021"><p>Credit for the title image: <a href="https://arxiv.org/abs/2107.13586">Liu et al. (2021)</a></p>
<p>2021 saw many exciting advances in machine learning (ML) and natural language processing (NLP). In this post, I will cover the papers and research areas that I found most inspiring. I tried to cover the papers that I was aware of but likely missed many relevant ones. Feel free to highlight them as well as ones that you found inspiring in the comments. I discuss the following highlights:</p>
<ol>
<li><a href="#1universalmodels">Universal Models</a></li>
<li><a href="#2massivemultitasklearning">Massive Multi-task Learning</a></li>
<li><a href="#3beyondthetransformer">Beyond the Transformer</a></li>
<li><a href="#4prompting">Prompting</a></li>
<li><a href="#5efficientmethods">Efficient Methods</a></li>
<li><a href="#6benchmarking">Benchmarking</a></li>
<li><a href="#7conditionalimagegeneration">Conditional Image Generation</a></li>
<li><a href="#8mlforscience">ML for Science</a></li>
<li><a href="#9programsynthesis">Program Synthesis</a></li>
<li><a href="#10bias">Bias</a></li>
<li><a href="#11retrievalaugmentation">Retrieval Augmentation</a></li>
<li><a href="#12tokenfreemodels">Token-free Models</a></li>
<li><a href="#13temporaladaptation">Temporal Adaptation</a></li>
<li><a href="#14theimportanceofdata">The Importance of Data</a></li>
<li><a href="#15metalearning">Meta-learning</a></li>
</ol>
<h1 id="1universalmodels">1) Universal Models</h1>
<figure>
      <img src="http://ruder.io/content/images/2022/01/xslr.png" style="width: 100%" title="Self-supervised speech representation learning" alt="ML and NLP Research Highlights of 2021">
<figcaption>Self-supervised cross-lingual representation learning on speech using XLS-R. The model is pre-trained on diverse multilingual speech data using a self-supervised wav2vec 2.0-style loss. The trained model can then be fine-tuned on different speech tasks <a href="https://arxiv.org/abs/2111.09296">(Babu et al., 2021)</a>.</figcaption>
</figure>
<p><strong>What happened?</strong>   2021 saw the continuation of the development of ever larger pre-trained models. Pre-trained models were applied in many different domains and started to be considered critical for ML research <sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>. In computer vision, supervised pre-trained models such as Vision Transformer <sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> have been scaled up <sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup> and self-supervised pre-trained models have started to match their performance <sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>. The latter have been scaled beyond the controlled environment of ImageNet to random collections of images <sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>. In speech, new models have been built based on wav2vec 2.0 <sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup> such as W2v-BERT <sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup> as well as more powerful multilingual models such as XLS-R <sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>. At the same time, we saw new unified pre-trained models for previously under-researched modality pairs such as for videos and language <sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup> as well as speech and language <sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup>. In vision and language, controlled studies shed new light on important components of such multi-modal models <sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup><sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup>. By framing different tasks in the paradigm of  language modelling, models have had great success also in other domains such as reinforcement learning <sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup> and protein structure prediction <sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup>. Given the observed scaling behaviour of many of these models, it has become common to report performance at different parameter sizes. However, increases in pre-training performance do not necessarily translate to downstream settings <sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup><sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup>.</p>
<p><strong>Why is it important?</strong>   Pre-trained models have been shown to generalize well to new tasks in a given domain or modality. They demonstrate strong few-shot learning behaviour and robust learning capabilities. As such, they are a valuable building block for research advances and enable new practical applications.</p>
<p><strong>What’s next?</strong>   We will undoubtedly see more and even larger pre-trained models developed in the future. At the same time, we should expect individual models to perform more tasks at the same time. This is already the case in language where models can perform many tasks by framing them in a common text-to-text format. Similarly, we will likely see image and speech models that can perform many common tasks in a single model. Finally, we will see more work that trains models for multiple modalities.</p>
<h1 id="2massivemultitasklearning">2) Massive Multi-task Learning</h1>
<figure>
      <img src="http://ruder.io/content/images/2022/01/ext5.png" style="width: 100%" title="Massive multi-task learning" alt="ML and NLP Research Highlights of 2021">
<figcaption>Massive multi-task learning with ExT5. During pre-training, the model is trained on the inputs (left) of a diverse set of different tasks in a text-to-text format to produce the corresponding outputs (right). The tasks include masked language modeling, summarization, semantic parsing, closed-book question answering, style transfer, dialogue modeling, natural language inference, Winograd-schema style coreference resolution (top to bottom), among others <a href="https://arxiv.org/abs/2111.10952">(Aribandi et al., 2021)</a>.</figcaption>
</figure>
<p><strong>What happened?</strong>   Most pre-trained models in the previous section are self-supervised. They generally learn from large amounts of unlabelled data via an objective that does not require explicit supervision. However, for many domains large amounts of labelled data are already available, which can be used to learn better representations. So far, multi-task models such as T0 <sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup>, FLAN <sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup>, and ExT5 <sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup> have been pre-trained on around 100 tasks mainly for language. Such massive multi-task learning is closely related to meta-learning. Given access to a diverse task distribution <sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup>, models can <em>learn</em> to learn different types of behaviour such as how to do in-context learning <sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup>.</p>
<p><strong>Why is it important?</strong>   Massive multi-task learning is possible due to the fact that many recent models such as T5 and GPT-3 use a text-to-text format. Models thus no longer require hand-engineered task-specific loss functions or task-specific layers in order to effectively learn across multiple tasks. Such recent approaches highlight the benefit of combining self-supervised pre-training with supervised multi-task learning and demonstrate that a combination of both leads to models that are more general.</p>
<p><strong>What’s next?</strong>   Given the availability and open-source nature of datasets in a unified format, we can imagine a virtuous cycle where newly created high-quality datasets are used to train more powerful models on increasingly diverse task collections, which could then be used in-the-loop to create more challenging datasets.</p>
<h1 id="3beyondthetransformer">3) Beyond the Transformer</h1>
<figure>
      <img src="http://ruder.io/content/images/2022/01/perceiver.png" style="width: 100%" title="Perceiver" alt="ML and NLP Research Highlights of 2021">
<figcaption>The Perceiver projects a high-dimensional input byte array via cross-attention to a fixed-dimensional latent array, which it processes with a transformer self-attention block. Cross-attention and self-attention blocks are then applied alternatingly <a href="https://arxiv.org/abs/2103.03206">(Jaegle et al., 2021)</a>.</figcaption>
</figure>
<p><strong>What happened?</strong>   Most pre-trained models discussed in the previous sections build on the transformer architecture <sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup>. 2021 saw the development of alternative model architectures that are viable alternatives to the transformer. The Perceiver <sup class="footnote-ref"><a href="#fn23" id="fnref23">[23]</a></sup> is a transformer-like architecture that scales to very high-dimensional inputs by using a latent array of a fixed dimensionality as its base representation and conditioning this on the input via cross-attention. Perceiver IO <sup class="footnote-ref"><a href="#fn24" id="fnref24">[24]</a></sup> extended the architecture to also deal with structured output spaces. Other models have tried to replace the ubiquituous self-attention layer, most notably using multilayer perceptrons (MLPs) such as in the MLP-Mixer <sup class="footnote-ref"><a href="#fn25" id="fnref25">[25]</a></sup> and gMLP <sup class="footnote-ref"><a href="#fn26" id="fnref26">[26]</a></sup>. Alternatively, FNet <sup class="footnote-ref"><a href="#fn27" id="fnref27">[27]</a></sup> uses 1D Fourier Transforms instead of self-attention to mix information at the token level. In general, it is useful to think of an architecture as decoupled from the pre-training strategy. If CNNs are pre-trained the same way as transformer models, they achieve competitive performance on many NLP tasks <sup class="footnote-ref"><a href="#fn28" id="fnref28">[28]</a></sup>. Similarly, using alternative pre-training objectives such as ELECTRA-style pre-training <sup class="footnote-ref"><a href="#fn29" id="fnref29">[29]</a></sup> may lead to gains <sup class="footnote-ref"><a href="#fn30" id="fnref30">[30]</a></sup>.</p>
<p><strong>Why is it important?</strong>   Research progresses by exploring many complementary or orthogonal directions at the same time. If most research focuses on a single architecture, this will inevitably lead to bias, blind spots, and missed opportunities. New models may address some of the transformers' limitations such as the computational complexity of attention, its black-box nature, and order-agnosticity. For instance, neural extensions of generalized additive models offer much better interpretability compared to current models <sup class="footnote-ref"><a href="#fn31" id="fnref31">[31]</a></sup>.</p>
<p><strong>What’s next?</strong>   While pre-trained transformers will likely continue to be deployed as standard baselines for many tasks, we should expect to see alternative architectures particularly in settings where current models fail short, such as modeling long-range dependencies and high-dimensional inputs or where interpretability and explainability are required.</p>
<h1 id="4prompting">4) Prompting</h1>
<figure>
      <img src="http://ruder.io/content/images/2022/01/p3.png" style="width: 100%" title="Prompting" alt="ML and NLP Research Highlights of 2021">
<figcaption>Prompt templates from the P3 prompt collection. Multiple templates are possible for each task. Each template consists of an input pattern and a target verbalizer. For paraphrasing, <i>Choices</i> consist of {Not duplicates, Duplicates} and {Yes, No} in the first and second template respectively <a href="https://arxiv.org/abs/2110.08207">(Sanh et al., 2021)</a>.</figcaption>
</figure>
<p><strong>What happened?</strong>   Popularized by GPT-3 <sup class="footnote-ref"><a href="#fn32" id="fnref32">[32]</a></sup>, prompting has emerged as a viable alternative input format for NLP models. Prompts typically include a <em>pattern</em> that asks the model to make a certain prediction and a <em>verbalizer</em> that converts the prediction to a class label. Several approaches such as PET, <sup class="footnote-ref"><a href="#fn33" id="fnref33">[33]</a></sup> iPET <sup class="footnote-ref"><a href="#fn34" id="fnref34">[34]</a></sup>, and AdaPET <sup class="footnote-ref"><a href="#fn35" id="fnref35">[35]</a></sup> leverage prompts for few-shot learning. Prompts are not a silver bullet, however. Models' performance varies drastically depending on the prompt and finding the best prompt still requires labeled examples <sup class="footnote-ref"><a href="#fn36" id="fnref36">[36]</a></sup>. In order to compare models reliably in a few-shot setting, new evaluation procedures have been developed <sup class="footnote-ref"><a href="#fn37" id="fnref37">[37]</a></sup>. A large number of prompts are available as part of the <a href="https://github.com/bigscience-workshop/promptsource">public pool of prompts (P3)</a>, enabling exploration of the best way to use prompts. This survey <sup class="footnote-ref"><a href="#fn38" id="fnref38">[38]</a></sup> provides an excellent overview of the general research area.</p>
<p><strong>Why is it important?</strong>   A prompt can be used to encode task-specific information, which can be worth up to 3,500 labeled examples, depending on the task <sup class="footnote-ref"><a href="#fn39" id="fnref39">[39]</a></sup>. Prompts thus an enable a new way to incorporate expert information into model training, beyond manually labeling examples or defining labeling functions <sup class="footnote-ref"><a href="#fn40" id="fnref40">[40]</a></sup>.</p>
<p><strong>What’s next?</strong>   We have only scratched the surface of using prompts to improve model learning. Prompts will become more elaborate, for instance including longer instructions <sup class="footnote-ref"><a href="#fn18" id="fnref18:1">[18:1]</a></sup> as well as positive and negative examples <sup class="footnote-ref"><a href="#fn41" id="fnref41">[41]</a></sup> and general heuristics. Prompts may also be a more natural way to incorporate natural language explanations <sup class="footnote-ref"><a href="#fn42" id="fnref42">[42]</a></sup> into model training.</p>
<h1 id="5efficientmethods">5) Efficient Methods</h1>
<figure>
      <img src="http://ruder.io/content/images/2022/01/magma.png" style="width: 100%" title="MAGMA" alt="ML and NLP Research Highlights of 2021">
<figcaption>Multi-modal adaptation with MAGMA. A frozen pre-trained language model is adapted to multi-modal tasks using a visual prefix learned via an image encoder as well as vision-specific adpater layers <a href="https://arxiv.org/abs/2112.05253">(Eichenberg et al., 2021)</a>.</figcaption>
</figure>
<p><strong>What happened?</strong>   A downside of pre-trained models is that they are generally very large and often inefficient to use in practice. 2021 brought advances both in more efficient architectures as well as in more efficient fine-tuning methods. On the modeling side, we saw several more efficient versions of self-attention <sup class="footnote-ref"><a href="#fn43" id="fnref43">[43]</a></sup><sup class="footnote-ref"><a href="#fn44" id="fnref44">[44]</a></sup>. This survey <sup class="footnote-ref"><a href="#fn45" id="fnref45">[45]</a></sup> provides an overview of pre-2021 models. Current pre-trained models are so powerful that they can be effectively conditioned by only updating few parameters, which has led to the development of more efficient fine-tuning approaches based on continuous prompts <sup class="footnote-ref"><a href="#fn46" id="fnref46">[46]</a></sup><sup class="footnote-ref"><a href="#fn47" id="fnref47">[47]</a></sup> and adapters <sup class="footnote-ref"><a href="#fn48" id="fnref48">[48]</a></sup><sup class="footnote-ref"><a href="#fn49" id="fnref49">[49]</a></sup><sup class="footnote-ref"><a href="#fn50" id="fnref50">[50]</a></sup>, among others. This capability also enables adaptation to new modalities by learning an appropriate prefix <sup class="footnote-ref"><a href="#fn51" id="fnref51">[51]</a></sup> or suitable transformations <sup class="footnote-ref"><a href="#fn52" id="fnref52">[52]</a></sup><sup class="footnote-ref"><a href="#fn53" id="fnref53">[53]</a></sup>. Other methods such as quantization for creating more efficient optimizers <sup class="footnote-ref"><a href="#fn54" id="fnref54">[54]</a></sup> as well as sparsity have also been used.</p>
<p><strong>Why is it important?</strong>   Models are not useful if they are infeasible or prohibitively expensive to run on standard hardware. Advances in efficiency will ensure that while models are growing larger, they will be benefical and accessible to practicioners.</p>
<p><strong>What’s next?</strong>   Efficient models and training methods should become easier to use and more accessible. At the same time, the community will develop more effective ways to interface with large models and to efficiently adapt, combine or modify them without having to pre-train a new model from scratch.</p>
<h1 id="6benchmarking">6) Benchmarking</h1>
<figure>
      <img src="http://ruder.io/content/images/2022/01/dataset_concentration.png" style="width: 100%" title="Benchmark saturation of popular ML benchmarks" alt="ML and NLP Research Highlights of 2021">
<figcaption>Increases in concentration of dataset usage on institutions and datasets over time. Map of dataset usages per institution (left). Over 50% of dataset usages can be attributed to 12 institutions. The concentration of dataset usage on institutions and specific datasets as measured by the Gini coefficient has increased in recent years (right) <a href="https://openreview.net/forum?id=zNQBIBKJRkd">(Koch et al., 2021)</a>.</figcaption>
</figure>
<p><strong>What happened?</strong>   The rapidly improving capabilities of recent ML and NLP models have outpaced the ability of many benchmarks to measure them. At the same time, communities evaluate on fewer and fewer benchmarks, which originate from a small number of elite institutions <sup class="footnote-ref"><a href="#fn55" id="fnref55">[55]</a></sup>. Consequently, 2021 saw much discussion of best practices and ways in which we can reliably evaluate such models going forward, which I cover in <a href="https://ruder.io/nlp-benchmarking/">this blog post</a>. Notable leaderboard paradigms that emerged in 2021 in the NLP community are dynamic adversarial evaluation <sup class="footnote-ref"><a href="#fn56" id="fnref56">[56]</a></sup>, community-driven evaluation where community members collaborate on creating evaluation datasets such as <a href="https://github.com/google/BIG-bench">BIG-bench</a>, interactive fine-grained evaluation across different error types <sup class="footnote-ref"><a href="#fn57" id="fnref57">[57]</a></sup>, and multi-dimensional evaluation that goes beyond evaluating models on a single performance metric <sup class="footnote-ref"><a href="#fn58" id="fnref58">[58]</a></sup>. In addition, new benchmarks were proposed for influential settings such as few-shot evaluation <sup class="footnote-ref"><a href="#fn59" id="fnref59">[59]</a></sup><sup class="footnote-ref"><a href="#fn60" id="fnref60">[60]</a></sup> and cross-domain generalization <sup class="footnote-ref"><a href="#fn61" id="fnref61">[61]</a></sup>. We also saw new benchmarks focused on evaluating general-purpose pre-trained models, for specific modalities such as speech <sup class="footnote-ref"><a href="#fn62" id="fnref62">[62]</a></sup> and specific languages, for instance, Indonesian and Romanian <sup class="footnote-ref"><a href="#fn63" id="fnref63">[63]</a></sup><sup class="footnote-ref"><a href="#fn64" id="fnref64">[64]</a></sup>, as well as across modalities <sup class="footnote-ref"><a href="#fn65" id="fnref65">[65]</a></sup> and in a multilingual setting <sup class="footnote-ref"><a href="#fn66" id="fnref66">[66]</a></sup>. We also should pay more attention to evaluation metrics. A machine translation (MT) meta-evaluation <sup class="footnote-ref"><a href="#fn67" id="fnref67">[67]</a></sup> revealed that among 769 MT papers of the last decade, 74.3% only used BLEU, despite 108 alternative metrics—often with better human correlation—having been proposed. Recent efforts such as GEM <sup class="footnote-ref"><a href="#fn68" id="fnref68">[68]</a></sup> and bidimensional leaderboards <sup class="footnote-ref"><a href="#fn69" id="fnref69">[69]</a></sup> thus propose to evaluate models and methods jointly.</p>
<p><strong>Why is it important?</strong>   Benchmarking and evaluation are the linchpins of scientific progress in machine learning and NLP. Without accurate and reliable benchmarks, it is not possible to tell whether we are making genuine progress or overfitting to entrenched datasets and metrics.</p>
<p><strong>What’s next?</strong>   Increased awareness around issues with benchmarking should lead to a more thoughful design of new datasets. Evaluation of new models should also focus less on a single performance metric but take multiple dimensions into account, such as a model's fairness, efficiency, and robustness.</p>
<h1 id="7conditionalimagegeneration">7) Conditional image generation</h1>
<figure>
      <img src="http://ruder.io/content/images/2022/01/clip_generation.gif" style="width: 100%" title="CLIP art generation" alt="ML and NLP Research Highlights of 2021">
<figcaption>How CLIP Generates Art. A generative model generates an image based on a latent vector. The latent vector is then updated based on the similarity of CLIP's embeddings of the generated image and the text description. This process is repeated until convergence (Credit: <a href="https://ml.berkeley.edu/blog/posts/clip-art/">Charlie Snell</a>).</figcaption>
</figure>
<p><strong>What happened?</strong>   Conditional image generation, i.e., generating images based on a text description, saw impressive results in 2021. An art scene emerged around the most recent generation of generative models (see <a href="https://ml.berkeley.edu/blog/posts/clip-art/">this blog post</a> for an overview). Rather than generating an image directly based on a text input as in the DALL-E model <sup class="footnote-ref"><a href="#fn70" id="fnref70">[70]</a></sup>, recent approaches steer the output of a powerful generative model such as VQ-GAN <sup class="footnote-ref"><a href="#fn71" id="fnref71">[71]</a></sup> using a joint image-and-text embedding model such as CLIP <sup class="footnote-ref"><a href="#fn72" id="fnref72">[72]</a></sup>. Likelihood-based diffusion models, which gradually remove noise from a signal have emerged as powerful new generative models that can outperform GANs <sup class="footnote-ref"><a href="#fn73" id="fnref73">[73]</a></sup>. By guiding their outputs based on text inputs, recent models are approaching photorealistic image quality <sup class="footnote-ref"><a href="#fn74" id="fnref74">[74]</a></sup>. Such models are also particularly good at inpainting and can modify regions of an image based on a description.</p>
<p><strong>Why is it important?</strong>   Automatic generation of high quality images that can be guided by users opens a wide range of artistic and commercial applications, from the automatic design of visual assets, model-assisted prototyping and design, personalization, etc.</p>
<p><strong>What’s next?</strong>   Sampling from recent diffusion-based models is much slower compared to their GAN-based counterparts. These models require improvements in efficiency to make them useful for real-world applications. This area also requires more research in human-computer interaction, to identify the best ways and applications where such models can assist humans.</p>
<h1 id="8mlforscience">8) ML for Science</h1>
<figure>
      <img src="http://ruder.io/content/images/2022/01/alphafold_2.0.png" style="width: 100%" title="AlphaFold 2.0" alt="ML and NLP Research Highlights of 2021">
<figcaption> The architecture of AlphaFold 2.0. The model attends over evolutionarily related protein sequences as well as amino acid residue pairs and iteratively passes information between both representations (Credit: <a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">DeepMind</a>).</figcaption>
</figure>
<p><strong>What happened?</strong>   2021 saw several breakthroughs in ML applied to advance the natural sciences. In meteorology, advances in precipitation nowcasting and forecasting <sup class="footnote-ref"><a href="#fn75" id="fnref75">[75]</a></sup><sup class="footnote-ref"><a href="#fn76" id="fnref76">[76]</a></sup> led to substantial improvements in forecast accuracy. In both cases, models outperformed state-of-the-art physics-based forecast models. In biology, AlphaFold 2.0 managed to predict the structure of proteins with unprecedented accuracy, even in cases where no similar structure is known <sup class="footnote-ref"><a href="#fn14" id="fnref14:1">[14:1]</a></sup>. In mathematics, ML was shown to be able to guide the intuition of mathematicians in order to discover new connections and algorithms <sup class="footnote-ref"><a href="#fn77" id="fnref77">[77]</a></sup>. Transformer models have also been shown to be capable of learning mathematical properties of differential systems such as local stability when trained on sufficient amounts of data <sup class="footnote-ref"><a href="#fn78" id="fnref78">[78]</a></sup>.</p>
<p><strong>Why is it important?</strong>   Using ML for advancing our understanding and applications in natural sciences is one of its most impactful applications. Using powerful ML methods enables both new applications and can greatly speed up existing ones such as drug design.</p>
<p><strong>What’s next?</strong>   Using models in-the-loop to assist researchers in the discovery and development of new advances is a particularly compelling direction. It requires both the development of powerful models as well as work on interactive machine learning and human-computer interaction.</p>
<h1 id="9programsynthesis">9) Program synthesis</h1>
<figure>
      <img src="http://ruder.io/content/images/2022/01/dobf.png" style="width: 100%" title="DOBF" alt="ML and NLP Research Highlights of 2021">
<figcaption> Comparison of masked language modeling (ML) and deobfuscation (DOBF) pre-training objectives for modeling code. MLM predicts randomly masked tokens, which mainly relate to a programming language's syntax. DOBF requires deobfuscating the names of functions and variables, which is much more challenging (<a href="https://openreview.net/forum?id=3ez9BSHTNT">Roziere et al., 2021</a>).</figcaption>
</figure>
<p><strong>What happened?</strong>   One of the most notable applications of large language models this year was code generation, which saw with Codex <sup class="footnote-ref"><a href="#fn79" id="fnref79">[79]</a></sup> its first integration into a major product as part of <a href="https://copilot.github.com/">GitHub Copilot</a>. Other advances in pre-training models ranged from better pre-training objectives <sup class="footnote-ref"><a href="#fn80" id="fnref80">[80]</a></sup><sup class="footnote-ref"><a href="#fn81" id="fnref81">[81]</a></sup> to scaling experiments <sup class="footnote-ref"><a href="#fn82" id="fnref82">[82]</a></sup><sup class="footnote-ref"><a href="#fn83" id="fnref83">[83]</a></sup>. Generating complex and long-form programs is still a challenge for current models, however. An interesting related direction is learning to execute or model programs, which can be improved by performing multi-step computation where intermediate computation steps are recorded in a &quot;scratchpad&quot; <sup class="footnote-ref"><a href="#fn84" id="fnref84">[84]</a></sup>.</p>
<p><strong>Why is it important?</strong>   Being able to automatically synthesize complex programs is useful for a wide variety of applications such as supporting software engineers.</p>
<p><strong>What’s next?</strong>   It is still an open question how much code generation models improve the workflow of software engineers in practice <sup class="footnote-ref"><a href="#fn85" id="fnref85">[85]</a></sup>. In order to be truly helpful, such models—similarly to dialogue models—need to be able to update their predictions based on new information and need to take the local and global context into account.</p>
<h1 id="10bias">10) Bias</h1>
<figure>
      <img src="http://ruder.io/content/images/2022/01/toxicity_reduction.png" style="width: 100%" title="Side-effects of toxicity mitigation" alt="ML and NLP Research Highlights of 2021">
<figcaption> Unintended side-effects of automatic toxicity mitigation. Over-filtering of text about marginalized groups reduces the ability of language models to produce (even positive) text about said groups (<a href="https://aclanthology.org/2021.findings-emnlp.210/">Welbl et al., 2021</a>).</figcaption>
</figure>
<p><strong>What happened?</strong>   Given the potential impact of large pre-trained models, it is crucial that they do not contain harmful biases, are not misused to generate harmful content, and are used in a sustainable manner. Several reviews <sup class="footnote-ref"><a href="#fn1" id="fnref1:1">[1:1]</a></sup><sup class="footnote-ref"><a href="#fn86" id="fnref86">[86]</a></sup><sup class="footnote-ref"><a href="#fn87" id="fnref87">[87]</a></sup> highlight the potential risks of such models. Bias has been investigated with regard to protected attributes such as gender, particular ethnic groups, and political leaning <sup class="footnote-ref"><a href="#fn88" id="fnref88">[88]</a></sup><sup class="footnote-ref"><a href="#fn89" id="fnref89">[89]</a></sup>. Removing bias from models such as toxicity, however, comes with trade-offs and can lead to reduced coverage for texts about and authored by marginalized groups <sup class="footnote-ref"><a href="#fn90" id="fnref90">[90]</a></sup>.</p>
<p><strong>Why is it important?</strong>   In order to use models in real-world applications, they should not exhibit any harmful bias and not discriminate against any group. Developing a better understanding of the biases of current models and how to remove them is thus crucial for enabling safe and responsible deployment of ML models.</p>
<p><strong>What’s next?</strong>   Bias has so far been mostly explored in English and in pre-trained models and for specific text generation or classification applications. Given the intended use and lifecycle of such models, we should also aim to identify and mitigate bias in a multilingual setting, with regard to the combination of different modalities, and at different stages of a pre-trained model's usage—after pre-training, after fine-tuning, and at test time.</p>
<h1 id="11retrievalaugmentation">11) Retrieval Augmentation</h1>
<figure>
      <img src="http://ruder.io/content/images/2022/01/retro.png" style="width: 100%" title="RETRO" alt="ML and NLP Research Highlights of 2021">
<figcaption> Overview of the RETRO architecture. An input sequence is split into multiple chunks (left). For each input chunk, nearest neighbor chunks are retrieved using approximate nearest neighbor search based on BERT embedding similarity. The model attends to the nearest neighbors using chunked cross-attention (right) interleaved with standard transformer layers  (<a href="https://arxiv.org/abs/2112.04426">Borgeaud et al., 2021</a>).</figcaption>
</figure>
<p><strong>What happened?</strong>   Retrieval-augmented language models, which integrate retrieval into pre-training and downstream usage, have already featured in my <a href="https://ruder.io/research-highlights-2020/#2-retrieval-augmentation">highlights of 2020</a>. In 2021, retrieval corpora have been scaled up to a trillion tokens <sup class="footnote-ref"><a href="#fn91" id="fnref91">[91]</a></sup> and models have been equipped with the ability to query the web for answering questions <sup class="footnote-ref"><a href="#fn92" id="fnref92">[92]</a></sup><sup class="footnote-ref"><a href="#fn93" id="fnref93">[93]</a></sup>. We have also seen new ways to integrate retrieval into pre-trained language models <sup class="footnote-ref"><a href="#fn94" id="fnref94">[94]</a></sup><sup class="footnote-ref"><a href="#fn95" id="fnref95">[95]</a></sup>.</p>
<p><strong>Why is it important?</strong>   Retrieval augmentation enables models to be much more parameter-efficient as they need to store less knowledge in their parameters and can instead retrieve it. It also enables effective domain adaptation by simply updating the data used for retrieval <sup class="footnote-ref"><a href="#fn96" id="fnref96">[96]</a></sup>.</p>
<p><strong>What’s next?</strong>   We might see different forms of retrieval to leverage different kinds of information such as common sense knowledge, factual relations, linguistic information, etc. Retrieval augmentation could also be combined with more structured forms of knowledge retrieval, such as methods from knowledge base population and open information extraction.</p>
<h1 id="12tokenfreemodels">12) Token-free Models</h1>
<figure>
      <img src="http://ruder.io/content/images/2022/01/charformer.png" style="width: 100%" title="Charformer" alt="ML and NLP Research Highlights of 2021">
<figcaption> Subword block formation and scoring in Charformer. Subwords are formed based on contiguous n-gram sequences (a), which are scored by a separate scoring network. Block scores are then replicated over their original positions (b). Finally, subwords at each position are summed, weighted based on their block scores to form latent subwords (<a href="https://arxiv.org/abs/2106.12672">Tay et al., 2021</a>).</figcaption>
</figure>
<p><strong>What happened?</strong>   2021 saw the emergence of new token-free methods that directly consume a sequence of characters <sup class="footnote-ref"><a href="#fn97" id="fnref97">[97]</a></sup><sup class="footnote-ref"><a href="#fn98" id="fnref98">[98]</a></sup><sup class="footnote-ref"><a href="#fn99" id="fnref99">[99]</a></sup>. These models have been demonstrated to outperform multilingual models and perform particularly well on non-standard language. They are thus a promising alternative to the entrenched subword-based transformer models (see <a href="https://newsletter.ruder.io/issues/iclr-2021-outstanding-papers-char-wars-speech-first-nlp-virtual-conference-ideas-483703">this newsletter</a> for a coverage of these 'Char Wars').</p>
<p><strong>Why is it important?</strong>   Since pre-trained language models like BERT, a text consisting of tokenized subwords has become the standard input format in NLP. However, subword tokenization has been shown to perform poorly on noisy input, such as on typos or spelling variations common on social media, and on certain types of morphology. In addition, it imposes a dependence on the tokenization, which can lead to a mismatch when adapting a model to new data.</p>
<p><strong>What’s next?</strong>   Due to their increased flexibility, token-free models are better able to model morphology and may generalize better to new words and language change. It is still unclear, however, how they fare compared to subword-based methods on different types of morphological or word formation processes and what trade-offs these models make.</p>
<h1 id="13temporaladaptation">13) Temporal Adaptation</h1>
<figure>
      <img src="http://ruder.io/content/images/2022/01/temporal_strategies.png" style="width: 100%" title="Temporal strategies" alt="ML and NLP Research Highlights of 2021">
<figcaption> Different training strategies for temporal adaptation with T5. The Uniform model (left) trains on all data without explicit time information. The Yearly setup (middle) trains a separate model for each year while the Temporal model (right) prepends a time prefix to each example (<a href="https://arxiv.org/abs/2106.15110">Dhingra et al., 2021</a>).</figcaption>
</figure>
<p><strong>What happened?</strong>   Models are biased in many ways based on the data that they are trained on. One of these biases that has received increasing attention in 2021 is a bias regarding the timeframe of the data the models have been trained on. Given that language continuously evolves and new terms enter the discourse, models that are trained on outdated data have been shown to generalize comparatively poorly <sup class="footnote-ref"><a href="#fn100" id="fnref100">[100]</a></sup>. When temporal adaptation is useful, however, may depend on the downstream task. For instance, it may be less helpful for tasks where event-driven changes in language use are not relevant for task performance <sup class="footnote-ref"><a href="#fn101" id="fnref101">[101]</a></sup>.</p>
<p><strong>Why is it important?</strong>   Temporal adaptation is particularly important for question answering where answers to a question may change depending on when the question was asked <sup class="footnote-ref"><a href="#fn102" id="fnref102">[102]</a></sup><sup class="footnote-ref"><a href="#fn103" id="fnref103">[103]</a></sup>.</p>
<p><strong>What’s next?</strong>   Developing methods that can adapt to new timeframes requires moving away from the static pre-train–fine-tune setting and requires efficient ways to update the knowledge of pre-trained models. Both <a href="#5efficientmethods">efficient methods</a> as well as <a href="#11retrievalaugmentation">retrieval augmentation</a> are useful in this regard. It also requires developing models for which the input does not exist in a vacuum but is grounded to extra-linguistic context and the real world. For more work on this topic, check out the EvoNLP workshop at EMNLP 2022.</p>
<h1 id="14theimportanceofdata">14) The Importance of Data</h1>
<figure>
      <img src="http://ruder.io/content/images/2022/01/marvl.png" style="width: 100%" title="MaRVL" alt="ML and NLP Research Highlights of 2021">
    <figcaption> An example from MaRVL related to the Swahili concept <i>leso</i> ("handkerchief"), which requires models to identify whether the description in the caption is true or false. The caption (in Swahili) is: <i>Picha moja ina watu kadhaa waliovaa leso na picha nyingine ina leso bila watu.</i> ("One picture contains several people wearing handkerchiefs and another picture has a handkerchief without people."). The label is false (<a href="https://aclanthology.org/2021.emnlp-main.818/">Liu et al., 2021</a>).</figcaption>
</figure>
<p><strong>What happened?</strong>   Data has long been a critical ingredient for ML but is typically overshadowed by advances in modelling. Given the importance of data for scaling up models, however, attention is slowly shifting from model-centric to data-centric approaches. Important topics include how to build and maintain new datasets efficiently and how to ensure data quality (see the <a href="https://datacentricai.org/">Data-centric AI workshop</a> at NeurIPS 2021 for an overview). In particular, large-scale datasets used by pre-trained models came under scrutiny this year including multi-modal datasets <sup class="footnote-ref"><a href="#fn104" id="fnref104">[104]</a></sup> as well as English and multilingual text corpora <sup class="footnote-ref"><a href="#fn105" id="fnref105">[105]</a></sup><sup class="footnote-ref"><a href="#fn106" id="fnref106">[106]</a></sup>. Such an analysis can inform the design of more representative resources such as MaRVL <sup class="footnote-ref"><a href="#fn107" id="fnref107">[107]</a></sup> for multi-modal reasoning.</p>
<p><strong>Why is it important?</strong>   Data is critically important for training large-scale ML models and a key factor in how models acquire new information. As models are scaled up, ensuring data quality at scale becomes more challenging.</p>
<p><strong>What’s next?</strong>   We currently lack best practices and principled methods regarding how to efficiently build datasets for different tasks, reliably ensure data quality, etc. It is also still poorly understood how data interacts with a model's learning and how the data shapes a model's biases. For instance, training data filtering may have negative effects on a language model's coverage of marginalized groups <sup class="footnote-ref"><a href="#fn90" id="fnref90:1">[90:1]</a></sup>.</p>
<h1 id="15metalearning">15) Meta-learning</h1>
<figure>
      <img src="http://ruder.io/content/images/2022/01/universal_template.png" style="width: 100%" title="Universal template" alt="ML and NLP Research Highlights of 2021">
    <figcaption> The training and test setup of the universal template model. A model consisting of shared convolutional weights and dataset-specific FiLM layers is trained in a multi-task setting (left). FiLM parameter values for a test episode are initialized based on a convex combination of the trained sets of FiLM parameters, learned on the training data (right). They are then updated using gradient descent on the support set, with a nearest-centroid classifier as the output layer (<a href="https://arxiv.org/abs/2105.07029">Triantafillou et al., 2021</a>).</figcaption>
</figure>
<p><strong>What happened?</strong>   Meta-learning and transfer learning, despite sharing the common goal of few-shot learning, have been studied mostly in distinct communitites. On a new benchmark <sup class="footnote-ref"><a href="#fn108" id="fnref108">[108]</a></sup>, large-scale transfer learning methods outperform meta-learning-based approaches. A promising direction is to scale up meta-learning methods, which, combined with more memory-efficient training methods, can improve the performance of meta-learning models on real-world benchmarks <sup class="footnote-ref"><a href="#fn109" id="fnref109">[109]</a></sup>. Meta-learning methods can also be combined with <a href="#5efficientmethods">efficient adaptation methods</a> such as FiLM layers <sup class="footnote-ref"><a href="#fn110" id="fnref110">[110]</a></sup> to adapt a general model effectively to new datasets <sup class="footnote-ref"><a href="#fn111" id="fnref111">[111]</a></sup>.</p>
<p><strong>Why is it important?</strong>   Meta-learning is an important paradigm but has fallen short of yielding state-of-the-art results on standard benchmarks that are not designed with meta-learning systems in mind. Bringing meta-learning and transfer learning communities closer together may lead to more practical meta-learnig methods that are useful in real-world applications.</p>
<p><strong>What’s next?</strong>   Meta-learning can be particularly useful when combined with the large number of natural tasks available for <a href="#2massivemultitasklearning">massive multi-task learning</a>. Meta-learning can also help improve prompting by learning how to design or use prompts based on the large number of available prompts.</p>
<h2 id="citation">Citation</h2>
<p>For attribution in academic contexts or books, please cite this work as:</p>
<pre><code>Sebastian Ruder, &quot;ML and NLP Research Highlights of 2021&quot;. http://ruder.io/ml-highlights-2021/, 2022.
</code></pre>
<p>BibTeX citation:</p>
<pre><code>@misc{ruder2022mlhighlights,
author = {Ruder, Sebastian},
title = {{ML and NLP Research Highlights of 2021}},
year = {2022},
howpublished = {\url{http://ruder.io/ml-highlights-2021/}},
}
</code></pre>
<h2 id="credits">Credits</h2>
<p>Thanks to Eleni Triantafillou and Dani Yogatama for thoughts and suggestions.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., … Liang, P. (2021). On the Opportunities and Risks of Foundation Models. <a href="http://arxiv.org/abs/2108.07258">http://arxiv.org/abs/2108.07258</a> <a href="#fnref1" class="footnote-backref">↩︎</a> <a href="#fnref1:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … Houlsby, N. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In Proceedings of ICLR 2021. <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Zhai, X., Kolesnikov, A., Houlsby, N., &amp; Beyer, L. (2021). Scaling Vision Transformers. <a href="http://arxiv.org/abs/2106.04560">http://arxiv.org/abs/2106.04560</a> <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>He, K., Chen, X., Xie, S., Li, Y., Dollár, P., &amp; Girshick, R. (2021). Masked Autoencoders Are Scalable Vision Learners. <a href="http://arxiv.org/abs/2111.06377">http://arxiv.org/abs/2111.06377</a> <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>Goyal, P., Caron, M., Lefaudeux, B., Xu, M., Wang, P., Pai, V., … Bojanowski, P. (2021). Self-supervised Pretraining of Visual Features in the Wild. <a href="http://arxiv.org/abs/2103.01988">http://arxiv.org/abs/2103.01988</a> <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>Baevski, A., Zhou, H., Mohamed, A., &amp; Auli, M. (2020). wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in Neural Information Processing Systems, 2020. <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p>Chung, Y.-A., Zhang, Y., Han, W., Chiu, C.-C., Qin, J., Pang, R., &amp; Wu, Y. (2021). W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training. <a href="http://arxiv.org/abs/2108.06209">http://arxiv.org/abs/2108.06209</a> <a href="#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p>Babu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu, Q., Goyal, N., … Auli, M. (2021). XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale. <a href="http://arxiv.org/abs/2111.09296">http://arxiv.org/abs/2111.09296</a> <a href="#fnref8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p>Fu, T.-J., Li, L., Gan, Z., Lin, K., Wang, W. Y., Wang, L., &amp; Liu, Z. (2021). VIOLET: End-to-End Video-Language Transformers with Masked Visual-token Modeling. <a href="http://arxiv.org/abs/2111.12681">http://arxiv.org/abs/2111.12681</a> <a href="#fnref9" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p>Bapna, A., Chung, Y., Wu, N., Gulati, A., Jia, Y., Clark, J. H., … Zhang, Y. (2021). SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training. <a href="http://arxiv.org/abs/2110.10329">http://arxiv.org/abs/2110.10329</a> <a href="#fnref10" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p>Bugliarello, E., Cotterell, R., Okazaki, N., &amp; Elliott, D. (2021). Multimodal pretraining unmasked: A meta-analysis and a unified framework of vision-and-language berts. Transactions of the Association for Computational Linguistics, 9, 978–994. <a href="https://doi.org/10.1162/tacl_a_00408">https://doi.org/10.1162/tacl_a_00408</a> <a href="#fnref11" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p>Hendricks, L. A., Mellor, J., Schneider, R., Alayrac, J. B., &amp; Nematzadeh, A. (2021). Decoupling the role of data, attention, and losses in multimodal transformers. Transactions of the Association for Computational Linguistics, 9, 570–585. <a href="https://doi.org/10.1162/tacl_a_00385">https://doi.org/10.1162/tacl_a_00385</a> <a href="#fnref12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p>Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., … Mordatch, I. (2021). Decision Transformer: Reinforcement Learning via Sequence Modeling. <a href="http://arxiv.org/abs/2106.01345">http://arxiv.org/abs/2106.01345</a> <a href="#fnref13" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn14" class="footnote-item"><p>Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., ... &amp; Hassabis, D. (2021). Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873), 583-589. <a href="#fnref14" class="footnote-backref">↩︎</a> <a href="#fnref14:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn15" class="footnote-item"><p>Abnar, S., Dehghani, M., Neyshabur, B., &amp; Sedghi, H. (2021). Exploring the Limits of Large Scale Pre-training. <a href="http://arxiv.org/abs/2110.02095">http://arxiv.org/abs/2110.02095</a> <a href="#fnref15" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn16" class="footnote-item"><p>Tay, Y., Dehghani, M., Rao, J., Fedus, W., Abnar, S., Chung, H. W., … Metzler, D. (2021). Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers. <a href="http://arxiv.org/abs/2109.10686">http://arxiv.org/abs/2109.10686</a> <a href="#fnref16" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn17" class="footnote-item"><p>Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., … Rush, A. M. (2021). Multitask Prompted Training Enables Zero-Shot Task Generalization. <a href="http://arxiv.org/abs/2110.08207">http://arxiv.org/abs/2110.08207</a> <a href="#fnref17" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn18" class="footnote-item"><p>Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., … Le, Q. V. (2021). Finetuned Language Models Are Zero-Shot Learners. <a href="http://arxiv.org/abs/2109.01652">http://arxiv.org/abs/2109.01652</a> <a href="#fnref18" class="footnote-backref">↩︎</a> <a href="#fnref18:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn19" class="footnote-item"><p>Aribandi, V., Tay, Y., Schuster, T., Rao, J., Zheng, H. S., Mehta, S. V., … Metzler, D. (2021). ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning. <a href="http://arxiv.org/abs/2111.10952">http://arxiv.org/abs/2111.10952</a> <a href="#fnref19" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn20" class="footnote-item"><p>Bansal, T., Gunasekaran, K., Wang, T., Munkhdalai, T., &amp; McCallum, A. (2021). Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP. In Proceedings of EMNLP 2021 (pp. 5812–5824). <a href="https://doi.org/10.18653/v1/2021.emnlp-main.469">https://doi.org/10.18653/v1/2021.emnlp-main.469</a> <a href="#fnref20" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn21" class="footnote-item"><p>Min, S., Lewis, M., Zettlemoyer, L., &amp; Hajishirzi, H. (2021). MetaICL: Learning to Learn In Context. <a href="http://arxiv.org/abs/2110.15943">http://arxiv.org/abs/2110.15943</a> <a href="#fnref21" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn22" class="footnote-item"><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention Is All You Need. In Proceedings of NIPS 2017. <a href="#fnref22" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn23" class="footnote-item"><p>Jaegle, A., Gimeno, F., Brock, A., Zisserman, A., Vinyals, O., &amp; Carreira, J. (2021). Perceiver: General Perception with Iterative Attention. In Proceedings of ICML 2021. <a href="http://arxiv.org/abs/2103.03206">http://arxiv.org/abs/2103.03206</a> <a href="#fnref23" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn24" class="footnote-item"><p>Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C., Ionescu, C., Ding, D., … Carreira, J. (2021). Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs. <a href="http://arxiv.org/abs/2107.14795">http://arxiv.org/abs/2107.14795</a> <a href="#fnref24" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn25" class="footnote-item"><p>Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., … Dosovitskiy, A. (2021). MLP-Mixer: An all-MLP Architecture for Vision. <a href="http://arxiv.org/abs/2105.01601">http://arxiv.org/abs/2105.01601</a> <a href="#fnref25" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn26" class="footnote-item"><p>Liu, H., Dai, Z., So, D. R., &amp; Le, Q. V. (2021). Pay Attention to MLPs, (Mlm). Retrieved from <a href="http://arxiv.org/abs/2105.08050">http://arxiv.org/abs/2105.08050</a> <a href="#fnref26" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn27" class="footnote-item"><p>Lee-Thorp, J., Ainslie, J., Eckstein, I., &amp; Ontanon, S. (2021). FNet: Mixing Tokens with Fourier Transforms. <a href="http://arxiv.org/abs/2105.03824">http://arxiv.org/abs/2105.03824</a> <a href="#fnref27" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn28" class="footnote-item"><p>Tay, Y., Dehghani, M., Gupta, J., Bahri, D., Aribandi, V., Qin, Z., &amp; Metzler, D. (2021). Are Pre-trained Convolutions Better than Pre-trained Transformers? In Proceedings of ACL 2021. Retrieved from <a href="http://arxiv.org/abs/2105.03322">http://arxiv.org/abs/2105.03322</a> <a href="#fnref28" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn29" class="footnote-item"><p>Clark, K., Luong, M.-T., Le, Q. V., &amp; Manning, C. D. (2020). ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. In Proceedings of ICLR 2020. <a href="#fnref29" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn30" class="footnote-item"><p>He, P., Gao, J., &amp; Chen, W. (2021). DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing. <a href="http://arxiv.org/abs/2111.09543">http://arxiv.org/abs/2111.09543</a> <a href="#fnref30" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn31" class="footnote-item"><p>Agarwal, R., Melnick, L., Frosst, N., Zhang, X., Lengerich, B., Caruana, R., &amp; Hinton, G. (2021). Neural Additive Models: Interpretable Machine Learning with Neural Nets. In Proceedings of NeurIPS 2021. <a href="http://arxiv.org/abs/2004.13912">http://arxiv.org/abs/2004.13912</a> <a href="#fnref31" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn32" class="footnote-item"><p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., … Amodei, D. (2020). Language Models are Few-Shot Learners. In Proceedings of NeurIPS 2020. <a href="http://arxiv.org/abs/2005.14165">http://arxiv.org/abs/2005.14165</a> <a href="#fnref32" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn33" class="footnote-item"><p>Schick, T., &amp; Schütze, H. (2021). Exploiting cloze questions for few shot text classification and natural language inference. In Proceedings of EACL 2021 (pp. 255–269). <a href="#fnref33" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn34" class="footnote-item"><p>Schick, T., &amp; Schütze, H. (2021). It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners. In Proceedings of NAACL 2021. <a href="http://arxiv.org/abs/2009.07118">http://arxiv.org/abs/2009.07118</a> <a href="#fnref34" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn35" class="footnote-item"><p>Tam, D., Menon, R. R., Bansal, M., Srivastava, S., &amp; Raffel, C. (2021). Improving and Simplifying Pattern Exploiting Training. <a href="http://arxiv.org/abs/2103.11955">http://arxiv.org/abs/2103.11955</a> <a href="#fnref35" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn36" class="footnote-item"><p>Perez, E., Kiela, D., &amp; Cho, K. (2021). True Few-Shot Learning with Language Models. In Proceedings of NeurIPS 2021. <a href="http://arxiv.org/abs/2105.11447">http://arxiv.org/abs/2105.11447</a> <a href="#fnref36" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn37" class="footnote-item"><p>Zheng, Y., Zhou, J., Qian, Y., Ding, M., Li, J., Salakhutdinov, R., … Yang, Z. (2021). FewNLU: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding. <a href="http://arxiv.org/abs/2109.12742">http://arxiv.org/abs/2109.12742</a> <a href="#fnref37" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn38" class="footnote-item"><p>Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., &amp; Neubig, G. (2021). Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. <a href="http://arxiv.org/abs/2107.13586">http://arxiv.org/abs/2107.13586</a> <a href="#fnref38" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn39" class="footnote-item"><p>Scao, T. Le, &amp; Rush, A. M. (2021). How Many Data Points is a Prompt Worth? In Proceedings of NAACL 2021. <a href="http://arxiv.org/abs/2103.08493">http://arxiv.org/abs/2103.08493</a> <a href="#fnref39" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn40" class="footnote-item"><p>Ratner, A., De Sa, C., Wu, S., Selsam, D., &amp; Ré, C. (2016). Data Programming: Creating Large Training Sets, Quickly. In Advances in Neural Information Processing Systems 29 (NIPS 2016). <a href="http://arxiv.org/abs/1605.07723">http://arxiv.org/abs/1605.07723</a> <a href="#fnref40" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn41" class="footnote-item"><p>Mishra, S., Khashabi, D., Baral, C., &amp; Hajishirzi, H. (2021). Cross-Task Generalization via Natural Language Crowdsourcing Instructions. <a href="http://arxiv.org/abs/2104.08773">http://arxiv.org/abs/2104.08773</a> <a href="#fnref41" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn42" class="footnote-item"><p>Wiegreffe, S., &amp; Marasović, A. (2021). Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing. In 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks. <a href="http://arxiv.org/abs/2102.12060">http://arxiv.org/abs/2102.12060</a> <a href="#fnref42" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn43" class="footnote-item"><p>Ma, X., Kong, X., Wang, S., Zhou, C., May, J., Ma, H., &amp; Zettlemoyer, L. (2021). Luna: Linear Unified Nested Attention. In Proceedings of NeurIPS 2021. <a href="http://arxiv.org/abs/2106.01540">http://arxiv.org/abs/2106.01540</a> <a href="#fnref43" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn44" class="footnote-item"><p>Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N. A., &amp; Lingpeng Kong. (2021). Random Feature Attention. In Proceedings of ICLR 2021. <a href="#fnref44" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn45" class="footnote-item"><p>Tay, Y., Dehghani, M., Bahri, D., &amp; Metzler, D. (2020). Efficient Transformers: A Survey. ArXiv Preprint ArXiv:2009.06732. Retrieved from <a href="http://arxiv.org/abs/2009.06732">http://arxiv.org/abs/2009.06732</a> <a href="#fnref45" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn46" class="footnote-item"><p>Lester, B., Al-Rfou, R., &amp; Constant, N. (2021). The Power of Scale for Parameter-Efficient Prompt Tuning. In Proceedings of EMNLP 2021. <a href="http://arxiv.org/abs/2104.08691">http://arxiv.org/abs/2104.08691</a> <a href="#fnref46" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn47" class="footnote-item"><p>Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., &amp; Tang, J. (2021). GPT Understands, Too. <a href="http://arxiv.org/abs/2103.10385">http://arxiv.org/abs/2103.10385</a> <a href="#fnref47" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn48" class="footnote-item"><p>Mao, Y., Mathias, L., Hou, R., Almahairi, A., Ma, H., Han, J., … Khabsa, M. (2021). UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning. <a href="http://arxiv.org/abs/2110.07577">http://arxiv.org/abs/2110.07577</a> <a href="#fnref48" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn49" class="footnote-item"><p>He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., &amp; Neubig, G. (2021). Towards a Unified View of Parameter-Efficient Transfer Learning. <a href="http://arxiv.org/abs/2110.04366">http://arxiv.org/abs/2110.04366</a> <a href="#fnref49" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn50" class="footnote-item"><p>Mahabadi, R. K., Henderson, J., &amp; Ruder, S. (2021). Compacter: Efficient Low-Rank Hypercomplex Adapter Layers. In Proceedings of NeurIPS 2021. <a href="http://arxiv.org/abs/2106.04647">http://arxiv.org/abs/2106.04647</a> <a href="#fnref50" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn51" class="footnote-item"><p>Tsimpoukelli, M., Menick, J., Cabi, S., Eslami, S. M. A., Vinyals, O., &amp; Hill, F. (2021). Multimodal Few-Shot Learning with Frozen Language Models. In Proceedings of NeurIPS 2021. <a href="http://arxiv.org/abs/2106.13884">http://arxiv.org/abs/2106.13884</a> <a href="#fnref51" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn52" class="footnote-item"><p>Pfeiffer, J., Vulić, I., Gurevych, I., &amp; Ruder, S. (2020). MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer. In Proceedings of EMNLP 2020. <a href="#fnref52" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn53" class="footnote-item"><p>Eichenberg, C., Black, S., Weinbach, S., Parcalabescu, L., &amp; Frank, A. (2021). MAGMA--Multimodal Augmentation of Generative Models through Adapter-based Finetuning. <a href="https://arxiv.org/abs/2112.05253">https://arxiv.org/abs/2112.05253</a> <a href="#fnref53" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn54" class="footnote-item"><p>Dettmers, T., Lewis, M., Shleifer, S., &amp; Zettlemoyer, L. (2021). 8-bit Optimizers via Block-wise Quantization. <a href="http://arxiv.org/abs/2110.02861">http://arxiv.org/abs/2110.02861</a> <a href="#fnref54" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn55" class="footnote-item"><p>Koch, B., Denton, E., Hanna, A., &amp; Foster, J. G. (2021). Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research. In 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks. <a href="http://arxiv.org/abs/2112.01716">http://arxiv.org/abs/2112.01716</a> <a href="#fnref55" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn56" class="footnote-item"><p>Kiela, D., Bartolo, M., Nie, Y., Kaushik, D., Geiger, A., Wu, Z., … Williams, A. (2021). Dynabench: Rethinking Benchmarking in NLP. In Proceedings of NAACL 2021 (pp. 4110–4124). <a href="https://doi.org/10.18653/v1/2021.naacl-main.324">https://doi.org/10.18653/v1/2021.naacl-main.324</a> <a href="#fnref56" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn57" class="footnote-item"><p>Liu, P., Fu, J., Xiao, Y., Yuan, W., Chang, S., Dai, J., … Neubig, G. (2021). ExplainaBoard: An Explainable Leaderboard for NLP. In Proceedings of ACL 2021: System demonstrations (pp. 280–289). <a href="#fnref57" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn58" class="footnote-item"><p>Ma, Z., Ethayarajh, K., Thrush, T., Jain, S., Wu, L., Jia, R., … Kiela, D. (2021). Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking. <a href="http://arxiv.org/abs/2106.06052">http://arxiv.org/abs/2106.06052</a> <a href="#fnref58" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn59" class="footnote-item"><p>Bragg, J., Cohan, A., Lo, K., &amp; Beltagy, I. (2021). FLEX: Unifying Evaluation for Few-Shot NLP. In Proceedings of NeurIPS 2021. Retrieved from <a href="http://arxiv.org/abs/2107.07170">http://arxiv.org/abs/2107.07170</a> <a href="#fnref59" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn60" class="footnote-item"><p>Ye, Q., Lin, B. Y., &amp; Ren, X. (2021). CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP. In Proceedings of EMNLP 2021. <a href="#fnref60" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn61" class="footnote-item"><p>Koh, P. W., Sagawa, S., Marklund, H., Xie, S. M., Zhang, M., Balsubramani, A., … Liang, P. (2021). WILDS: A Benchmark of in-the-Wild Distribution Shifts. In Proceedings of ICML 2021. <a href="http://arxiv.org/abs/2012.07421">http://arxiv.org/abs/2012.07421</a> <a href="#fnref61" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn62" class="footnote-item"><p>Yang, S., Chi, P.-H., Chuang, Y.-S., Lai, C.-I. J., Lakhotia, K., Lin, Y. Y., … Lee, H. (2021). SUPERB: Speech processing Universal PERformance Benchmark. In Proceedings of Interspeech 2021. <a href="http://arxiv.org/abs/2105.01051">http://arxiv.org/abs/2105.01051</a> <a href="#fnref62" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn63" class="footnote-item"><p>Cahyawijaya, S., Winata, G. I., Wilie, B., Vincentio, K., Li, X., Kuncoro, A., … Fung, P. (2021). IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation. In Proceedings of EMNLP 2021 (pp. 8875–8898). <a href="https://doi.org/10.18653/v1/2021.emnlp-main.699">https://doi.org/10.18653/v1/2021.emnlp-main.699</a> <a href="#fnref63" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn64" class="footnote-item"><p>Dumitrescu, S., Rebeja, P., Rosia, L., Marchidan, G., Yogatama, D., Avram, A., … Morogan, L. (2021). LiRo: Benchmark and leaderboard for Romanian language tasks. In 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks. <a href="#fnref64" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn65" class="footnote-item"><p>Tamkin, A., Liu, V., Lu, R., Fein, D., Schultz, C., &amp; Goodman, N. (2021). DABS: A Domain-Agnostic Benchmark for Self-Supervised Learning. In 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks. <a href="http://arxiv.org/abs/2111.12062">http://arxiv.org/abs/2111.12062</a> <a href="#fnref65" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn66" class="footnote-item"><p>Ruder, S., Constant, N., Botha, J., Siddhant, A., Firat, O., Fu, J., … Johnson, M. (2021). XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation. In Proceedings of EMNLP 2021. <a href="http://arxiv.org/abs/2104.07412">http://arxiv.org/abs/2104.07412</a> <a href="#fnref66" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn67" class="footnote-item"><p>Marie, B., Fujita, A., &amp; Rubino, R. (2021). Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers. In Proceedings of ACL 2021 (pp. 7297–7306). <a href="https://doi.org/10.18653/v1/2021.acl-long.566">https://doi.org/10.18653/v1/2021.acl-long.566</a> <a href="#fnref67" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn68" class="footnote-item"><p>Gehrmann, S., Adewumi, T., Aggarwal, K., Ammanamanchi, P. S., Anuoluwapo, A., Bosselut, A., … Zhou, J. (2021). The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics. <a href="http://arxiv.org/abs/2102.01672">http://arxiv.org/abs/2102.01672</a> <a href="#fnref68" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn69" class="footnote-item"><p>Kasai, J., Sakaguchi, K., Bras, R. Le, Dunagan, L., Morrison, J., Fabbri, A. R., … Smith, N. A. (2021). Bidimensional Leaderboards: Generate and Evaluate Language Hand in Hand. <a href="http://arxiv.org/abs/2112.04139">http://arxiv.org/abs/2112.04139</a> <a href="#fnref69" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn70" class="footnote-item"><p>Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., … Sutskever, I. (2021). Zero-Shot Text-to-Image Generation. <a href="https://arxiv.org/abs/2102.12092">https://arxiv.org/abs/2102.12092</a> <a href="#fnref70" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn71" class="footnote-item"><p>Esser, P., Rombach, R., &amp; Ommer, B. (2020). Taming Transformers for High-Resolution Image Synthesis. <a href="https://arxiv.org/abs/2012.09841">https://arxiv.org/abs/2012.09841</a> <a href="#fnref71" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn72" class="footnote-item"><p>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. <a href="http://arxiv.org/abs/2103.00020">http://arxiv.org/abs/2103.00020</a> <a href="#fnref72" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn73" class="footnote-item"><p>Dhariwal, P., &amp; Nichol, A. (2021). Diffusion Models Beat GANs on Image Synthesis. <a href="https://arxiv.org/abs/2105.05233">https://arxiv.org/abs/2105.05233</a> <a href="#fnref73" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn74" class="footnote-item"><p>Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., … Chen, M. (2021). GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. <a href="http://arxiv.org/abs/2112.10741">http://arxiv.org/abs/2112.10741</a> <a href="#fnref74" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn75" class="footnote-item"><p>Ravuri, S., Lenc, K., Willson, M., Kangin, D., Lam, R., Mirowski, P., …  &amp; Mohamed, S. (2021). Skillful Precipitation Nowcasting using Deep Generative Models of Radar. Nature, 597. <a href="https://www.nature.com/articles/s41586-021-03854-z">https://www.nature.com/articles/s41586-021-03854-z</a> <a href="#fnref75" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn76" class="footnote-item"><p>Espeholt, L., Agrawal, S., Sønderby, C., Kumar, M., Heek, J., Bromberg, C., … Kalchbrenner, N. (2021). Skillful Twelve Hour Precipitation Forecasts using Large Context Neural Networks, 1–34. Retrieved from <a href="http://arxiv.org/abs/2111.07470">http://arxiv.org/abs/2111.07470</a> <a href="#fnref76" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn77" class="footnote-item"><p>Davies, A., Veličković, P., Buesing, L., Blackwell, S., Zheng, D., Tomašev, N., … Kohli, P. (2021). Advancing mathematics by guiding human intuition with AI. Nature, 600(7887), 70–74. <a href="https://doi.org/10.1038/s41586-021-04086-x">https://doi.org/10.1038/s41586-021-04086-x</a> <a href="#fnref77" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn78" class="footnote-item"><p>Charton, F., Hayat, A., &amp; Lample, G. (2021). Deep Differential System Stability Learning advanced computations from examples. In Proceedings of ICLR 2021. <a href="#fnref78" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn79" class="footnote-item"><p>Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., … Zaremba, W. (2021). Evaluating Large Language Models Trained on Code. Retrieved from <a href="http://arxiv.org/abs/2107.03374">http://arxiv.org/abs/2107.03374</a> <a href="#fnref79" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn80" class="footnote-item"><p>Roziere, B., Marc, M. L., &amp; Guillaume, S. (2021). DOBF: A Deobfuscation Pre-Training Objective for Programming Languages. In Proceedings of NeurIPS 2021. <a href="#fnref80" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn81" class="footnote-item"><p>Jain, P., Jain, A., Zhang, T., Abbeel, P., Gonzalez, J. E., &amp; Stoica, I. (2021). Contrastive Code Representation Learning. In Proceedings of EMNLP 2021. <a href="#fnref81" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn82" class="footnote-item"><p>Elnaggar, A., Gibbs, T., &amp; Matthes, F. (2021). CodeTrans: Towards Cracking the Language of Silicone’s Code Through Self-Supervised Deep Learning and High Performance Computing. <a href="#fnref82" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn83" class="footnote-item"><p>Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., … Sutton, C. (2021). Program Synthesis with Large Language Models, 1–34. <a href="http://arxiv.org/abs/2108.07732">http://arxiv.org/abs/2108.07732</a> <a href="#fnref83" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn84" class="footnote-item"><p>Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., … Odena, A. (2021). Show Your Work: Scratchpads for Intermediate Computation with Language Models, 1–16. <a href="http://arxiv.org/abs/2112.00114">http://arxiv.org/abs/2112.00114</a> <a href="#fnref84" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn85" class="footnote-item"><p>Xu, F. F., Vasilescu, B., &amp; Neubig, G. (2021). In-IDE Code Generation from Natural Language: Promise and Challenges. ACM Transactions on Software Engineering and Methodology. <a href="#fnref85" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn86" class="footnote-item"><p>Bender, E., Gebru, T., McMillan-Major, A., &amp; Shmitchell, S. (2021). On the dangers of stochastic parrots: can language models be too big? In FAccT 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. Association for Computing Machinery. <a href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a> <a href="#fnref86" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn87" class="footnote-item"><p>Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S., … Gabriel, I. (2021). Ethical and social risks of harm from Language Models. <a href="#fnref87" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn88" class="footnote-item"><p>Liu, R., Jia, C., Wei, J., Xu, G., Wang, L., &amp; Vosoughi, S. (2021). Mitigating Political Bias in Language Models Through Reinforced Calibration. In Proceedings of AAAI 2021. <a href="#fnref88" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn89" class="footnote-item"><p>Ahn, J., &amp; Oh, A. (2021). Mitigating Language-Dependent Ethnic Bias in BERT. In Proceedings of EMNLP 2021. <a href="https://doi.org/10.18653/v1/2021.emnlp-main.42">https://doi.org/10.18653/v1/2021.emnlp-main.42</a> <a href="#fnref89" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn90" class="footnote-item"><p>Welbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L. A., … Huang, P.-S. (2021). Challenges in Detoxifying Language Models. In Findings of EMNLP 2021 (pp. 2447–2469). <a href="http://arxiv.org/abs/2109.07445">http://arxiv.org/abs/2109.07445</a> <a href="#fnref90" class="footnote-backref">↩︎</a> <a href="#fnref90:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn91" class="footnote-item"><p>Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., … Sifre, L. (2021). Improving language models by retrieving from trillions of tokens. <a href="http://arxiv.org/abs/2112.04426">http://arxiv.org/abs/2112.04426</a> <a href="#fnref91" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn92" class="footnote-item"><p>Komeili, M., Shuster, K., &amp; Weston, J. (2021). Internet-Augmented Dialogue Generation. <a href="#fnref92" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn93" class="footnote-item"><p>Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., … Schulman, J. (2021). WebGPT: Browser-assisted question-answering with human feedback. <a href="http://arxiv.org/abs/2112.09332">http://arxiv.org/abs/2112.09332</a> <a href="#fnref93" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn94" class="footnote-item"><p>Sachan, D. S., Reddy, S., Hamilton, W., Dyer, C., &amp; Yogatama, D. (2021). End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering. In Proceedings of NeurIPS 2021. <a href="http://arxiv.org/abs/2106.05346">http://arxiv.org/abs/2106.05346</a> <a href="#fnref94" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn95" class="footnote-item"><p>Yogatama, D., D’autume, C. de M., &amp; Kong, L. (2021). Adaptive semiparametric language models. Transactions of the Association for Computational Linguistics, 9, 362–373. <a href="https://doi.org/10.1162/tacl_a_00371">https://doi.org/10.1162/tacl_a_00371</a> <a href="#fnref95" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn96" class="footnote-item"><p>Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., &amp; Lewis, M. (2020). Generalization through Memorization: Nearest Neighbor Language Models. In Proceedings of ICLR 2020. <a href="http://arxiv.org/abs/1911.00172">http://arxiv.org/abs/1911.00172</a> <a href="#fnref96" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn97" class="footnote-item"><p>Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., … Raffel, C. (2021). ByT5: Towards a token-free future with pre-trained byte-to-byte models. ArXiv Preprint ArXiv:2105.13626. ttp://arxiv.org/abs/2105.13626 <a href="#fnref97" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn98" class="footnote-item"><p>Clark, J. H., Garrette, D., Turc, I., &amp; Wieting, J. (2021). Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation. <a href="https://arxiv.org/abs/2103.06874">https://arxiv.org/abs/2103.06874</a> <a href="#fnref98" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn99" class="footnote-item"><p>Tay, Y., Tran, V. Q., Ruder, S., Gupta, J., Chung, H. W., Bahri, D., … Metzler, D. (2021). Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. <a href="http://arxiv.org/abs/2106.12672">http://arxiv.org/abs/2106.12672</a> <a href="#fnref99" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn100" class="footnote-item"><p>Lazaridou, A., Kuncoro, A., &amp; Gribovskaya, E. (2021). Mind the Gap : Assessing Temporal Generalization in Neural Language Models. In Proceedings of NeurIPS 2021. <a href="#fnref100" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn101" class="footnote-item"><p>Röttger, P., &amp; Pierrehumbert, J. B. (2021). Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media. In Findings of EMNLP 2021 (pp. 2400–2412). <a href="https://doi.org/10.18653/v1/2021.findings-emnlp.206">https://doi.org/10.18653/v1/2021.findings-emnlp.206</a> <a href="#fnref101" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn102" class="footnote-item"><p>Dhingra, B., Cole, J. R., Eisenschlos, J. M., Gillick, D., Eisenstein, J., &amp; Cohen, W. W. (2021). Time-Aware Language Models as Temporal Knowledge Bases. <a href="http://arxiv.org/abs/2106.15110">http://arxiv.org/abs/2106.15110</a> <a href="#fnref102" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn103" class="footnote-item"><p>Zhang, M. J. Q., &amp; Choi, E. (2021). SituatedQA: Incorporating Extra-Linguistic Contexts into QA. In Proceedings of EMNLP 2021. <a href="https://doi.org/10.18653/v1/2021.emnlp-main.586">https://doi.org/10.18653/v1/2021.emnlp-main.586</a> <a href="#fnref103" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn104" class="footnote-item"><p>Birhane, A., Prabhu, V. U., &amp; Kahembwe, E. (2021). Multimodal datasets: misogyny, pornography, and malignant stereotypes. <a href="http://arxiv.org/abs/2110.01963">http://arxiv.org/abs/2110.01963</a> <a href="#fnref104" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn105" class="footnote-item"><p>Dodge, J., Sap, M., Marasović, A., Agnew, W., Ilharco, G., Groeneveld, D., &amp; Gardner, M. (2021). Documenting the English Colossal Clean Crawled Corpus. In Proceedings of EMNLP 2021. <a href="#fnref105" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn106" class="footnote-item"><p>Kreutzer, J., Caswell, I., Wang, L., Wahab, A., Esch, D. van, Ulzii-Orshikh, N., … Adeyemi, M. (2021). Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. In Transactions of the ACL 2021. <a href="#fnref106" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn107" class="footnote-item"><p>Liu, F., Bugliarello, E., Ponti, E. M., Reddy, S., Collier, N., &amp; Elliott, D. (2021). Visually Grounded Reasoning across Languages and Cultures. In Proceedings of EMNLP 2021. <a href="https://arxiv.org/abs/2109.13238">https://arxiv.org/abs/2109.13238</a> <a href="#fnref107" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn108" class="footnote-item"><p>Dumoulin, V., Houlsby, N., Evci, U., Zhai, X., Goroshin, R., Gelly, S., &amp; Larochelle, H. (2021). Comparing Transfer and Meta Learning Approaches on a Unified Few-Shot Classification Benchmark. In 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks. <a href="http://arxiv.org/abs/2104.02638">http://arxiv.org/abs/2104.02638</a> <a href="#fnref108" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn109" class="footnote-item"><p>Bronskill, J., Massiceti, D., Patacchiola, M., Hofmann, K., Nowozin, S., &amp; Turner, R. E. (2021). Memory Efficient Meta-Learning with Large Images, (NeurIPS). <a href="http://arxiv.org/abs/2107.01105">http://arxiv.org/abs/2107.01105</a> <a href="#fnref109" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn110" class="footnote-item"><p>Perez, E., Strub, F., De Vries, H., Dumoulin, V., &amp; Courville, A. (2018). FiLM: Visual reasoning with a general conditioning layer. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 (pp. 3942–3951). <a href="#fnref110" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn111" class="footnote-item"><p>Triantafillou, E., Larochelle, H., Zemel, R., &amp; Dumoulin, V. (2021). Learning a Universal Template for Few-shot Dataset Generalization. In Proceedings of ICML 2021. <a href="http://arxiv.org/abs/2105.07029">http://arxiv.org/abs/2105.07029</a> <a href="#fnref111" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Multi-domain Multilingual Question Answering]]></title><description><![CDATA[This post expands on the EMNLP 2021 tutorial on Multi-domain Multilingual Question Answering and highlights key insights and takeaways.]]></description><link>http://ruder.io/multi-qa-tutorial/</link><guid isPermaLink="false">619bf3696d05ee2bd16c34c0</guid><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Mon, 06 Dec 2021 12:38:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2021/12/rc_olympics.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2021/12/rc_olympics.png" alt="Multi-domain Multilingual Question Answering"><p>This post expands on the <a href="https://tinyurl.com/multi-qa-tutorial">EMNLP 2021 tutorial on Multi-domain Multilingual Question Answering</a>.</p><p>The tutorial was organised by <a href="https://researcher.watson.ibm.com/researcher/view_person_pubs.php?person=us-avi&amp;t=1">Avi Sil</a> and me. In this post, I highlight key insights and takeaways of the tutorial. The slides are <a href="https://tinyurl.com/multi-qa-tutorial">available online</a>. You can find the table of contents below:</p><!--kg-card-begin: markdown--><ol>
<li><a href="#introduction">Introduction</a>
<ol>
<li><a href="#open-retrieval-qa-vs-reading-comprehension">Open-Retrieval QA vs Reading Comprehension</a></li>
<li><a href="#what-is-a-domain">What is a Domain?</a></li>
</ol>
</li>
<li><a href="#multi-domain-qa">Multi-Domain QA</a>
<ol>
<li><a href="#datasets-for-multi-domain-qa">Datasets for Multi-Domain QA</a></li>
<li><a href="#multi-domain-qa-models">Multi-Domain QA Models</a>
<ul>
<li><a href="#unsupervised-domain-adaptation-for-qa">Unsupervised Domain Adaptation for QA</a></li>
<li><a href="#domain-adaptation-with-pre-trained-lms">Domain Adaptation with Pre-trained LMs</a></li>
<li><a href="#domain-generalization">Domain Generalization</a></li>
</ul>
</li>
</ol>
</li>
<li><a href="#multilingual-qa">Multilingual QA</a>
<ol>
<li><a href="#datasets-for-multilingual-qa">Datasets for Multilingual QA</a>
<ul>
<li><a href="#issues-of-multilingual-qa-datasets">Issues of Multilingual QA Datasets</a></li>
<li><a href="#creating-qa-datasets">Creating QA Datasets</a></li>
<li><a href="#multilingual-qa-evaluation">Multilingual QA Evaluation</a></li>
</ul>
</li>
<li><a href="#multilingual-qa-models">Multilingual QA Models</a>
<ul>
<li><a href="#models-for-multilingual-reading-comprehension">Models for Multilingual Reading Comprehension</a></li>
<li><a href="#models-for-multilingual-open-retrieval-qa">Models for Multilingual Open-Retrieval QA</a></li>
</ul>
</li>
</ol>
</li>
<li><a href="#open-research-directions">Open Research Directions</a></li>
</ol>
<!--kg-card-end: markdown--><hr><h1 id="introduction">Introduction</h1><p>Question answering is one of the most impactful tasks in natural language processing (NLP). In the tutorial, we focus on two main categories of question answering studied in the literature: open-retrieval question answering (ORQA) and reading comprehension (RC). </p><h2 id="open-retrieval-qa-vs-reading-comprehension">Open-Retrieval QA vs Reading Comprehension</h2><p>Open-retrieval QA focuses on the most general setting where given a question we first need to retrieve relevant documents from a large corpus such as Wikipedia. We then process these documents to identify the relevant answer as can be seen below. We avoid using the term open-domain QA as "open-domain" may also refer to a setting covering many domains.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/orqa-1.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>Open-Retrieval Question Answering (image based on <a href="https://aclanthology.org/P17-1171/">Chen et al., 2017</a>)</figcaption></figure><p>Reading comprehension can be seen as a sub-problem of open-retrieval QA as it assumes that we have access to the gold paragraph that contains the answer (see below). We then only need to find the corresponding answer in this paragraph. In both settings, answers are commonly represented as a minimal span.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/reading_comprehension-1.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>Reading Comprehension assumes a gold paragraph is provided</figcaption></figure><p>Standard approaches for reading comprehension build on pre-trained models such as BERT. The model is provided with the question and candidate paragraph as input and is trained to predict whether the question is answerable (typically using the representation associated with its [CLS] token) and whether each token is the start or end of an answer span, which can be seen below. The same approach can be used for ORQA, with some modifications (<a href="https://arxiv.org/abs/1901.08634">Alberti et al., 2019</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/bert_rc.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>Using BERT for reading comprehension involves fine-tuning it to predict a) whether a question is answerable and b) whether each token is the start and end of an answer span.</figcaption></figure><p>Information retrieval (IR) methods are used to retrieve the relevant paragraphs. Classic sparse methods such as BM25 (<a href="https://www.staff.city.ac.uk/~sbrp622/papers/foundations_bm25_review.pdf">Robertson et al., 2009</a>) do not require any training as they weigh terms and documents based on their frequency using <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf-idf</a> measures. Recent dense neural approaches such as DPR (<a href="https://aclanthology.org/2020.emnlp-main.550/">Karpukhin et al., 2020</a>) train models to maximize the similarity between question and passage and then retrieve the most relevant passages via maximum inner product search.</p><h2 id="what-is-a-domain">What is a Domain?</h2><p>A domain can be seen as a manifold in a high-dimensional variety space consisting of many dimensions such as socio-demographics, language, genre, sentence type, etc (<a href="https://arxiv.org/abs/1608.07836">Plank et al., 2016</a>). Domains differ in terms of their granularity and large domains such as Twitter can consist of several more narrow domains to which we might want to adapt our models. Two major facets of this variety space are <strong>genre </strong>(we will use this term interchangeably with 'domain') and <strong>language</strong>, which will be the focus of this post.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/domain_granularity.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>Domain granularity (Credit: <a href="https://aclanthology.org/2020.acl-main.740/">Gururangan et al., 2020</a>)</figcaption></figure><h1 id="multi-domain-qa">Multi-Domain QA</h1><p>In each of the two main sections of this post, we will first discuss common datasets and then modelling approaches.</p><h2 id="datasets-for-multi-domain-qa">Datasets for Multi-Domain QA</h2><p>Research in question answering has spanned many domains, as can be seen below. The most common domain is <strong>Encyclopedia</strong>, which covers many Wikipedia-based datasets such as SQuAD (<a href="https://aclanthology.org/D16-1264/">Rajpurkar et al., 2016</a>), Natural Questions (NQ; <a href="https://aclanthology.org/Q19-1026/">Kwiatkowski et al., 2019</a>), DROP (<a href="https://arxiv.org/abs/1903.00161">Dua et al., 2019</a>), and WikiReading (<a href="https://aclanthology.org/P16-1145/">Hewlett et al., 2016</a>), among many others. Datasets in this domain are typically referred to as "open-domain" QA.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/rc_olympics.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>RC Olympics: The many domains of reading comprehension</figcaption></figure><p>Datasets in the <strong>Fiction</strong> domain typically require processing narratives in books such as NarrativeQA (<a href="https://aclanthology.org/Q18-1023/">Kočiský et al., 2018</a>), Children's Book Test (<a href="https://arxiv.org/abs/1511.02301">Hill et al., 2016</a>), and BookTest (<a href="https://arxiv.org/abs/1610.00956">Bajgar et al., 2016</a>) or narratives written by crowd workers such as MCTest (<a href="https://aclanthology.org/D13-1020/">Richardson et al., 2013</a>), MCScript (<a href="https://aclanthology.org/L18-1564">Modi et al., 2016</a>), and ROCStories (<a href="https://aclanthology.org/N16-1098/">Mostafazedh et al., 2016</a>).</p><p><strong>Academic tests</strong> datasets target science questions in US school tests such as ARC (<a href="https://arxiv.org/abs/1803.05457">Clark et al., 2018</a>), college-level exam resources such as ReClor (<a href="https://openreview.net/forum?id=HJgJtT4tvB">Yu et al., 2019</a>) and school-level exam resources such as RACE (<a href="https://aclanthology.org/D17-1082/">Lai et al., 2017</a>).</p><p><strong>News</strong> datasets include NewsQA (<a href="https://aclanthology.org/W17-2623/">Trischler et al., 2017</a>), CNN / Daily Mail (<a href="https://proceedings.neurips.cc/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf">Hermann et al., 2015</a>), and NLQuAD (<a href="https://aclanthology.org/2021.eacl-main.106/">Soleimani et al., 2021</a>).</p><p>In addition, there are datasets focused on <strong>Specialized Expert Materials</strong> including manuals, reports, scientific papers, etc. Such domains are most common in industry as domain-specific chatbots are increasingly used by companies to respond to users queries but associated datasets are rarely made available. Existing datasets focus on tech forums such as TechQA (<a href="https://aclanthology.org/2020.acl-main.117/">Castelli et al., 2020</a>) and AskUbuntu (<a href="https://aclanthology.org/P15-2114/">dos Santos et al., 2015</a>) as well as scientific articles including BioASQ (<a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0564-6">Tsatsaronis et al., 2015</a>) and Qasper (<a href="https://aclanthology.org/2021.naacl-main.365/">Dasigi et al., 2021</a>). In addition, the pandemic saw the creation of many datasets related to COVID-19 such as COVID-QA-2019 (<a href="https://aclanthology.org/2020.nlpcovid19-acl.18/">Möller et al., 2020</a>), COVID-QA-147 (<a href="https://arxiv.org/abs/2004.11339">Tang et al., 2020</a>), and COVID-QA-111 (<a href="https://aclanthology.org/2020.nlpcovid19-2.1/">Lee et al., 2020</a>).</p><p>Beyond these datasets focusing on individual domains, there are several datasets spanning multiple domains such as CoQA (<a href="https://aclanthology.org/Q19-1016/">Reddy et al., 2019</a>), QuAIL (<a href="https://ojs.aaai.org//index.php/AAAI/article/view/6398">Rogers et al., 2020</a>), and MultiReQA (<a href="https://aclanthology.org/2021.adaptnlp-1.10/">Guo et al., 2021</a>), among others. Lastly, the <a href="https://leaderboard.allenai.org/orb/submissions/about">ORB</a> evaluation server (<a href="https://arxiv.org/abs/1912.12598">Dua et al., 2019</a>) enables the evaluation of systems across datasets spanning multiple domains.</p><h2 id="multi-domain-qa-models">Multi-Domain QA Models</h2><p>In most cases when learning in a multi-domain setting there may be limited or no labelled data available in the target domain available. We discuss how to adapt to a target domain using only unlabelled data, the most effective way to use pre-trained language models for domain adaptation in QA, and how to generalize across domains.</p><h3 id="unsupervised-domain-adaptation-for-qa">Unsupervised Domain Adaptation for QA</h3><p>Unsupervised domain adaptation for QA assumes access to labelled data in a source domain and unlabelled target domain data. In the absence of labelled gold data in the target domain, most methods rely on generating 'silver' &lt;question, paragraph, answer&gt; data in the target domain. To this end, these methods train a question generation model based on a pre-trained LM on the source domain and then apply it to the target domain to generate synthetic questions given answer spans (<a href="https://aclanthology.org/D19-1254/">Wang et al., 2019</a>; <a href="https://aclanthology.org/2020.emnlp-main.439/">Shakeri et al., 2020</a>; <a href="https://aclanthology.org/2021.emnlp-main.754/">Yue et al., 2021</a>). The QA model is then trained jointly on the gold source domain and silver target domain data, as can be seen below. This is often combined with other domain adaptation strategies such as adversarial learning and self-training (<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6245">Cao et al., 2020</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/qagen.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>Joint training on gold source and silver target domain data that is generated using BART or T5</figcaption></figure><h3 id="domain-adaptation-with-pre-trained-lms">Domain Adaptation with Pre-trained LMs </h3><p>In many scenarios with specialized domains, a general pre-trained language model such as BERT may not be sufficient. Instead, <a href="https://ruder.io/recent-advances-lm-fine-tuning/#adaptive-fine-tuning">domain-adaptive fine-tuning</a> (<a href="https://www.aclweb.org/anthology/P18-1031/">Howard &amp; Ruder, 2018</a>; <a href="https://www.aclweb.org/anthology/2020.acl-main.740/">Gururangan et al., 2020</a>) of a pre-trained LM on target domain data typically performs better. Recent examples such as BioBERT (<a href="https://arxiv.org/abs/1901.08746">Lee et al., 2019</a>) and SciBERT (<a href="https://aclanthology.org/D19-1371/">Beltagy et al., 2019</a>) have been effective for biomedical and scientific domains respectively.</p><p>When labelled data in a source domain and in the target domain are available, a general recipe is to first fine-tune a model on labelled source domain data and then to subsequently fine-tune it on labelled data in the target domain. However, naive fine-tuning generally leads to deterioration of performance in the source domain, which may be undesirable. Instead, strategies from continual learning such as L2 regularization (<a href="https://arxiv.org/abs/1911.00202">Xu et al., 2019</a>) ensure that the parameters of the model fine-tuned on the target domain do not diverge significantly from the source domain model, thereby reducing catastrophic forgetting as can be seen below. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/l2_regularization-1.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>Strategies such as L2 regularization result in reasonable performance on the source domain (SQuAD) after fine-tuning on the target domain (BioASQ)</figcaption></figure><p>For very low-resource domains, another strategy is to explicitly adapt the model to characteristics of the target domain. If the target domain contains specialized terms, the model's vocabulary can be augmented with these terms to learn better representations. In addition, for many domains, the structure of the unlabelled data can contain information that may be useful for the end task. This structure such as the abstract, error description, associated cause, etc can be leveraged to generate synthetic data on which the model can be fine-tuned (<a href="https://aclanthology.org/2020.emnlp-main.440/">Zhang et al., 2020</a>).</p><p>In the open-retrieval setting, dense retrieval methods (<a href="https://aclanthology.org/2020.emnlp-main.550/">Karpukhin et al., 2020</a>) trained on a source domain may not generalize in a zero-shot manner to low-resource target domains. Instead, we can leverage the same question generation methods discussed in the previous section to create silver data for training a retrieval model on the target domain (<a href="https://www.semanticscholar.org/paper/Synthetic-Target-Domain-Supervision-for-Open-QA-Reddy-Iyer/824666e5e6721bc83a1768579062077d719f7089">Reddy et al., 2021</a>). Ensembling over BM25 and the adapted DPR model yields the best results. </p><h3 id="domain-generalization">Domain Generalization</h3><p>In practice, pre-trained models fine-tuned on a single domain often generalize poorly. Training on multiple source distributions reduces the need for selecting a single source dataset (<a href="https://aclanthology.org/P19-1485/">Talmor &amp; Berant, 2019</a>). Additional fine-tuning on a related task helps even when using pre-trained models.</p><p>However, different datasets often have different formats, which makes it difficult to train a joint model for them without task-specific engineering. Recent pre-trained LMs such as T5 facilitate such cross-dataset learning as each dataset can be converted to a unified text-to-text format. Training a model jointly across multiple QA datasets with such a text-to-text format leads to better generalization to unseen domains (<a href="https://aclanthology.org/2020.findings-emnlp.171/">Khashabi et al., 2020</a>).</p><h1 id="multilingual-qa">Multilingual QA</h1><p>Language can be seen as another facet of the domain manifold. As we will see, many methods discussed in the previous section can also be successfully applied to the multilingual setting. At the same time, multilingual QA poses its own unique challenges.</p><h2 id="datasets-for-multilingual-qa">Datasets for Multilingual QA</h2><p>Many early multilingual IR and QA datasets have been collected as part of community evaluations. Many test collections used newswire articles, e.g. <a href="https://www.nist.gov/publications/clir-evaluation-trec">CLIR at TREC</a> 1994–2004, QA at CLEF 2003–2005 (<a href="https://staff.fnwi.uva.nl/m.derijke/Publications/Files/clef_qa_03_overview_proceedings.pdf">Magnini et al., 2003</a>) or Wikipedia, e.g. QA at CLEF 2006–2008 (<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.495.4731&amp;rep=rep1&amp;type=pdf">Gillard et al., 2006</a>). Such datasets focused mainly on Indo-European languages, though more recent ones also covered other languages, e.g. Hindi and Bengali at FIRE 2012 (<a href="https://www.isical.ac.in/~fire/data/working-notes_2012/adhoc/adhoc_ISM.pdf">Yadav et al., 2012</a>).</p><p>There are wide range of <strong>monolingual reading comprehension</strong> <strong>datasets</strong>, many of them variants of SQuAD (<a href="https://aclanthology.org/D16-1264/">Rajpurkar et al., 2016</a>). Most of them are available in Chinese, Russian, and French and they generally consist of naturalistic data in each language.</p><p><strong>Monolingual open-retrieval QA</strong> <strong>datasets</strong> are more diverse in nature. They differ based on the type and amount of context they provide and often focus on specialized domains, from Chinese history exams (<a href="https://aclanthology.org/C18-1038/">Zhao &amp; Zhao, 2018</a>) to Chinese maternity forums (<a href="https://aclanthology.org/2020.acl-main.330/">Xu et al., 2020</a>) and Polish 'Did you know?' questions (<a href="https://www.researchgate.net/profile/Maciej-Piasecki/publication/272685856_Open_dataset_for_development_of_Polish_Question_Answering_systems/links/560f8ff708aec422d1133caa/Open-dataset-for-development-of-Polish-Question-Answering-systems.pdf">Marcinczuk et al., 2013</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/mlqa_annotation.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>The MLQA alignment and annotation process (<a href="https://www.aclweb.org/anthology/2020.acl-main.653/">Lewis et al., 2020</a>)</figcaption></figure><p><strong>Multilingual reading comprehension</strong> <strong>datasets</strong> have generally been created using translations. MLQA (<a href="https://www.aclweb.org/anthology/2020.acl-main.653/">Lewis et al., 2020</a>), which can be seen above has been created by automatically aligning Wikipedia paragraphs across multiple languages and annotating questions and answers on the aligned paragraphs. Such automatic alignment, however, may lead to severe quality issues for some languages (<a href="https://arxiv.org/abs/2103.12028">Caswell et al., 2021</a>) and may result in overfitting to the biases of the alignment model. Another dataset, XQuAD (<a href="https://aclanthology.org/2020.acl-main.421/">Artetxe et al., 2020</a>) was created by professionally translating a subset of SQuAD to 10 other languages. Finally, MLQA-R and XQuAD-R (<a href="https://aclanthology.org/2020.emnlp-main.477/">Roy et al., 2020</a>) are conversions of the former datasets to the answer sentence retrieval setting.</p><p><strong>Multilingual open-retrieval QA</strong> <strong>datasets</strong> typically consist of naturalistic data. XQA (<a href="https://aclanthology.org/P19-1227/">Liu et al., 2019</a>) covers 'Did you know?' Wikipedia questions converted to a <a href="https://en.wikipedia.org/wiki/Cloze_test">Cloze format</a>. For each question, the top 10 Wikipedia documents ranked by BM25 are provided as context. TyDi QA (<a href="https://aclanthology.org/2020.tacl-1.30/">Clark et al., 2020</a>) ask annotators to write "information-seeking" questions based on short Wikipedia prompts in typologically diverse languages. Such information-seeking questions lead to less lexical overlap compared to RC datasets like MLQA and XQuAD and thus result in a more challenging QA setting. However, as in-language Wikipedias are used for finding context paragraphs (via Google Search) and as the Wikipedias of many under-represented languages are very small, many questions in TyDi QA are unanswerable.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/xor_tydiqa.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>The XOR-TyDi QA annotation process (<a href="https://aclanthology.org/2021.naacl-main.46/">Asai et al., 2021</a>)</figcaption></figure><p>XOR-TyDi QA (<a href="https://aclanthology.org/2021.naacl-main.46/">Asai et al., 2021</a>) addresses this issue via the above procedure, which translates unanswerable TyDi QA questions to English and retrieves context paragraphs from English Wikipedia. This strategy significantly decreases the fraction of unanswerable questions. While XOR-TyDi QA focuses on <em>cross-lingual</em> retrieval, Mr. TyDi (<a href="https://aclanthology.org/2021.mrl-1.12/">Zhang et al., 2021</a>) augments TyDi QA with in-language documents for evaluating <em>monolingual</em> retrieval models. As answers in TyDi QA are spans in a sentence, Gen-TyDi QA (<a href="https://arxiv.org/abs/2110.07150">Muller et al., 2021</a>) extends the dataset with human-generated answers to enable the evaluation of generative QA models. Finally, MKQA (<a href="https://arxiv.org/abs/2007.15207">Longpre et al., 2020</a>) translates 10k queries from Natural Questions (<a href="https://aclanthology.org/Q19-1026/">Kwiatkowski et al., 2019</a>) to 25 other languages. In addition, it augments the dataset with annotations that link directly against Wikidata entities, enabling evaluation beyond span extraction.</p><p>An emerging category of multilingual QA datasets is <strong>multilingual common sense reasoning</strong>. Such datasets consist of multiple-choice assertions that are translated into other languages (<a href="https://aclanthology.org/2020.emnlp-main.185/">Ponti et al., 2020</a>; <a href="https://aclanthology.org/2021.acl-long.102/">Lin et al., 2021</a>).</p><h3 id="issues-of-multilingual-qa-datasets">Issues of Multilingual QA Datasets</h3><p>Existing monolingual and multilingual QA datasets have some issues that one should be aware of.</p><p><strong>Language distribution</strong>  Existing datasets predominantly focus on "high-resource" languages where large amounts of data are available. Evaluating QA models on such datasets provides a distorted view of progress in the field. For instance, questions that can be solved by string matching are easy in English but much harder in morphologically rich languages (<a href="https://aclanthology.org/2020.tacl-1.30/">Clark et al., 2020</a>). Among current key applications of NLP, QA has the lowest linguistic global utility, i.e. performance averaged across the world's languages (<a href="https://arxiv.org/abs/2110.06733">Blasi et al., 2021</a>), which can be seen below. While QA datasets cover languages with many speakers, there is still a long way to go in terms of an equitable coverage across the world's languages.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/utility_nlp_applications.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>Linguistic and demographic utility of different NLP applications (<a href="https://arxiv.org/abs/2110.06733">Blasi et al., 2021</a>)</figcaption></figure><p><strong>Homogeneity  </strong>In order to make collection scalable, multilingual datasets often collect data that covers the same question or similar topics across languages, thus missing out on language-specific nuances. In addition, it is often not feasible to do an in-depth error analysis for every language. The most common source of homogeneity is translation, which carries its own biases.</p><p><strong>Limitations of translation</strong>  "Translationese" differs in many aspects from natural language (<a href="http://cs.haifa.ac.il/~shuly/publications/vered.pdf">Volanksy et al., 2015</a>). Translated questions often do not have answers in a target language Wikipedia (<a href="https://arxiv.org/abs/2103.16613">Valentim et al., 2021</a>). In addition, datasets created via translation inherit artefacts such as a large train-test overlap of answers in NQ (<a href="https://aclanthology.org/2021.eacl-main.86.pdf">Lewis et al., 2020</a>) and translation also leads to new artefacts, e.g. in NLI when premise and hypothesis are translated separately (<a href="https://aclanthology.org/2020.emnlp-main.618/">Artetxe et al., 2020</a>). Finally, translated questions differ from the types of questions "naturally" asked by speakers of different languages, leading to an English and Western-centric bias.</p><p><strong>English and Western-centric bias</strong>  Examples in many QA datasets are biased towards questions asked by English speakers. Cultures differ in what types of questions are typically asked, e.g. speakers outside the US probably would not ask about famous American football or baseball players. In COPA (<a href="https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF">Roemmele et al., 2011</a>), many referents have no language-specific terms in some languages, e.g. bowling ball, hamburger, lottery (<a href="https://aclanthology.org/2020.emnlp-main.185/">Ponti et al., 2020</a>). Common sense knowledge, social norms, taboo topics, assessments of social distance, etc are also culture-dependent (<a href="https://academic.oup.com/applij/article-abstract/4/2/91/167524?redirectedFrom=fulltext">Thomas, 1983</a>). Lastly, the common setting of training on English data leads to an overestimation of transfer performance on languages similar to English and underestimation on more distant languages.</p><p><strong>Dependence on retrieval</strong>  The standard setting of identifying a minimal span for open-domain QA in the retrieved documents benefits extractive systems. It assumes there is a single gold paragraph providing the correct answer and does not consider information from other paragraphs or pages. For unanswerable questions, answers may often be found in other pages that were not retrieved (<a href="https://aclanthology.org/2021.acl-long.118/">Asai &amp; Choi, 2021</a>).</p><p><strong>Information scarcity</strong>  Typical knowledge resources such as language-specific Wikipedias often do not contain the relevant information, particularly for under-represented languages. For such languages, datasets must necessarily be cross-lingual. In addition, some information is only available from other sources, e.g. IMDb, news articles, etc.</p><p><strong>Difficulty of multilingual comparison</strong>  Comparing a model's performance across different languages is difficult due to a range of factors such as different levels of question difficulty, different amounts and quality of monolingual data, impact of translationese, etc. Instead, it is better to perform system-level comparisons across languages.</p><p><strong>Monolingual vs multilingual QA datasets</strong>  Creating multilingual QA datasets is expensive and thus often infeasible with academic budgets. In contrast, work on monolingual QA datasets is often perceived as "niche". Such work, however, is arguably much more important and impactful than incremental modelling advances, which are commonly accepted to conferences (<a href="https://arxiv.org/abs/2107.12708">Rogers et al., 2021</a>). In order to foster inclusivity and diversity in NLP, it is key to enable and reward such work, particularly for under-represented languages. Language-specific QA datasets can go beyond a "replication" of English work by, for instance, performing analyses of language-specific phenomena and extending or improving the QA setting.</p><h3 id="creating-qa-datasets">Creating QA Datasets</h3><p><strong>Efficient multilingual QA evaluation at scale</strong>  A key challenge of multilingual QA is the lack of data for many languages. Instead of labelling large amounts of data in every language in order to cover the entire distribution, we can create targeted tests that probe for specific capabilities, for instance using CheckList (<a href="https://aclanthology.org/2020.acl-main.442/">Ribeiro et al., 2020</a>). This way, a small number of templates can cover many different model capabilities. Such template-based tests have so far been used for evaluating multilingual reading comprehension (<a href="https://aclanthology.org/2021.emnlp-main.802/">Ruder et al., 2021</a>) and closed-book QA (<a href="https://aclanthology.org/2020.emnlp-main.479.pdf">Jiang et al., 2020</a>; <a href="https://aclanthology.org/2021.eacl-main.284.pdf">Kassner et al., 2021</a>) where they enable fine-grained evaluation across languages as can be seen below. However, in order to scale such tests across languages native speaker expertise or translation are still required.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/multichecklist_evaluation.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>Error rate of mBERT (left) and XLM-R (right) fine-tuned on English SQuAD v1.1 across different languages in MultiCheckList (<a href="https://aclanthology.org/2021.emnlp-main.802/">Ruder et al., 2021</a>)&nbsp;</figcaption></figure><p><strong>Best practices</strong>  When creating a new QA dataset, it is important to focus on the research questions you want to answer with your dataset. Try to avoid creating confounding variables (translationese, morphology, syntax, etc) that obfuscate answering these questions. Consider collecting data in a typologically diverse set of languages and think about the use case of your dataset and how systems based on the data could help humans. Chose an appropriate dataset format: If you want to help people around the world answer questions, focus on information-seeking questions and avoid cultural bias. If you want to help users ask questions about a short document, focus on reading comprehension. Finally, in order to create inclusive and diverse QA datasets, it is important to work with speaker communities and conduct participatory research (<a href="https://aclanthology.org/2020.findings-emnlp.195/">∀ et al., 2020</a>).</p><h3 id="multilingual-qa-evaluation">Multilingual QA Evaluation</h3><p>Common evaluation settings in multilingual QA range from <strong>monolingual QA</strong> where all data is in the same language to <strong>cross-lingual </strong>scenarios where the question, context, and answer can be in different languages as well as <strong>zero-shot cross-lingual</strong> <strong>transfer</strong> settings where training data is in a high-resource language and test data is in another language.</p><p><strong>Evaluation metrics</strong> are based on lexical overlap using either Exact Match (EM) or mean token F1, with optional pre-processing of predictions and answers (<a href="https://www.aclweb.org/anthology/2020.acl-main.653/">Lewis et al., 2020</a>). Such token-based metrics, however, are not appropriate for languages without whitespace separation and require a language-specific segmentation method, which introduces a dependence on the evaluation setting. Furthermore, metrics based on string matching penalize morphologically rich languages as extracted spans may contain irrelevant morphemes, favour extractive over generative systems, and are biased towards short answers.</p><p>Alternatively, evaluation can be performed on the character or byte level. As standard metrics used for natural language generation (NLG) such as BLEU or ROUGE show little correlation with human judgements for some languages (<a href="https://arxiv.org/abs/2110.07150">Muller et al., 2021</a>), learned metrics based on strong pre-trained models such as BERTScore (<a href="https://openreview.net/forum?id=SkeHuCVFDr">Zhang et al., 2020</a>) or SAS (<a href="https://aclanthology.org/2021.mrqa-1.15/">Risch et al., 2021</a>) may be preferred, particularly for evaluating generative models.</p><h2 id="multilingual-qa-models">Multilingual QA Models</h2><p>Multilingual models for QA are generally based on pre-trained multilingual Transformers such as mBERT (<a href="https://aclanthology.org/N19-1423/">Devlin et al., 2019</a>), XLM-R (<a href="https://aclanthology.org/2020.acl-main.747/">Conneau et al., 2020</a>), or mT5 (<a href="https://aclanthology.org/2021.naacl-main.41/">Xue et al., 2021</a>).</p><h3 id="models-for-multilingual-reading-comprehension">Models for Multilingual Reading Comprehension</h3><p>For reading comprehension, a multilingual model is typically fine-tuned on data in English and then applied to test data in the target language via zero-shot transfer. Recent models generally perform well on high-resource languages present in standard QA datasets while performance is slightly lower for languages with different scripts (<a href="https://aclanthology.org/2021.emnlp-main.802/">Ruder et al., 2021</a>), which can be seen below. Fine-tuning the model on data in the target language using masked language modelling (MLM) before training on task-specific data generally improves performance (<a href="https://aclanthology.org/2020.emnlp-main.617/">Pfeiffer et al., 2020</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/multilingual_rc_evaluation.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>Zero-shot cross-lingual transfer performance (F1) of representative models on XQuAD (left) and MLQA (right) (<a href="https://aclanthology.org/2021.emnlp-main.802/">Ruder et al., 2021</a>)</figcaption></figure><p>In practice, fine-tuning on a few labelled target language examples can significantly improve transfer performance compared to zero-shot transfer (<a href="https://arxiv.org/abs/2003.11080">Hu et al., 2020</a>; <a href="https://aclanthology.org/2020.emnlp-main.363/">Lauscher et al., 2020</a>). However, the same does not hold for the more challenging open-retrieval QA (<a href="https://arxiv.org/abs/2110.04374">Kirstain et al., 2021</a>). Multi-task training on data in many languages improves performance further (<a href="https://aclanthology.org/2021.acl-short.79.pdf">Debnath et al., 2021</a>).</p><p>Most prior work on multilingual QA uses the <strong>translate-test</strong> setting, which translates all data into English—typically using an online MT system as a black box—and then applies a QA model trained on English to it (<a href="http://doras.dcu.ie/16441/1/Efficient_Question_Answering_with_Question_Decomposition_and_Multiple_Answer_Streams.pdf">Hartrumpf et al., 2008</a>; <a href="http://research.nii.ac.jp/~ntcadm/workshop/OnlineProceedings8/NTCIR/06-NTCIR8-CCLQA-LinC.pdf">Lin &amp; Kuo, 2010</a>; <a href="http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings8/NTCIR/04-NTCIR8-CCLQA-ShimaH.pdf">Shima &amp; Mitamura, 2010</a>). In order to map the predicted English answer to the target language, back-translation of the answer span does not work well as it is agnostic of the paragraph context. Instead, recent methods employ the attention weights from a neural MT system to align the English answer span to a span in the original document (<a href="https://arxiv.org/abs/1809.03275">Asai et al., 2018</a>; <a href="https://www.aclweb.org/anthology/2020.acl-main.653/">Lewis et al., 2020</a>).</p><p>Alternatively, in the <strong>translate-train</strong> setting the English training data is translated to the target language and a target language QA model is trained on the data. In this case, it is crucial to ensure that answer spans can be recovered after translation by enclosing them with tags or using fuzzy search (<a href="https://aclanthology.org/D19-1607/">Hsu et al., 2019</a>; <a href="https://arxiv.org/abs/2003.11080">Hu et al., 2020</a>). We can go even further and translate the English data to <em>all</em> target languages on which we train a multilingual QA model. This <strong>translate-train-all</strong> setting generally performs best for reading comprehension (<a href="https://aclanthology.org/2020.acl-main.747/">Conneau et al., 2020</a>; <a href="https://arxiv.org/abs/2003.11080">Hu et al., 2020</a>; <a href="https://aclanthology.org/2021.emnlp-main.802/">Ruder et al., 2021</a>) and achieves performance close to English on high-resource languages but lower on others. The below flowchart shows what method achieves the best performance depending on the available data.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/multilingual_rc_flowchart.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>Flowchart for multilingual reading comprehension</figcaption></figure><h3 id="models-for-multilingual-open-retrieval-qa">Models for Multilingual Open-Retrieval QA</h3><p><strong>Translate-test</strong> is the standard approach for open-retrieval QA as it only requires access to English resources. In addition to training a document reader model on English data, the open-retrieval setting also necessitates training an English retrieval model. During inference, we apply the models to the translated data and back-translate the answer to the target language, which can be seen below. However, low-quality MT may lead to error propagation and for some questions answers may not be available in the English Wikipedia. On the other hand, <strong>translate-train</strong> is generally infeasible in the open-retrieval setting as it requires translating all possible context (such as the entire Wikipedia) to the target language. As an alternative, only questions can be translated, which may outperform translate-test (<a href="https://arxiv.org/abs/2007.15207">Longpre et al., 2020</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/open-retrieval_translate-test.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>Inference with translate-test in the open-retrieval QA setup (<em>note</em>: flags are used as succinct visual representations and are not meant to reflect a particular language variety)</figcaption></figure><p>Without using translations, we need to train cross-lingual retrieval and document reader models that can assess similarity between questions and context paragraphs and answer spans respectively across languages. To this end, we need to fine-tune a pre-trained multilingual model using target language questions and English contexts. However, restricting retrieval to English documents limits the viewpoints and knowledge sources at our disposal. We would thus like to extend retrieval to documents in multiple languages.</p><p>Fine-tuning a pre-trained multilingual model to retrieve passages only on English data does not generalize well to other languages (<a href="https://arxiv.org/abs/2108.08787">Zhang et al., 2021</a>), similar to the multi-domain setting. In order to train a retrieval model that better generalizes to other languages, we can fine-tune the model on multilingual data instead (<a href="https://arxiv.org/abs/2107.11976">Asai et al., 2021</a>). Alternatively, we can employ data augmentation. Similar to the multi-domain setting, we can obtain silver data in the target language by generating synthetic target language questions, in this case using a translate-train model (<a href="https://arxiv.org/abs/2109.01628">Shi et al., 2021</a>). In addition, we can obtain weakly supervised examples using language links in Wikipedia as can be seen below (<a href="https://arxiv.org/abs/2109.01628">Shi et al., 2021</a>; <a href="https://arxiv.org/abs/2107.11976">Asai et al., 2021</a>). Specifically, we retrieve the articles corresponding to the original answer and answer paragraph in other languages and use them as new training examples. Lastly, a combination of BM25 + dense retrieval also performs best in this setting (<a href="https://arxiv.org/abs/2108.08787">Zhang et al., 2021</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/cross-lingual_data_expansion.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>Cross-lingual data expansion via Wikidata language links; based on an example by <a href="https://arxiv.org/abs/2107.11976">Asai et al. (2021)</a></figcaption></figure><p>To aggregate the retrieved passages in different languages we can train a pre-trained multilingual text-to-text model such as mT5 to generate an answer when provided with the passages as input (<a href="https://arxiv.org/abs/2110.07150">Muller et al., 2021</a>; <a href="https://arxiv.org/abs/2107.11976">Asai et al., 2021</a>). The full pipeline consisting of multilingual retrieval and answer generation models can be seen below. As the model will only learn to generate answers in languages covered by existing datasets, data augmentation is again key. Furthermore, models can be iteratively trained using newly retrieved and newly identified answers as additional training data in subsequent iterations (<a href="https://arxiv.org/abs/2107.11976">Asai et al., 2021</a>). The best models achieve strong performance in the full open-retrieval setting but there is still significant headroom left.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/mdpr_mgen.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>Multilingual retrieval and multilingual answer generation pipeline (<a href="https://arxiv.org/abs/2107.11976">Asai et al., 2021</a>)</figcaption></figure><p>Two of the most challenging aspects of multilingual open-retrieval QA are finding the paragraph containing the answer (<strong>paragraph selection</strong>) and identifying whether a document contains the answer to a query (<strong>answerability prediction</strong>; <a href="https://aclanthology.org/2021.acl-long.118/">Asai &amp; Choi, 2021</a>). A related problem is answer sentence selection where models predict whether a sentence contains the answer (<a href="https://arxiv.org/abs/1911.04118">Garg et al., 2020</a>). Unanswerability is often due to errors in document retrieval or unanswerable questions requiring multiple paragraphs to answer. To address this headroom, <a href="https://aclanthology.org/2021.acl-long.118/">Asai and Choi (2021)</a> recommend to a) go beyond using Wikipedia for retrieval; b) to improve the quality of annotated questions in existing and future datasets; and c) to move from extracting a span to generating the answer.</p><h1 id="open-research-directions">Open Research Directions</h1><p><strong>Multi-modal question answering</strong>  For many language varieties and domains, it may be easier to obtain data in other modalities. As a recent example, SD-QA (<a href="https://aclanthology.org/2021.findings-emnlp.281/">Faisal et al., 2021</a>) augments TyDi QA with spoken utterances matching the questions in four languages and multiple dialects.</p><p><strong>Other domains: time and geography</strong>  A domain can include many facets not covered in existing work. For instance, answers often depend on extra-linguistic context such as the time and location where the questions were asked. SituatedQA (<a href="https://aclanthology.org/2021.emnlp-main.586/">Zhang &amp; Choi, 2021</a>) augments context-dependent questions in Natural Questions with time and geography-dependent contexts to study such questions.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/11/situatedqa.png" class="kg-image" alt="Multi-domain Multilingual Question Answering"><figcaption>Temporal and geographical question contexts in SituatedQA (<a href="https://aclanthology.org/2021.emnlp-main.586/">Zhang &amp; Choi, 2021</a>)</figcaption></figure><p><strong>Code-switching</strong>   Code-switching is a common phenomenon in multilingual communities but mostly neglected in QA research. Only few resources exist in Bengali, Hindi, Telugu, and Tamil (<a href="http://www2015.thewebconf.org/documents/proceedings/companion/p853.pdf">Raghavi et al., 2015</a>; <a href="http://personales.upv.es/prosso/resources/BanerjeeEtAl_MultiLingMine16.pdf">Banerjee et al., 2016</a>; <a href="https://aclanthology.org/W18-3204/">Chandu et al., 2018</a>; <a href="https://aclanthology.org/W18-3205/">Gupta et al., 2018</a>). For a broader overview of code-switching, have a look at this survey (<a href="https://aclanthology.org/2021.acl-long.131.pdf">Doğruöz et al., 2021</a>).</p><p><strong>Multilingual multi-domain generalization </strong> Most open-retrieval QA datasets only cover Wikipedia while many domains important in real-world applications (e.g. tech questions) only have English QA datasets. Other domains without much data are particularly relevant in non-Western contexts, e.g. finance for small businesses, legal and health questions. In addition, currently unanswerable questions require retrieving information from a wider set of domains such as IMDb (<a href="https://arxiv.org/abs/2107.11976">Asai et al., 2021</a>). In order to create truly open-domain QA systems, we thus need to train open-retrieval QA systems to answer questions from many different domains.</p><p><strong>Data augmentation</strong>  Generation of synthetic multilingual QA data has been little explored beyond translation and retrieval (<a href="https://arxiv.org/abs/2109.01628">Shi et al., 2021</a>). The generation of data about non-Western entities may be particularly helpful.</p><p><strong>Generative question answering</strong>  In most existing QA datasets, the short answer is a span in the context. In order to train and evaluate models more effectively, more datasets need to include longer, more natural answers. The generation of long-form answers is particularly challenging, however (<a href="https://aclanthology.org/2021.naacl-main.393/">Krishna et al., 2021</a>).</p><p><strong>Aggregating evidence from diverse sources</strong>  We need to develop better aggregation methods that cover reasoning paths, e.g. for multi-hope reasoning (<a href="https://arxiv.org/abs/1911.10470">Asai et al., 2020</a>). Models also need to be able to generate answers that are faithful to the retrieved passages, requiring clear answer attribution. Finally, we require methods that can effectively combine evidence from different domains and even different modalities.</p><p><strong>Conversational question answering</strong>  Current open-retrieval QA datasets are generally single-turn and do not depend on any external context. In order to train generally useful QA systems, models should also be able to take into account conversational context as required by datasets such as QuAC (<a href="https://aclanthology.org/D18-1241/">Choi et al., 2018</a>). In particular, they should be able to handle coreference, ask for clarification regarding ambiguous questions, etc.</p><h2 id="further-reading">Further reading</h2><p>Here are some additional resources that may be useful to learn more about different aspects of the topic:</p><ul><li>a survey on <strong>question answering datasets</strong> with a particular focus on the required reasoning skills (<a href="https://arxiv.org/abs/2107.12708">Rogers et al., 2021</a>);</li><li>a survey on <strong>neural unsupervised domain adaptation</strong> in NLP (<a href="https://aclanthology.org/2020.coling-main.603/">Ramponi &amp; Plank, 2020</a>);</li><li>the <a href="https://github.com/danqi/acl2020-openqa-tutorial">ACL 2020 tutorial</a> on <strong>open-domain question answering</strong>;</li><li>and my <a href="https://tinyurl.com/xlingual">ACL 2019 tutorial</a> on <strong>cross-lingual representation learning</strong>.</li></ul><h2 id="credit">Credit</h2><p>Thanks to the following people for feedback on a draft of the tutorial slides: Jon Clark, Tim Möller, Sara Rosenthal, Md Arafat Sultan, and Benjamin Muller.</p><h2 id="citation">Citation</h2><p>If you found this post helpful, consider citing <a href="https://aclanthology.org/2021.emnlp-tutorials.4/">the tutorial</a> as:</p><pre><code>@inproceedings{ruder-sil-2021-multi,
    title = "Multi-Domain Multilingual Question Answering",
    author = "Ruder, Sebastian  and
      Sil, Avi",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic {\&amp;} Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-tutorials.4",
    pages = "17--21",
}
</code></pre>]]></content:encoded></item><item><title><![CDATA[Challenges and Opportunities in NLP Benchmarking]]></title><description><![CDATA[Recent NLP models have outpaced the benchmarks to test for them. This post provides an overview of challenges and opportunities for NLP benchmarks.]]></description><link>http://ruder.io/nlp-benchmarking/</link><guid isPermaLink="false">6116d6eaa1fdd895bf988b19</guid><category><![CDATA[natural language processing]]></category><category><![CDATA[transfer learning]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Mon, 23 Aug 2021 09:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2021/08/squad_2_progress.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2021/08/squad_2_progress.png" alt="Challenges and Opportunities in NLP Benchmarking"><p>Over the last years, models in NLP have become much more powerful, driven by <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit?usp=sharing">advances in transfer learning</a>. A consequence of this drastic increase in performance is that existing benchmarks have been left behind. Recent models "have outpaced the benchmarks to test for them" (<a href="https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report_Master.pdf#page=11">AI Index Report 2021</a>), quickly reaching super-human performance on standard benchmarks such as <a href="https://super.gluebenchmark.com/">SuperGLUE</a> and <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a>. Does this mean that we have solved natural language processing? Far from it.</p><p>However, the traditional practices for evaluating performance of NLP models, using a single metric such as accuracy or BLEU, relying on static benchmarks and abstract task formulations no longer work as well in light of models' surprisingly robust <em>superficial</em> natural language understanding ability. We thus need to rethink how we design our benchmarks and evaluate our models so that they can still serve as useful indicators of progress going forward.</p><p>This post aims to give an overview of challenges and opportunities in benchmarking in NLP, together with some general recommendations. I tried to cover perspectives from recent papers, talks at ACL 2021 as well as at the <a href="https://github.com/kwchurch/Benchmarking_past_present_future">ACL 2021 Workshop on Benchmarking: Past, Present and Future</a>, in addition to some of my own thoughts. </p><p><em>Header image: Performance on <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD 2.0</a> over time (Credit: <a href="https://paperswithcode.com/sota/question-answering-on-squad20">Papers with Code</a>)</em></p><p>Table of contents:</p><ul><li><a href="#what-is-a-benchmark">What is a benchmark?</a></li><li><a href="#a-brief-history-of-benchmarking">A brief history of benchmarking</a></li><li><a href="#metrics-matter">Metrics matter</a></li><li><a href="#consider-the-downstream-use-case">Consider the downstream use case</a></li><li><a href="#fine-grained-evaluation">Fine-grained evaluation</a></li><li><a href="#the-long-tail-of-benchmark-performance">The long tail of benchmark performance</a></li><li><a href="#large-scale-continuous-evaluation">Large-scale continuous evaluation</a></li></ul><h2 id="what-is-a-benchmark">What is a benchmark?</h2><blockquote><em>"Datasets are the telescopes of our field."</em>—<a href="https://youtu.be/t_A36DDcG_0?t=964">Aravind Joshi</a></blockquote><p>The original use of the term refers to horizontal marks made by <a href="https://en.wikipedia.org/wiki/Surveying">surveyors</a> in stone structures, into which an angle-iron could be placed to form a "bench" for a <a href="https://en.wikipedia.org/wiki/Level_staff">leveling rod</a>. Figuratively, a benchmark refers to a standard point of reference against which things can be compared. A benchmark as it is used in ML or NLP typically has several components: it consists of one or multiple datasets, one or multiple associated metrics, and a way to aggregate performance.</p><p>A benchmark sets a standard for assessing the performance of different systems that is agreed upon by the community. To ensure that a benchmark is accepted by the community, many recent benchmarks either select a representative set of standard tasks, such as <a href="https://gluebenchmark.com/">GLUE</a> or <a href="https://github.com/google-research/xtreme">XTREME</a> or actively solicit task proposals from the community, such as <a href="https://super.gluebenchmark.com/">SuperGLUE</a>, <a href="https://gem-benchmark.com/">GEM</a>, or <a href="https://github.com/google/BIG-bench">BIG-Bench</a>.</p><p>For people in the field, benchmarks are crucial tools to track progress. <a href="https://www.youtube.com/watch?t=964&amp;v=t_A36DDcG_0&amp;feature=youtu.be">Aravind Joshi</a> said that without benchmarks to assess the performance of our models, we are just like "astronomers wanting to see the stars but refusing to build telescopes".</p><p>For practitioners and outsiders, benchmarks provide an objective lens into a field that enables them to identify useful models and keep track of a field's progress. For instance, the <a href="https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report_Master.pdf#page=62">AI Index Report 2021</a> uses SuperGLUE and SQuAD as a proxy for overall progress in natural language processing.</p><p>Reaching human performance on influential benchmarks is often seen as a key milestone for a field. AlphaFold 2 reaching performance competitive with experimental methods on the CASP 14 competition marked a <a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">major scientific advance in the field of structural biology</a>.</p><h2 id="a-brief-history-of-benchmarking">A brief history of benchmarking</h2><blockquote><em>"Creating good benchmarks is harder than most imagine."</em>—John R. Mashey; foreword to <a href="https://www.springer.com/gp/book/9783030417048">Systems Benchmarking (2020)</a></blockquote><p>Benchmarks have a long history of being used to assess the <a href="https://en.wikipedia.org/wiki/Benchmark_(computing)">performance of computational systems</a>. The Standard Performance Evaluation Corporation (<a href="https://en.wikipedia.org/wiki/Standard_Performance_Evaluation_Corporation">SPEC</a>), established in 1988 is one of the oldest organisations dedicated to benchmarking the performance of computer hardware. Crucially, SPEC had support from most important companies in the field. Every year, it would release different benchmark sets, each composed of multiple programs, with performance measured as the <a href="https://en.wikipedia.org/wiki/Geometric_mean">geometric mean</a> of millions of instructions per second (<a href="https://en.wikipedia.org/wiki/Instructions_per_second">MIPS</a>).</p><p>A recent ML-specific analogue to SPEC is <a href="https://mlcommons.org/en/">MLCommons</a>, which organises the <a href="https://mlcommons.org/en/training-normal-10/">MLPerf</a> series of performance benchmarks focusing on model training and inference. Similar to SPEC, MLPerf has a broad base of support from academia and industry, building on previous individual efforts for measuring performance such as <a href="https://github.com/baidu-research/DeepBench">DeepBench</a> by Baidu or <a href="https://dawn.cs.stanford.edu/benchmark/">DAWNBench</a> by Stanford. </p><p>For US agencies such as <a href="https://en.wikipedia.org/wiki/DARPA">DARPA</a> and <a href="https://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology">NIST</a>, benchmarks played a crucial role in measuring and tracking scientific progress. Early benchmarks for automatic speech recognition (ASR) such as <a href="https://en.wikipedia.org/wiki/TIMIT">TIMIT</a> and <a href="https://catalog.ldc.upenn.edu/LDC97S62">Switchboard</a> were funded by DARPA and coordinated by <a href="https://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology">NIST</a> starting in 1986. Later influential benchmarks in other areas of ML such as <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> were also based on NIST data. </p><p>For language technology and information retrieval (IR), NIST ran the DARPA-funded <a href="https://en.wikipedia.org/wiki/Text_Retrieval_Conference">TREC</a> series of workshops covering a wide array of tracks and topics, which can be seen below. TREC organised competitions built on an evaluation paradigm pioneered by <a href="https://en.wikipedia.org/wiki/Cranfield_experiments">Cranfield</a> in the 1960s where models are evaluated based on a set of test collections, consisting of documents, questions, and human relevance judgements. As the variance in performance across topics is large, scores are averaged over many topics. TREC's "standard, widely available, and carefully constructed set of data laid the groundwork for further innovation" (<a href="https://googleblog.blogspot.com/2008/03/why-data-matters.html">Varian, 2008</a>) in IR.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/08/trec_series.png" class="kg-image" alt="Challenges and Opportunities in NLP Benchmarking"><figcaption>Tasks and topics in the TREC workshops from 1992–2020 (Credit: <a href="https://www.nist.gov/video/coopetition-ir-research-presented-ellen-voorhees">Ellen Voorhees</a>)</figcaption></figure><p>Many recent influential benchmarks such as <a href="https://en.wikipedia.org/wiki/ImageNet">ImageNet</a>, <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a>, or <a href="https://aclanthology.org/D15-1075/">SNLI</a> are large in scale, consisting of hundreds of thousands of examples and were developed by academic groups at well-funded universities. In the era of deep learning, such large-scale datasets have been credited as one of the pillars driving progress in research, with fields such as NLP or <a href="https://twitter.com/OriolVinyalsML/status/1333436710303264772?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1333436710303264772%7Ctwgr%5E%7Ctwcon%5Es1_c10&amp;ref_url=https%3A%2F%2Fsyncedreview.com%2F2020%2F11%2F30%2Fbiologys-imagenet-moment-deepmind-says-its-alphafold-has-cracked-a-50-year-old-biology-challenge%2F">biology</a> witnessing their <a href="https://thegradient.pub/nlp-imagenet/">'ImageNet moment'</a>. </p><p>As models have become more powerful and general-purpose, benchmarks have become more application-oriented and increasingly moved from single-task to multi-task and single-domain to multi-domain benchmarks. Key examples of these trends are a transition from a focus on core linguistic tasks such as <a href="http://nlpprogress.com/english/part-of-speech_tagging.html">part-of-speech tagging</a> and <a href="http://nlpprogress.com/english/dependency_parsing.html">dependency parsing</a> to tasks that are closer to the real-world such as goal-oriented dialogue and open-domain question answering (<a href="https://research.google/pubs/pub47761/">Kwiatkowski et al., 2019</a>); the emergence of multi-task datasets such as <a href="https://gluebenchmark.com/">GLUE</a>; and multi-modality datasets such as <a href="https://wilds.stanford.edu/">WILDS</a>.</p><p>However, while it took more than 15 years to achieve superhuman performance on classic benchmarks such as MNIST or Switchboard, models have achieved superhuman performance on more recent benchmarks such as GLUE and SQuAD 2.0 within about a year of their release, as can be seen in the figure below. At the same time, we know that the capabilities that these benchmarks aim to test, such as general question answering are far from being solved.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/08/dynabench_plot.png" class="kg-image" alt="Challenges and Opportunities in NLP Benchmarking"><figcaption>Benchmark saturation over time for popular benchmarks. Initial performance and human performance are normalised to -1 and 0 respectively (<a href="https://aclanthology.org/2021.naacl-main.324.pdf">Kiela et al., 2021</a>).</figcaption></figure><p>Another factor that has contributed to the saturation of these benchmarks is that limitations and annotation artefacts of recent datasets have been identified much more quickly compared to earlier benchmarks. In SNLI, annotators have been shown to rely on heuristics, which allow models to make the correct prediction in many cases using the hypothesis alone (<a href="https://aclanthology.org/N18-2017.pdf">Gururangan et al., 2018</a>) while models trained on SQuAD are subject to adversarially inserted sentences (<a href="https://aclanthology.org/D17-1215/">Jia and Liang, 2017</a>).</p><p>A recent trend is the development of adversarial datasets such as Adversarial NLI (<a href="https://aclanthology.org/2020.acl-main.441/">Nie et al., 2020</a>), Beat the AI (<a href="https://aclanthology.org/2020.tacl-1.43.pdf">Bartolo et al., 2020</a>), and others where examples are created to be difficult for current models. <a href="https://dynabench.org/">Dynabench</a> (<a href="https://aclanthology.org/2021.naacl-main.324.pdf">Kiela et al., 2021</a>), a recent open-source platform has been designed to facilitate the creation of such datasets. A benefit of such benchmarks is that they can be dynamically updated to be challenging as new models emerge and consequently do not saturate as easily as static benchmarks.</p><h2 id="metrics-matter">Metrics matter</h2><blockquote><em>"When you can measure what you are speaking of and express it in numbers, you know that on which you are discussing. But when you cannot measure it and express it in numbers, your knowledge is of a very meagre and unsatisfactory kind."</em>—Lord Kelvin</blockquote><p>When it comes to measuring performance, metrics play an important and often under-appreciated role. For classification tasks, accuracy or <a href="https://en.wikipedia.org/wiki/F-score">F-score</a> metrics may seem like the obvious choice but—depending on the application—different types of errors incur different costs. For fine-grained sentiment analysis, confusing between <em>positive</em> and <em>very positive</em> may not be problematic while mixing up <em>very positive</em> and <em>very negative</em> is. Chris Potts highlights an <a href="https://youtu.be/t_A36DDcG_0?t=1824">array of practical examples</a> where metrics like F-score fall short, many in scenarios where errors are much more costly.</p><!--kg-card-begin: markdown--><p>Designing a good metric requires domain expertise. <a href="https://mlcommons.org/en/training-normal-10/">MLPerf</a> measures the wallclock time required to train a model to a dataset-specific performance target, a measure informed by both end use cases and the difficulty to compare other efficiency metrics such as FLOPS across models. In ASR, only the percentage of correctly transcribed words (akin to accuracy) was initially used as the metric. The community later settled on <a href="https://en.wikipedia.org/wiki/Word_error_rate">word error rate</a>, i.e. $\frac{\text{substitutions} + \text{deletions} + \text{insertions}}{\text{number of words in reference}}$ as it directly reflects the cost of correcting transcription errors.</p>
<!--kg-card-end: markdown--><p>There is a large difference between metrics designed for decades-long research and metrics designed for near-term development of practical applications, as highlighted by <a href="https://youtu.be/a-ukPup8gKw?t=770">Mark Liberman</a>. For developing decade-scale technology, we need efficient metrics that can be crude as long as they point in the general direction of our distant goal. Examples of such metrics are the word error rate in ASR (which assumes that all words are equally important) and BLEU in machine translation (which assumes that word order is not important). In contrast, for the evaluation of practical technology we need metrics that are designed with the requirements of specific applications in mind and that can consider different types of error classes.</p><p>The rapid increase in model performance in recent years has catapulted us from the decade-long to the near-term regime for many applications. However, even in this more application-oriented setting we are still relying on the same metrics that we have used to measure long-term research progress thus far. In a recent meta-analysis, <a href="https://aclanthology.org/2021.acl-long.566.pdf">Marie et al. (2021)</a> found that 82% papers of machine translation (MT) papers between 2019–2020 only evaluate on BLEU—despite 108 alternative metrics having been proposed for MT evaluation in the last decade, many of which correlate better with human judgements. As models become stronger, metrics like BLEU are no longer able to accurately identify and compare the best-performing models.</p><p>While evaluation of natural language generation (NLG) models is notoriously difficult, standard n-gram overlap-based metrics such as ROUGE or BLEU are furthermore less suited to languages with rich morphology, which will be assigned relatively lower scores.</p><p>A recent trend in NLG is towards the development of automatic metrics such as BERTScore (<a href="https://openreview.net/forum?id=SkeHuCVFDr">Zhang et al., 2020</a>) that leverage the power of large pre-trained models. A recent modification of this method makes it more suitable for near-term MT evaluation by assigning larger weights to more difficult tokens, i.e. tokens that are translated correctly only by few MT systems (<a href="https://aclanthology.org/2021.acl-short.5.pdf">Zhan et al., 2021</a>).</p><p>In order to continue to make progress, we need to be able to update and refine our metrics, to replace efficient simplified metrics with application-specific ones. The recent <a href="https://gem-benchmark.com/">GEM benchmark</a>, for instance, explicitly includes <a href="https://github.com/GEM-benchmark/GEM-metrics">metrics</a> as a component that should be improved over time, as can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/08/opportunities_of_benchmarks.png" class="kg-image" alt="Challenges and Opportunities in NLP Benchmarking"><figcaption>Opportunities (circle) and challenges of benchmark evaluation (<a href="https://aclanthology.org/2021.gem-1.10.pdf">Gehrmann et al., 2021</a>).</figcaption></figure><p><strong>Recommendations:</strong></p><ol><li>Consider metrics that are better suited to the downstream task and language.</li><li>Consider metrics that highlight the trade-offs of the downstream setting.</li><li>Update and refine metrics over time.</li></ol><h2 id="consider-the-downstream-use-case">Consider the downstream use case</h2><blockquote><em>"[...] benchmarks shape a field, for better or worse. Good benchmarks are in alignment with real applications, but bad benchmarks are not, forcing engineers to choose between making changes that help end users or making changes that only help with marketing."</em>—David A. Patterson; foreword to <a href="https://www.springer.com/gp/book/9783030417048">Systems Benchmarking (2020)</a></blockquote><p>NLP technology is increasingly used in many real-world application areas, <a href="https://youtu.be/t_A36DDcG_0?t=278">from creative self-expression to fraud detection and recommendation</a>. It is thus key to design benchmarks with such real-world settings in mind. </p><p>A benchmark's data composition and evaluation protocol should reflect the real-world use case, as highlighted by <a href="https://youtu.be/Jb6G25ZJ-VU?t=307">Ido Dagan</a>. For relation classification, for instance, the <a href="https://aclanthology.org/D18-1514/">FewRel</a> dataset <a href="https://youtu.be/Jb6G25ZJ-VU?t=657">lacks some important realistic properties</a>, which <a href="https://arxiv.org/abs/2104.08481">Few-shot TACRED</a> addresses. For binary sentiment classification on the <a href="https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf">IMDb dataset</a>, only highly polarised positive and negative reviews are considered and labels are exactly balanced. In information retrieval, retrieving relevant before non-relevant documents is <a href="https://www.nist.gov/video/coopetition-ir-research-presented-ellen-voorhees">necessary <em>but not sufficient</em> for real-world usage</a>.</p><p>As a first rule of social responsibility for NLP, <a href="https://youtu.be/t_A36DDcG_0?t=661">Chris Potts proposes</a> "Do exactly what you said you would do". As researchers in the field, we should communicate clearly what performance on a benchmark reflects and how this corresponds to real-world settings. In a similar vein, <a href="https://aclanthology.org/2021.naacl-main.385.pdf">Bowman and Dahl (2021)</a> argue that good performance on a benchmark should imply robust in-domain performance on the task. </p><p>However, the real-world application of a task may confront the model with data different from its training distribution. It is thus key to assess the robustness of the model and how well it generalises to such out-of-distribution data, including data with a temporal shift and data from other language varieties.</p><p>In light of the limited linguistic diversity in NLP research (<a href="https://aclanthology.org/2020.acl-main.560/">Joshi et al., 2020</a>), it is furthermore crucial not to treat English as the singular language for evaluation. When designing a benchmark, collecting—at a minimum—test data in other languages may help to highlight new challenges and promote language inclusion. Similarly, when evaluating models, leveraging the increasing number of non-English language datasets in tasks such as <a href="https://nlpprogress.com/">question answering</a> and summarisation (<a href="https://aclanthology.org/2021.findings-acl.413/">Hasan et al., 2021</a>) can provide additional evidence of a model's versatility.</p><p>Ultimately, considering the challenges of current and future real-world applications of language technology may provide inspiration for many new evaluations and benchmarks. Benchmarks are among the most impactful artefacts of our field and often lead to entirely new research directions, so it is crucial for them to reflect real-world and potentially ambitious use cases of our technology.</p><p><strong>Recommendations:</strong></p><ol><li>Design the benchmark and its evaluation so that it reflects the real-world use case.</li><li>Evaluate in-domain and out-of-domain generalisation.</li><li>Collect data and evaluate models on other languages.</li><li>Take inspiration from real-world applications of language technology.</li></ol><h2 id="fine-grained-evaluation">Fine-grained evaluation</h2><blockquote><em>"No matter how much people want performance to be a single number, even the <strong>right</strong> mean with no distribution can be misleading, and the <strong>wrong</strong> mean certainly is no better."</em>—John R. Mashey</blockquote><p>The downstream use case of technology should also inform the metrics we use for evaluation. In particular, for downstream applications often not a single metric but an array of constraints need to be considered. Rada Mihalcea <a href="https://youtu.be/FXCSWvIsdEE?t=1226">calls for moving away from just focusing on accuracy</a> and to focus on other important aspects of real-world scenarios. What is important in a particular setting, in other words, the utility of an NLP system, ultimately depends on the requirements of each individual user (<a href="https://aclanthology.org/2020.emnlp-main.393/">Ethayarajh and Jurafsky, 2020</a>). </p><p>Societal needs have generally not been emphasised in machine learning research (<a href="https://arxiv.org/abs/2106.15590">Birhane et al., 2021</a>). However, for real-world applications it is particularly crucial that a model does not exhibit any harmful social biases. Testing for such biases in a task-specific manner should thus become a standard part of algorithm development and model evaluation.</p><p>Another aspect that is important for practical applications is efficiency. Depending on the application, this can relate to both sample efficiency, <a href="https://en.wikipedia.org/wiki/FLOPS">FLOPS</a>, and memory constraints. Evaluating models in resource-constrained settings can often lead to new research directions. For instance, the EfficientQA competition (<a href="https://arxiv.org/abs/2101.00133">Min et al., 2020</a>) at NeurIPS 2020 demonstrated the benefits of <a href="https://ruder.io/research-highlights-2020/#2-retrieval-augmentation">retrieval augmentation</a> and large collections of weakly supervised question–answer pairs (<a href="https://arxiv.org/abs/2102.07033">Lewis et al., 2021</a>).</p><p>In order to better understand the strengths and weaknesses of our models, we furthermore require more fine-grained evaluation across a <em>single</em> metric, highlighting on what types of examples models excel and fail at. <a href="http://explainaboard.nlpedia.ai/">ExplainaBoard</a> (<a href="https://aclanthology.org/2021.acl-demo.34/">Liu et al., 2021</a>) implements such a fine-grained breakdown of model performance across different tasks, which can be seen below. Another way to obtain a more fine-grained estimate of model performance is to create test cases for specific phenomena and model behaviour, for instance using the CheckList framework (<a href="https://aclanthology.org/2020.acl-main.442/">Ribeiro et al., 2020</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/08/explainaboard_leaderboard-1.png" class="kg-image" alt="Challenges and Opportunities in NLP Benchmarking"><figcaption>The <a href="http://explainaboard.nlpedia.ai/leaderboard/task-ner/index.php">ExplainaBoard interface for the CoNLL-2003 NER dataset</a> for the three best systems including single-system analyses for the best system (A), pairwise analysis results for the top-2 systems (B), a common error table (C), and combined results (D) (<a href="https://aclanthology.org/2021.acl-demo.34.pdf">Liu et al., 2021</a>).</figcaption></figure><!--kg-card-begin: markdown--><p>As individual metrics can be flawed, it is key to evaluate across multiple metrics. When evaluating on multiple metrics, scores are typically averaged to obtain a single score. A single score is useful to compare models at a glance and provides people outside the community a clear way to assess model performance. However, using the arithmetic mean is not appropriate for all purposes. SPEC used the geometric mean, $\sqrt[\leftroot{-2}\uproot{2}n]{x_1 x_2 \ldots x_n}$, which is useful when aggregating values that are exponential in nature, such as runtimes.</p>
<!--kg-card-end: markdown--><p>An alternative is to use a weighted sum and to enable the user to define their own weights for each component. <a href="https://dynabench.org/">DynaBench</a> uses such a dynamic weighting to weight the importance of model performance but also consider model throughput, memory consumption, fairness, and robustness, which enables users to effectively define their own leaderboard (<a href="https://aclanthology.org/2020.emnlp-main.393/">Ethayarajh and Jurafsky, 2020</a>), as can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/08/dynabench_weighting.png" class="kg-image" alt="Challenges and Opportunities in NLP Benchmarking"><figcaption>Dynamic metric weighting in the <a href="https://dynabench.org/tasks/nli">DynaBench natural language inference task leaderboard</a></figcaption></figure><p><strong>Recommendations:</strong></p><ol><li>Move away from using a single metric for performance evaluation.</li><li>Evaluate social bias and efficiency.</li><li>Perform a fine-grained evaluation of models.</li><li>Consider how to aggregate multiple metrics. </li></ol><h2 id="the-long-tail-of-benchmark-performance">The long tail of benchmark performance</h2><p>Given that current models perform surprisingly well on in-distribution examples, it is time to shift our attention to the tail of the distribution, to outliers and atypical examples. Rather than considering only the average case, we should care more about the worst case and subsets of our data where our models perform the worst.</p><p>As models become more powerful, the fraction of examples where the performance of models differs and that thus will be able to differentiate between strong and the best models will grow smaller. To ensure that evaluation on this long tail of examples is reliable, benchmarks need to be large enough so that small differences in performance can be detected. It is important to note that larger models are not uniformly better across all examples (<a href="https://aclanthology.org/2021.findings-acl.334.pdf">Zhong et al., 2021</a>).</p><p>As an alternative, we can develop mechanisms that allow us to identify the best systems with few examples. This is particularly crucial in settings where assessing performance of many systems is expensive, such as in human evaluation for natural language generation. <a href="https://aclanthology.org/2021.acl-long.242/">Mendonça et al. (2021)</a> frame this as an online learning problem in the context of MT.</p><p>Benchmarks can also focus on annotating examples that are much more challenging. This is the direction taken by recent adversarial benchmarks (<a href="https://aclanthology.org/2021.naacl-main.324.pdf">Kiela et al., 2021</a>). Such benchmarks, as long as they are not biased towards a specific model, can be a useful complement to regular benchmarks that sample from the natural distribution. These directions benefit from the development of <em>active </em>evaluation methods to identify or generate the most salient and discriminative examples to assess model performance as well as interpretability methods to allow annotators to better understand models' decision boundaries.</p><p>As the budget (and thus size) of benchmarks generally remains constant, statistical significance testing becomes even more important as it enables us to reliably detect qualitatively relevant performance differences between systems. </p><p>In order to perform reliable comparisons, the benchmark's annotations should be correct and reliable. However, as models become more powerful, many instances of what look like model errors may be genuine examples of ambiguity in the data. <a href="https://aclanthology.org/2021.naacl-main.385.pdf">Bowman and Dahl (2021)</a> highlight how a model may exploit clues about such disagreements to reach super-human performance on a benchmark.</p><p>If possible, benchmarks should aim to collect multiple annotations to identify ambiguous examples. Such information may provide a useful learning signal (<a href="https://aclanthology.org/P14-2083.pdf">Plank et al., 2014</a>) and can be helpful for error analysis. In light of such ambiguity, it is even more important to report standard metrics such as inter-annotator agreement as this provides a ceiling for a model's performance on a benchmark.</p><p><strong>Recommendations:</strong></p><ol><li>Include many and/or hard examples in the benchmark.</li><li>Conduct statistical significance testing.</li><li>Collect multiple annotations for ambiguous examples.</li><li>Report inter-annotator agreement.</li></ol><h2 id="large-scale-continuous-evaluation">Large-scale continuous evaluation</h2><blockquote><em>"When a measure becomes a target, it ceases to be a good measure."</em>—Goodhart's law</blockquote><p>Multi-task benchmarks such as GLUE have become key indicators of progress but such static benchmark collections quickly become outdated. Modelling advances generally also do not lead to uniform progress across tasks. While models have achieved super-human performance on most GLUE tasks, a gap to 5-way human agreement remains on some tasks such as CoLA (<a href="https://aclanthology.org/P19-1449.pdf">Nangia and Bowman, 2019</a>). On <a href="https://sites.research.google/xtreme/">XTREME</a>, models have improved much more on cross-lingual retrieval.</p><p>In light of the fast pace of model improvements, we are in need of more nimble mechanisms for model evaluation. Specifically, beyond dynamic <em>single-task</em> evaluations such as <a href="https://dynabench.org/">DynaBench</a>, it would be useful to define a dynamic <em>collection</em> of benchmark datasets on which models have not reached human performance. This collection should be managed by the community, with datasets removed or down-weighted as models reach human performance and new challenging datasets being regularly added. Such a collection needs to be versioned, to enable updates beyond the cycle of academic review and to enable replicability and comparison to prior approaches.</p><p>Existing multi-task benchmarks such as <a href="https://gem-benchmark.com/">GEM</a> (<a href="https://arxiv.org/abs/2102.01672">Gehrmann et al., 2021</a>), which explicitly aims to be a 'living' benchmark, generally include around 10–15 different tasks. Rather than limiting the benchmark to a small collection of representative tasks, in light of the number of new datasets constantly being released, it might be more useful to include a larger cross-section of NLP tasks. Given the diverse nature of tasks in NLP, this would provide a more robust and up-to-date evaluation of model performance. <a href="https://www.luge.ai/">LUGE</a> by Baidu is a step towards such a large collection of tasks for Chinese natural language processing, currently consisting of 28 datasets.</p><p>The collection of tasks can be broken down in various ways, providing more a fine-grained assessment of model capabilities. Such a breakdown may be particularly insightful if tasks or subsets of task data are categorised according to the behaviour they are testing. <a href="https://github.com/google/BIG-bench">BIG-Bench</a>, a recent collaborative benchmark for language model probing includes a <a href="https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/keywords_to_tasks.md">categorisation by keyword</a>.</p><p>A key challenge for such large-scale multi-task evaluation is accessibility. Tasks need to be available in a common input format so that they can be run easily. In addition, tasks should be efficient to run or alternatively infrastructure needs to be available to run tasks even without much compute.</p><p>Another point of consideration is that such a collection favours large general-purpose models, which are generally trained by deep-pocketed companies or institutions. Such models, however, are already used as the starting point for most current research efforts and can be—once trained—more efficiently used via <a href="https://ruder.io/recent-advances-lm-fine-tuning/#behavioural-fine-tuning">fine-tuning</a>, distillation, or pruning.</p><p><strong>Recommendations:</strong></p><ol><li>Consider collecting and evaluating on a large, diverse, versioned collection of NLP tasks.</li></ol><h2 id="conclusion">Conclusion</h2><p>In order to keep up with advances in modelling, we need to revisit many tacitly accepted benchmarking practices such as relying on simplistic metrics like F1-score and BLEU. To this end, we should take inspiration from real-world applications of language technology and consider the constraints and requirements that such settings pose for our models. We should also care more about the long tail of the distribution as that is where improvements will be observed for many applications. Lastly, we should be more rigorous in the evaluation on our models and rely on multiple metrics and statistical significance testing, contrary to <a href="https://aclanthology.org/2021.acl-long.566.pdf">current trends</a>.</p><h2 id="citation">Citation</h2><p>For attribution in academic contexts, please cite this work as:</p><pre><code>@misc{ruder2021benchmarking,
  author = {Ruder, Sebastian},
  title = {{Challenges and Opportunities in NLP Benchmarking}},
  year = {2021},
  howpublished = {\url{http://ruder.io/nlp-benchmarking}},
}</code></pre>]]></content:encoded></item><item><title><![CDATA[Recent Advances in Language Model Fine-tuning]]></title><description><![CDATA[This article provides an overview of recent methods to fine-tune large pre-trained language models.]]></description><link>http://ruder.io/recent-advances-lm-fine-tuning/</link><guid isPermaLink="false">60226bf5a96736463841e6a2</guid><category><![CDATA[language models]]></category><category><![CDATA[natural language processing]]></category><category><![CDATA[transfer learning]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Wed, 24 Feb 2021 09:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2021/02/fine-tuning_methods.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2021/02/fine-tuning_methods.png" alt="Recent Advances in Language Model Fine-tuning"><p>Fine-tuning a pre-trained language model (LM) has become the de facto standard for doing transfer learning in natural language processing. Over the last three years (<a href="https://thegradient.pub/nlp-imagenet/">Ruder, 2018</a>), fine-tuning (<a href="https://www.aclweb.org/anthology/P18-1031/">Howard &amp; Ruder, 2018</a>) has superseded the use of feature extraction of pre-trained embeddings (<a href="https://www.aclweb.org/anthology/N18-1202/">Peters et al., 2018</a>) while pre-trained language models are favoured over models trained on translation (<a href="https://papers.nips.cc/paper/2017/file/20c86a628232a67e7bd46f76fba7ce12-Paper.pdf">McCann et al., 2018</a>), natural language inference (<a href="https://www.aclweb.org/anthology/D17-1070/">Conneau et al., 2017</a>), and other tasks due to their increased sample efficiency and performance (<a href="https://www.aclweb.org/anthology/W18-5448/">Zhang and Bowman, 2018</a>). The empirical success of these methods has led to the development of ever larger models (<a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>; <a href="https://jmlr.org/papers/v21/20-074.html">Raffel et al., 2020</a>). Recent models are so large in fact that they can achieve reasonable performance <em>without</em> any parameter updates (<a href="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown et al., 2020</a>). The limitations of this zero-shot setting (see <a href="#text-to-text-fine-tuning">this section</a>), however, make it likely that in order to achieve the best performance or stay reasonably efficient, <strong>fine-tuning will continue to be the modus operandi when using large pre-trained LMs in practice.</strong></p><p>In the standard transfer learning setup (see below; see <a href="https://ruder.io/state-of-transfer-learning-in-nlp/">this post</a> for a general overview), a model is first pre-trained on large amounts of unlabelled data using a language modelling loss such as masked language modelling (MLM; <a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>). The pre-trained model is then fine-tuned on labelled data of a downstream task using a standard cross-entropy loss.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/02/pretraining_finetuning.png" class="kg-image" alt="Recent Advances in Language Model Fine-tuning"><figcaption>The standard pre-training—fine-tuning setting (adapted from <a href="https://tiny.cc/NAACLTransfer">(Ruder et al., 2019)</a>)</figcaption></figure><p>While pre-training is compute-intensive, fine-tuning can be done comparatively inexpensively. Fine-tuning is more important for the practical usage of such models as individual pre-trained models are downloaded—and fine-tuned—millions of times (see <a href="https://huggingface.co/models?sort=downloads">the Hugging Face models repository</a>). Consequently, fine-tuning is the main focus of this post. In particular, I will highlight the most recent advances that have shaped or are likely to change the way we fine-tune language models, which can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/02/fine-tuning_methods_overview.png" class="kg-image" alt="Recent Advances in Language Model Fine-tuning"><figcaption>Overview of fine-tuning methods discussed in this post.</figcaption></figure><h2 id="adaptive-fine-tuning">Adaptive fine-tuning</h2><p>Even though pre-trained language models are more robust in terms of out-of-distribution generalisation than previous models (<a href="https://www.aclweb.org/anthology/2020.acl-main.244/">Hendrycks et al., 2020</a>), they are still poorly equipped to deal with data that is substantially different from the one they have been pre-trained on. Adaptive fine-tuning is a way to bridge such a shift in distribution by fine-tuning the model on data that is closer to the distribution of the target data. Specifically, adaptive fine-tuning involves fine-tuning the model on additional data prior to task-specific fine-tuning, which can be seen below. Importantly, the model is fine-tuned with the pre-training objective, so adaptive fine-tuning only requires unlabelled data.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/02/adaptive_fine-tuning-1.png" class="kg-image" alt="Recent Advances in Language Model Fine-tuning"><figcaption>Adaptive fine-tuning as part of the standard transfer learning setting. A pre-trained model is trained with the pre-training loss (typically masked language modelling) on data that is closer to the target distribution.</figcaption></figure><!--kg-card-begin: markdown--><p>Formally, given a target domain $\mathcal{D}_T$ consisting of a feature space $\mathcal{X}$ and a marginal probability distribution over the feature space $P(X)$ where $X = \{x_1, \ldots, x_n \} \in \mathcal{X}$ (<a href="https://ieeexplore.ieee.org/document/5288526">Pan and Yang, 2009</a>; <a href="https://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=62">Ruder, 2019</a>), adaptive fine-tuning allows us to learn about both the feature space $\mathcal{X}$ and the distribution of the target data $P(X)$.</p>
<!--kg-card-end: markdown--><p>Variants of adaptive fine-tuning—domain, task, and language-adaptive fine-tuning—have been used to adapt a model to data of the target domain, target task, and target language respectively. <a href="https://papers.nips.cc/paper/2015/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf">Dai and Le (2015)</a> first showed the benefits of domain-adaptive fine-tuning. <a href="https://www.aclweb.org/anthology/P18-1031/">Howard and Ruder (2018)</a> later demonstrated improved sample efficiency by fine-tuning on in-domain data as part of ULMFiT. They also proposed task-adaptive fine-tuning, which fine-tunes the model with the pre-training objective on the task training data. As the pre-training loss provides richer information for modelling the target data compared to the cross-entropy over one-hot task labels, task-adaptive fine-tuning is useful beyond regular fine-tuning. Alternatively, adaptive and regular fine-tuning can be done jointly via multi-task learning (<a href="https://www.aclweb.org/anthology/N19-1213/">Chronopoulou et al., 2019</a>).</p><p>Domain and task-adaptive fine-tuning have recently been applied to the latest generation of pre-trained models (<a href="https://www.aclweb.org/anthology/P19-1335/">Logeswaran et al., 2019</a>; <a href="https://www.aclweb.org/anthology/D19-1433/">Han and Eisenstein, 2019</a>; <a href="https://www.aclweb.org/anthology/P19-1373/">Mehri et al., 2019</a>). <a href="https://www.aclweb.org/anthology/2020.acl-main.740/">Gururangan et al. (2020)</a> show that adapting to data of the target domain and target task are complementary. Recently, <a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/">Pfeiffer et al. (2020)</a> proposed language-adaptive fine-tuning to adapt a model to new languages.</p><p>An adaptively fine-tuned model is specialised to a particular data distribution, which it will be able to model well. However, this comes at the expense of its ability to be a general model of language. <strong>Adaptive fine-tuning is thus most useful when high performance on (potentially multiple) tasks of a single domain is important</strong> and can be computationally inefficient if a pre-trained model should be adapted to a large number of domains.</p><h2 id="behavioural-fine-tuning">Behavioural fine-tuning</h2><!--kg-card-begin: markdown--><p>While adaptive fine-tuning enables us to specialise our model to $\mathcal{D}_T$, it does not teach us anything directly about the target task. Formally, a target task $\mathcal{T}_T$ consists of a label space $\mathcal{Y}$, a prior distribution $P(Y)$ where $Y = \{y_1, \ldots, y_n \} \in \mathcal{Y}$, and a conditional probability distribution $P(Y | X)$. Alternatively, we can teach a model capabilities useful for doing well on the target task by fine-tuning it on relevant tasks, as can be seen below. We will refer to this setting as <em>behavioural fine-tuning</em> as it focuses on learning useful behaviours and to distinguish it from adaptive fine-tuning.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/02/behavioural_fine-tuning.png" class="kg-image" alt="Recent Advances in Language Model Fine-tuning"><figcaption>Behavioural fine-tuning of a pre-trained model. The pre-trained model is trained with task-specific supervised or self-supervised objectives on tasks that are relevant for the target task.</figcaption></figure><p>One way to teach a model relevant capabilities is to fine-tune it on relevant labelled data of a related task prior to task-specific fine-tuning (<a href="https://arxiv.org/abs/1811.01088">Phang et al., 2018</a>). This so-called intermediate-task training works best with tasks that require high-level inference and reasoning capabilities (<a href="https://www.aclweb.org/anthology/2020.acl-main.467/">Pruksachatkun et al., 2020</a>; <a href="https://www.aclweb.org/anthology/2020.aacl-main.56/">Phang et al., 2020</a>). Behavioural fine-tuning with labelled data has been used to teach a model information about named entities (<a href="https://www.aclweb.org/anthology/K19-1063/">Broscheit, 2019)</a>, paraphrasing (<a href="https://www.aclweb.org/anthology/D19-1542/">Arase and Tsujii, 2019</a>), syntax (<a href="https://arxiv.org/abs/2008.06788">Glavaš and Vulić, 2020</a>), answer sentence selection (<a href="https://arxiv.org/abs/1911.04118">Garg et al., 2020</a>), and question answering (<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.171/">Khashabi et al., 2020)</a>. <a href="https://arxiv.org/abs/2101.11038">Aghajanyan et al. (2021)</a> fine-tune on around 50 labelled datasets in a massively multi-task setting and observe that a large, diverse collection of tasks is important for good transfer performance. </p><p>As supervised data of such high-level reasoning tasks is generally hard to obtain, we can instead train on objectives that teach the model capabilities that are relevant for the downstream task but which can still be learned in a self-supervised manner. For instance, <a href="https://arxiv.org/abs/2101.08231">Dou and Neubig (2021)</a> fine-tune a model for word alignment with an objective that teaches it to identify parallel sentences, among others. <a href="https://www.aclweb.org/anthology/2020.acl-main.704/">Sellam et al. (2020)</a> fine-tune BERT for quality evaluation with a range of sentence similarity signals. In both cases, a diversity of learning signals is important.</p><p>Another effective way is to frame the target task as a form of masked language modelling. To this end, <a href="https://www.aclweb.org/anthology/2020.tacl-1.33/">Ben-David et al. (2020)</a> fine-tune a model for sentiment domain adaptation with a pivot-based objective. Others propose pre-training objectives, which can be used similarly during fine-tuning: <a href="https://arxiv.org/abs/2101.00438">Ram et al. (2021)</a> pre-train a model for QA with a span selection task while <a href="https://www.aclweb.org/anthology/2020.emnlp-main.38/">Bansal et al. (2020)</a> pre-train a model for few-shot learning by automatically generating cloze-style multi-class classification tasks.</p><!--kg-card-begin: markdown--><p><strong>Distinguishing between adaptive and behavioural fine-tuning encourages us to consider the inductive biases we aim to instill in our model and whether they relate to properties of the domain $\mathcal{D}$ or the task $\mathcal{T}$</strong>. Disentangling the role of domain and task is important as information about a domain can often be learned using limited unlabelled data (<a href="https://www.aclweb.org/anthology/2020.coling-main.603/">Ramponi and Plank, 2020</a>) while the acquisition of high-level natural language understanding skills with current methods generally requires billions of pre-training data samples (<a href="https://arxiv.org/abs/2011.04946">Zhang et al., 2020</a>).</p>
<p>The distinction between task and domain becomes fuzzier, however, when we frame tasks in terms of the pre-training objective. A sufficiently general pre-training task such as MLM may provide useful information for learning $P(Y | X)$ but likely does not contain every signal important for the task. For instance, models pre-trained with MLM struggle with modelling negations, numbers, or named entities (<a href="https://www.aclweb.org/anthology/2020.tacl-1.54/">Rogers et al., 2020</a>).</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>Similarly, the use of data augmentation entangles the roles of $\mathcal{D}$ and $\mathcal{T}$ as it allows us to encode the desired capabilities directly in the data. For instance, by fine-tuning a model on text where gendered words are replaced with those of the opposite gender, a model can be made more robust to gender bias (<a href="https://www.aclweb.org/anthology/N18-2003/">Zhao et al., 2018</a>; <a href="https://www.aclweb.org/anthology/N19-1064/">Zhao et al., 2019</a>; <a href="https://arxiv.org/abs/2101.09688">Manela et al., 2021</a>).</p>
<!--kg-card-end: markdown--><h2 id="parameter-efficient-fine-tuning">Parameter-efficient fine-tuning</h2><p>When a model needs to be fine-tuned in many settings such as for a large number of users, it is computationally expensive to store a copy of a fine-tuned model for every scenario. Consequently, recent work has focused on keeping most of the model parameters fixed and fine-tuning a small number of parameters per task. In practice, this enables storing a single copy of a large model and many much smaller files with task-specific modifications.</p><p>The first approaches in this line of work are based on adapters (<a href="https://proceedings.neurips.cc/paper/2017/hash/e7b24b112a44fdd9ee93bdf998c6ca0e-Abstract.html">Rebuffi et al., 2017</a>), small bottleneck layers that are inserted between the layers of a pre-trained model (<a href="http://proceedings.mlr.press/v97/houlsby19a.html">Houlsby et al., 2019</a>; <a href="http://proceedings.mlr.press/v97/stickland19a.html">Stickland and Murray, 2019</a>) whose parameters are fixed. Adapters render common settings such as storing multiple checkpoints during training as well as more advanced techniques such as checkpoint averaging (<a href="https://arxiv.org/abs/1803.05407">Izmailov et al., 2018</a>), snapshot ensembling (<a href="https://openreview.net/forum?id=BJYwwY9ll">Huang et al., 2017</a>) and temporal ensembling (<a href="https://openreview.net/forum?id=BJ6oOfqge">Laine and Aila, 2017</a>) much more space-efficient. Using adapters, a general-purpose model can be efficiently adapted to many settings such as different languages (<a href="https://www.aclweb.org/anthology/D19-1165/">Bapna and Firat, 2019</a>). <a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/">Pfeiffer et al. (2020)</a> recently demonstrated that adapters are modular and can be combined via stacking, which enables learning specialised representations in isolation. This is particularly useful when working with the previously discussed methods: <strong>an adaptively or behaviourally fine-tuned adapter can be evaluated without any task-specific fine-tuning by stacking a trained task adapter on top of it</strong>. This setting can be seen below where a task adapter trained on named entity recognition (NER) is stacked on either an English (left) or Quechua language adapter (right).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/02/modular_adapters.png" class="kg-image" alt="Recent Advances in Language Model Fine-tuning"><figcaption>Task and language adapters inserted in a Transformer block in the MAD-X framework (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/">Pfeiffer et al., 2020</a>). Adapters learn encapsulated representations and can be replaced with each other, enabling zero-shot transfer.</figcaption></figure><!--kg-card-begin: markdown--><p>While adapters modify the model's activations without changing the underlying parameters, another line of work modifies the pre-trained parameters directly. To illustrate this set of methods, we can view fine-tuning as learning how to perturb the parameters of a pre-trained model. Formally, in order to obtain the parameters of a fine-tuned model $\theta_{\text{fine-tuned}} \in \mathbb{R}^D$ where $D$ is the dimensionality of the model, we learn a task-specific parameter vector $\theta_{\text{task}} \in \mathbb{R}^D$ that captures how to change the pre-trained model parameters $\theta_{\text{pre-trained}} \in \mathbb{R}^D$. The fine-tuned parameters are the result of applying the task-specific permutations to the pre-trained parameters:<br>
\begin{equation}<br>
\theta_{\text{fine-tuned}} = \theta_{\text{pre-trained}} + \theta_{\text{task}}<br>
\end{equation}</p>
<p>Instead of storing a copy of $\theta_{\text{fine-tuned}}$ for every task, we can store a single copy of $\theta_{\text{pre-trained}}$ and a copy of $\theta_{\text{task}}$ for every task. This setting is cheaper if we can parameterise $\theta_{\text{task}}$ more efficiently. To this end, <a href="https://arxiv.org/abs/2012.07463">Guo et al. (2020)</a> learn $\theta_{\text{task}}$ as a sparse vector. <a href="https://arxiv.org/abs/2012.13255">Aghajanyan et al. (2020)</a> set $\theta_{\text{task}} = \theta_\text{low} \textbf{M}$ where $\theta_\text{low}$ is a low-dimensional vector and $\textbf{M}$ is a random linear projection (in their case, the FastFood transform (<a href="https://openreview.net/forum?id=ryup8-WCW">Li et al., 2018</a>)).</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>Alternatively, we can apply modifications only to a subset of the pre-trained parameters. A classic method in computer vision (<a href="http://proceedings.mlr.press/v32/donahue14.html">Donahue et al., 2014</a>) fine-tunes only the last layer of the model. Let $\theta_{\text{pre-trained}}$ be the collection of pre-trained parameters across all $L$ layers of the model, i.e. $\theta_{\text{pre-trained}} = \bigcup\limits_{l=1}^{L} \theta_{\text{pre-trained}}^l$ where $\theta_{\text{pre-trained}}^l$ is the parameter vector associated with the $l$-th layer, with analogous notation for $\theta_{\text{fine-tuned}}$ and $\theta_{\text{task}}$. Fine-tuning only the last layer is then equivalent to:<br>
\begin{equation}<br>
\begin{split}<br>
\theta_{\text{fine-tuned}} = &amp; (\bigcup\limits_{l=1}^{L-1} \theta_{\text{pre-trained}}^l) \\<br>
&amp; \cup (\theta_{\text{pre-trained}}^L + \theta_{\text{task}}^L)<br>
\end{split}<br>
\end{equation}</p>
<!--kg-card-end: markdown--><p>While this works less well in NLP (<a href="https://www.aclweb.org/anthology/P18-1031/">Howard &amp; Ruder, 2018</a>), there are other subsets of parameters that are more effective to fine-tune. For instance, <a href="https://nlp.biu.ac.il/~yogo/bitfit.pdf">Ben-Zaken et al. (2020)</a> achieve competitive performance by only fine-tuning a model's bias parameters. </p><p>Another line of work prunes parameters of the pre-trained model during fine-tuning. Such methods use different criteria for pruning weights such as based on zero-th or first-order information about a weight's importance (<a href="https://papers.nips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf">Sanh et al., 2020</a>). As there is limited support of sparse architectures with current hardware, approaches that are <em>structurally</em> sparse, i.e. where updates are concentrated in a limited set of layers, matrices, or vectors are currently preferable. For instance, the last few layers of pre-trained models have been shown to be of limited use during fine-tuning and can be randomly reinitialised (<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.125/">Tamkin et al., 2020</a>; <a href="https://openreview.net/forum?id=cO1IH43yUF">Zhang et al., 2021</a>) or even completely removed (<a href="https://openreview.net/forum?id=xpFFI_NtgpW">Chung et al., 2021</a>).</p><!--kg-card-begin: markdown--><p>While pruning methods focus on reducing the total number of parameters of task-specific models, most of the other methods focus on reducing the number of <em>trainable</em> parameters—while maintaining a copy of $\theta_{\text{pre-trained}}$. The most recent of the latter approaches generally match the performance of full fine-tuning while training around 0.5% of the model's parameters per task (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/">Pfeiffer et al., 2020</a>; <a href="https://arxiv.org/abs/2012.07463">Guo et al., 2020</a>; <a href="https://nlp.biu.ac.il/~yogo/bitfit.pdf">Ben-Zaken et al., 2020</a>).</p>
<!--kg-card-end: markdown--><p>There is increasing evidence that large pre-trained language models learn representations that compress NLP tasks well (<a href="https://openreview.net/forum?id=ryup8-WCW">Li et al., 2018</a>; <a href="https://www.aclweb.org/anthology/2020.repl4nlp-1.18/">Gordon et al., 2020</a>; <a href="https://arxiv.org/abs/2012.13255">Aghajanyan et al., 2020</a>). This practical evidence coupled with their <strong>convenience, availability (<a href="https://www.aclweb.org/anthology/2020.emnlp-demos.7/">Pfeiffer et al., 2020</a>) as well as recent empirical successes make these methods promising both for conducting experiments as well as in practical settings.</strong></p><h2 id="text-to-text-fine-tuning">Text-to-text fine-tuning</h2><p>Another development in transfer learning is a move from masked language models such as BERT (<a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>) and RoBERTa (<a href="https://arxiv.org/abs/1907.11692">Liu et al., 2019</a>) to autoregressive models of language such as T5 (<a href="https://jmlr.org/papers/v21/20-074.html">Raffel et al., 2019</a>) and GPT-3 (<a href="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown et al., 2020</a>). While both sets of methods can be used to assign likelihood scores to text (<a href="https://www.aclweb.org/anthology/2020.acl-main.240/">Salazar et al., 2020</a>), autoregressive LMs are easier to sample from. In contrast, masked LMs are generally restricted to fill-in-the-blank settings, e.g. (<a href="https://www.aclweb.org/anthology/D19-1250/">Petroni et al., 2019</a>).</p><p>The standard way to use masked LMs for fine-tuning is to replace the output layer used for MLM with a randomly initialised task-specific head that is learned on the target task (<a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>). Alternatively, the pre-trained model's output layer can be reused by recasting a task as MLM in a cloze-style format (<a href="https://www.aclweb.org/anthology/2020.tacl-1.48/">Talmor et al., 2020</a>; <a href="https://arxiv.org/abs/2001.07676">Schick and Schütze, 2021</a>). Analogously, autoregressive LMs generally cast the target task in a text-to-text format (<a href="https://arxiv.org/abs/1806.08730">McCann et al., 2018</a>; <a href="https://jmlr.org/papers/v21/20-074.html">Raffel et al., 2020</a>; <a href="https://openreview.net/forum?id=US-TP-xnXI">Paolini et al., 2021</a>). In both settings, <strong>the models are able to benefit from all their pre-trained knowledge and do not need to learn any new parameters from scratch, which improves their sample efficiency.</strong></p><p>In the extreme when no parameters are fine-tuned, framing a target task in terms of the pre-training objective enables zero-shot or few-shot learning using a task-specific prompt and a small number of examples of a task (<a href="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown et al., 2020</a>). However, while such few-shot learning is possible, it is not the most effective way to use such models (<a href="https://arxiv.org/abs/2009.07118">Schick and Schütze, 2020</a>; see <a href="https://ruder.io/research-highlights-2020/#3-few-shot-learning">this post</a> for a brief overview). Learning without updates requires a huge model as the model needs to rely entirely on its existing knowledge. The amount of information available to the model is also restricted by its context window and the prompts shown to the model need to be carefully engineered.</p><p>Retrieval augmentation (see <a href="https://ruder.io/research-highlights-2020/#2-retrieval-augmentation">this post</a> for an overview) can be used to off-load the storage of external knowledge and symbolic approaches could be used to teach a model task-specific rules akin to (<a href="https://openreview.net/forum?id=SkeuexBtDr">Awasthi et al., 2020</a>). Pre-trained models will also become larger and more powerful and may be behaviourally fine-tuned to be good at the zero-shot setting. However, without fine-tuning a model is ultimately limited in its ability to adapt to a new task.</p><p>Consequently, <strong>for most practical settings the best path forward arguably is to fine-tune all or a subset of the model's parameters using the methods described in the previous sections.</strong> <strong>In addition, we will increasingly see an emphasis of pre-trained models' generative capabilities.</strong> While current methods generally focus on modifying a model's natural language input such as via automatic prompt design (<a href="https://arxiv.org/abs/2009.07118">Schick and Schütze, 2020</a>; <a href="https://arxiv.org/abs/2012.15723">Gao et al., 2020</a>; <a href="https://arxiv.org/abs/2010.15980">Shin et al., 2020</a>), the most effective way to modulate the output of such models will likely act directly on their hidden representations (<a href="https://openreview.net/forum?id=H1edEyBKDS">Dathathri et al., 2020</a>; see <a href="https://lilianweng.github.io/lil-log/2021/01/02/controllable-neural-text-generation.html">Lillian Weng's post</a> for an overview of methods for controllable generation). </p><h2 id="mitigating-fine-tuning-instabilities">Mitigating fine-tuning instabilities</h2><p>A practical problem with fine-tuning pre-trained models is that performance can vary drastically between different runs, particularly on small datasets (<a href="https://arxiv.org/abs/1811.01088">Phang et al., 2018</a>). <a href="https://arxiv.org/abs/2002.06305">Dodge et al., 2020</a> find that both the weight initialisation of the output layer and the order of the training data contribute to variation in performance. As instabilities are generally apparent early in training, they recommend stopping the least promising runs early after 20-30% of training. <a href="https://openreview.net/forum?id=nzpLWnVAyah">Mosbach et al. (2021</a>) additionally recommend using small learning rates and to increase the number of epochs when fine-tuning BERT.</p><p>A number of recent methods seek to mitigate instabilities during fine-tuning by relying on adversarial or trust region-based approaches (<a href="https://openreview.net/forum?id=BygzbyHFvB">Zhu et al., 2019</a>; <a href="https://www.aclweb.org/anthology/2020.acl-main.197/">Jiang et al., 2020</a>; <a href="https://openreview.net/forum?id=OQ08SN70M1V">Aghajanyan et al., 2021</a>). Such methods generally augment the fine-tuning loss with a regularisation term that bounds the divergence between update steps.</p><p>In light of the previous section, we can make another recommendation for minimising instabilities during fine-tuning: <strong>Avoid using a randomly initialised output layer on the target task for small datasets by framing the target task as a form of LM or use behavioural fine-tuning to fine-tune the output layer prior to task-specific fine-tuning.</strong> While text-to-text models are thus more robust to fine-tuning on small datasets, they suffer from instabilities in the few-shot setting and are sensitive to the prompt and few-shot examples (<a href="https://arxiv.org/abs/2102.09690">Zhao et al., 2021</a>).</p><p>Overall, as models are increasingly applied to challenging tasks with fewer training examples, it is crucial to develop methods that are robust to possible variations and that can be reliably fine-tuned.</p><h2 id="citation">Citation</h2><p>For attribution in academic contexts, please cite this work as:</p><pre><code>@misc{ruder2021lmfine-tuning,
  author = {Ruder, Sebastian},
  title = {{Recent Advances in Language Model Fine-tuning}},
  year = {2021},
  howpublished = {\url{http://ruder.io/recent-advances-lm-fine-tuning}},
}</code></pre>]]></content:encoded></item><item><title><![CDATA[ML and NLP Research Highlights of 2020]]></title><description><![CDATA[This post summarizes progress in 10 exciting and impactful directions in ML and NLP in 2020.]]></description><link>http://ruder.io/research-highlights-2020/</link><guid isPermaLink="false">5fcb79c34be0c26392ea1525</guid><category><![CDATA[transfer learning]]></category><category><![CDATA[natural language processing]]></category><category><![CDATA[language models]]></category><category><![CDATA[reinforcement learning]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Tue, 19 Jan 2021 09:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2021/01/lra_analysis-2.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2021/01/lra_analysis-2.png" alt="ML and NLP Research Highlights of 2020"><p>The selection of areas and methods is heavily influenced by my own interests; the selected topics are biased towards representation and transfer learning and towards natural language processing (NLP). I tried to cover the papers that I was aware of but likely missed many relevant ones—feel free to highlight them in the comments below. In all, I discuss the following highlights:</p><ol><li><a href="#1-scaling-up-and-down">Scaling up—and down</a></li><li><a href="#2-retrieval-augmentation">Retrieval augmentation</a></li><li><a href="#3-few-shot-learning">Few-shot learning</a></li><li><a href="#4-contrastive-learning">Contrastive learning</a></li><li><a href="#5-evaluation-beyond-accuracy">Evaluation beyond accuracy</a></li><li><a href="#6-practical-concerns-of-large-lms">Practical concerns of large LMs</a></li><li><a href="#7-multilinguality">Multilinguality</a></li><li><a href="#8-image-transformers">Image Transformers</a></li><li><a href="#9-ml-for-science">ML for science</a></li><li><a href="#10-reinforcement-learning">Reinforcement learning</a></li></ol><h1 id="1-scaling-up-and-down">1) Scaling up—and down</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/large_models.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>Model sizes of language models from 2018–2020 (Credit: <a href="https://www.stateof.ai/">State of AI Report 2020</a>)</figcaption></figure><p><strong>What happened?</strong>  2020 saw the development of ever larger language and dialogue models such as Meena (<a href="https://arxiv.org/abs/2001.09977">Adiwardana et al., 2020</a>), <a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">Turing-NLG</a>, BST (<a href="https://arxiv.org/abs/2004.13637">Roller et al., 2020</a>), and GPT-3 (<a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown et al., 2020</a>). At the same time, researchers have become more aware of how expensive and energy-hungry these models can be (<a href="https://www.aclweb.org/anthology/P19-1355/">Strubell et al., 2019</a>) and work that focuses on making them smaller has gained momentum: Recent approaches rely on pruning (<a href="https://arxiv.org/abs/2004.03844">Sajjad et al., 2020</a>; <a href="https://openreview.net/forum?id=SylO2yStDr">Fan et al., 2020a</a>; <a href="https://papers.nips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf">Sanh et al., 2020</a>), quantization (<a href="https://openreview.net/forum?id=dV19Yyi1fS3">Fan et al., 2020b</a>), distillation (<a href="https://arxiv.org/abs/1910.01108">Sanh et al., 2019</a>; <a href="https://www.aclweb.org/anthology/2020.acl-main.195/">Sun et al., 2020</a>), and compression (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.633/">Xu et al., 2020</a>). Other approaches focused on making the Transformer architecture itself more efficient. Models in this line include the Performer (<a href="https://openreview.net/forum?id=Ua6zuk0WRH">Choromanski et al., 2020</a>) and Big Bird (<a href="https://papers.nips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf">Zaheer et al., 2020</a>), which can be seen in the cover image above. The image shows performance (y axis), speed (x axis) and memory footprint (circle size) of different models on the Long Range Arena benchmark (<a href="https://openreview.net/forum?id=qVyeW-grC2k">Tay et al., 2020</a>). </p><p>Tools such as the experiment-impact-tracker (<a href="https://arxiv.org/abs/2002.05651">Henderson et al., 2020</a>) have made it easier to track the energy efficiency of models. They have also facilitated competitions and benchmarks that evaluate models primarily based on their efficiency such as the <a href="https://sites.google.com/view/sustainlp2020/home?utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">SustaiNLP workshop</a> at EMNLP 2020, the <a href="https://ai.google.com/research/NaturalQuestions/efficientqa?utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">Efficient QA competition</a> at NeurIPS 2020, and HULK (<a href="https://arxiv.org/abs/2002.05829">Zhou et al., 2020</a>).</p><p><strong>Why is it important?</strong>  Scaling up models allows us to keep pushing the boundaries of what current models can do. In order to deploy and use them in real-world scenarios, however, they need to be efficient. Ultimately, both directions benefit each other: Compressing large models yields efficient models with strong performance (<a href="https://arxiv.org/abs/2002.11794">Li et al., 2020</a>) while more efficient methods may lead to stronger, larger models (<a href="https://arxiv.org/abs/2003.10555">Clark et al., 2020</a>).</p><p><strong>What's next?</strong>  I am hopeful that—in light of the increasing interest in efficiency and the availability of tools—it will become more common not only to report a model's performance and number of parameters but also its energy efficiency. This should contribute to a more holistic evaluation that may help to bridge the gap to real-world ML use cases.</p><h1 id="2-retrieval-augmentation">2) Retrieval augmentation</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/retrieval_augmented_lm.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>Unsupervised pre-training with REALM (<a href="https://arxiv.org/abs/2002.08909">Guu et al., 2020</a>); retriever and encoder are jointly pre-trained</figcaption></figure><p><strong>What happened?</strong>  Large models have been shown to have learned a surprising amount of world knowledge from their pre-training data, which allows them to reproduce facts (<a href="https://www.aclweb.org/anthology/2020.tacl-1.28.pdf">Jiang et al., 2020</a>) and answer questions even without access to external context (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.437.pdf">Roberts et al., 2020</a>). However, storing such knowledge implicitly in the parameters of a model is inefficient and requires ever larger models to retain more information. Instead, recent approaches jointly trained retrieval models and large language models, which led to strong results on knowledge-intensive NLP tasks such as open-domain question answering (<a href="https://arxiv.org/abs/2002.08909">Guu et al., 2020</a>; <a href="https://arxiv.org/abs/2005.11401">Lewis et al., 2020</a>) and language modelling (<a href="https://openreview.net/forum?id=HklBjCEKvH">Khandelwal et al., 2020</a>). The main advantage of these methods is that they integrate retrieval directly into language model pre-training, which allows language models to be much more efficient by being able to off-load the recall of facts and focus on learning the more challenging aspects of natural language understanding. Consequently, the best systems in the NeurIPS 2020 EfficientQA competition (<a href="https://arxiv.org/abs/2101.00133">Min et al., 2020</a>) all relied on retrieval.</p><p><strong>Why is it important?</strong>  Retrieval was the standard in many generative tasks, such as text summarization or dialogue and has largely been superseded by abstractive generation (<a href="https://arxiv.org/abs/1707.02268">Allahyari et al., 2017</a>). Retrieval-augmented generation enables combining the best of both worlds: the factual correctness and faithfulness of retrieved segments and the relevancy and composition of generated text.</p><p><strong>What's next?</strong>  Retrieval-augmented generation should be particularly useful for dealing with failure cases that have plagued generative neural models in the past, such as dealing with hallucinations (<a href="https://www.aclweb.org/anthology/P19-1256/">Nie et al., 2019</a>). It may also help make systems more interpretable by directly providing evidence for their prediction.</p><h1 id="3-few-shot-learning">3) Few-shot learning</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/prompt-based_fine-tuning.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>Prompt-based fine-tuning uses templated prompts and demonstrations (<a href="https://arxiv.org/abs/2012.15723">Gao et al., 2020</a>)</figcaption></figure><p><strong>What happened?</strong>  Over the last years, driven by advances in pre-training, the number of training examples to perform a given task has progressively gone down (<a href="https://www.aclweb.org/anthology/N18-1202/">Peters et al., 2018</a>; <a href="https://www.aclweb.org/anthology/P18-1031/">Howard et al., 2018</a>). We are now at a stage where tens of examples can be used to demonstrate a given task (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.38/">Bansal et al., 2020</a>). A very natural paradigm for few-shot learning is to reframe a task as language modelling. The most prominent instantiation of this, the in-context learning approach of GPT-3 (<a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown et al., 2020</a>) performs a prediction based on a few demonstrations of input–output pairs in the model's context and a prompt without any gradient updates. This setting, however, has a few limitations: It requires a huge model—without any updates the model needs to rely on its existing knowledge—, the amount of knowledge that the model can use is restricted by its context window, and prompts need to be hand-engineered.</p><p>Recent work has sought to make such few-shot learning more effective by using a smaller model, integrating fine-tuning, and automatically generating natural language prompts (<a href="https://arxiv.org/abs/2009.07118">Schick and Schütze, 2020</a>; <a href="https://arxiv.org/abs/2012.15723">Gao et al., 2020</a>; <a href="https://arxiv.org/abs/2010.15980">Shin et al., 2020</a>). Such work is closely related to the broader area of controllable neural text generation, which broadly seeks to leverage the generative capabilities of powerful pre-trained models. For an excellent overview, check out <a href="https://lilianweng.github.io/lil-log/2021/01/02/controllable-neural-text-generation.html">Lilian Weng's blog post</a>.</p><p>Few-shot learning enables rapid adaptation of a model to many tasks. However, updating all model parameters for each task is wasteful. Instead, it is preferable to perform localized updates that concentrate changes in a small set of parameters. There have been a few approaches that make such efficient fine-tuning more practical including using adapters (<a href="http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf">Houlsby et al., 2019</a>; <a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/">Pfeiffer et al., 2020a</a>,<a href="https://www.aclweb.org/anthology/2020.emnlp-demos.7/">b</a>; <a href="https://www.aclweb.org/anthology/2020.emnlp-main.180.pdf">Üstün et al., 2020</a>), adding a sparse parameter vector (<a href="https://arxiv.org/abs/2012.07463">Guo et al., 2020</a>), and only modifying bias values (<a href="https://nlp.biu.ac.il/~yogo/bitfit.pdf">Ben-Zaken et al., 2020</a>).</p><p><strong>Why is it important?</strong>  Being able to teach a model a task based on only a few examples greatly reduces the barrier to entry for applying ML and NLP models in practice. This opens up applications where data is very expensive to collect and enables adapting models swiftly to new domains.</p><p><strong>What's next?</strong>  For many real-world scenarios, it is possible to collect thousands of training examples. Models should thus be able to scale seamlessly from learning from a few to learning from thousands of examples and should not be limited by e.g. their context length. Given that models have achieved super-human performance on many popular tasks such as <a href="https://super.gluebenchmark.com/leaderboard">SuperGLUE</a> when fine-tuned on entire training datasets, enhancing their few-shot performance is a natural area for improvement.</p><h1 id="4-contrastive-learning">4) Contrastive learning</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/instance_discrimination.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>Instance discrimination compares features from different transformations of the same images to each other (<a href="https://proceedings.neurips.cc/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf">Caron et al., 2020</a>)</figcaption></figure><p><strong>What happened?</strong>  Contrastive learning—learning to differentiate a positive example from negative samples, often from a noise distribution—such as using negative sampling or noise contrastive estimation is a staple of representation learning and self-supervised learning and a prominent part of classic approaches such as word2vec (<a href="https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Mikolov et al., 2013</a>). More recently, contrastive learning gained popularity in self-supervised representation learning in computer vision and speech (<a href="https://arxiv.org/abs/1807.03748">van den Oord, 2018</a>; <a href="https://arxiv.org/abs/1905.09272">Hénaff et al., 2019</a>). The recent generation of increasingly powerful self-supervised approaches for visual representation learning rely on contrastive learning using an instance discrimination task: different images are treated as negative pairs and views of the same image are treated as positive pairs. Recent approaches have further refined this general framework: SimCLR (<a href="http://proceedings.mlr.press/v119/chen20j/chen20j.pdf">Chen et al., 2020</a>) defines the contrastive loss over augmented examples, Momentum Contrast (<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf">He et al., 2020</a>) seeks to ensure a large and consistent set of pairs, SwAV (<a href="https://proceedings.neurips.cc/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf">Caron et al., 2020</a>) leverages online clustering, and BYOL only employs positive pairs (<a href="https://arxiv.org/abs/2006.07733">Grill et al., 2020</a>). <a href="https://arxiv.org/abs/2011.10566">Chen and He (2020</a>) have furthermore proposed a simpler formulation that relates to the previous methods.</p><p>Recently, <a href="https://openreview.net/forum?id=tC6iW2UUbJf">Zhao et al. (2020)</a> find that data augmentation is essential for contrastive learning. This might indicate why <em>unsupervised</em> contrastive learning has not been successful with large pre-trained models in NLP where data augmentation is less common. They also hypothesize that the reason instance discrimination may work better than supervised pre-training in computer vision is that it does not try to make the features of all instances from a class similar but retains the information from each instance. This is less of a problem in NLP where unsupervised pre-training involves classification over thousands of word types. In NLP, <a href="https://openreview.net/forum?id=cu7IUiOhujH">Gunel et al. (2020)</a> recently employ contrastive learning for <em>supervised</em> fine-tuning. </p><p><strong>Why is it important?</strong>  The cross-entropy objective between one-hot labels and a model's output logits commonly used in language modelling has several limitations such as generalizing poorly to imbalanced classes (<a href="http://papers.neurips.cc/paper/8435-learning-imbalanced-datasets-with-label-distribution-aware-margin-loss.pdf">Cao et al., 2019</a>). Contrastive learning is an alternative, complementary paradigm that may help ameliorate some of these deficits.</p><p><strong>What's next?</strong>  Contrastive learning combined with masked language modelling may enable us to learn representations that are richer and more robust. It could help model outliers and rare syntactic and semantic phenomena, which are a challenge for current NLP models.</p><h1 id="5-evaluation-beyond-accuracy">5) Evaluation beyond accuracy</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/checklist_negation.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>A CheckList template and tests probing for an understanding of negation in sentiment analysis (<a href="https://www.aclweb.org/anthology/2020.acl-main.442/">Ribeiro et al., 2020</a>)</figcaption></figure><p><strong>What happened?</strong>  State-of-the-art models in NLP have achieved superhuman performance across many tasks. Whether or not we believe that such models can achieve true natural language understanding (<a href="https://arxiv.org/abs/1901.11373">Yogatama et al., 2019</a>; <a href="https://www.aclweb.org/anthology/2020.acl-main.463/">Bender and Koller, 2020</a>), we know that current models are not close to this elusive goal. However, the simple performance metrics of our tasks fail to capture the limitations of existing models. There are two key themes in this area: a) curating examples that are difficult for current models; and b) going beyond simple metrics such as accuracy towards more fine-grained evaluation.</p><p>Regarding the former, the common methodology is to use adversarial filtering (<a href="https://www.aclweb.org/anthology/D18-1009/">Zellers et al., 2018</a>) during dataset creation to filter out examples that are predicted correctly by current models. Recent work proposes more efficient adversarial filtering methods (<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6399/6255">Sakaguchi et al., 2020</a>; <a href="http://proceedings.mlr.press/v119/bras20a/bras20a.pdf">Le Bras et al., 2020</a>) and an iterative dataset creation process (<a href="https://www.aclweb.org/anthology/2020.acl-main.441/">Nie et al., 2020</a>; <a href="https://transacl.org/ojs/index.php/tacl/article/view/2129/649">Bartolo et al., 2020</a>) where examples are filtered and models are re-trained over multiple rounds. A subset of such evolving benchmarks are available in <a href="https://dynabench.org/">Dynabench</a>.</p><p>The methods that regard the second point are similar in spirit. However, rather than creating examples that target a specific model, examples are used to probe for phenomena common to a task of interest. Commonly, minimal pairs—also known as counterfactual examples or contrast sets—(<a href="https://openreview.net/forum?id=Sklgs0NFvr">Kaushik et al., 2020</a>;  <a href="https://arxiv.org/abs/2004.02709">Gardner et al., 2020</a>; <a href="https://transacl.org/ojs/index.php/tacl/article/view/2013/527">Warstadt et al., 2020</a>) are created, which perturb examples in a minimal way and often change the gold label. <a href="https://www.aclweb.org/anthology/2020.acl-main.442/">Ribeiro et al. (2020)</a> formalized some of the underlying intuitions in their CheckList framework, which enables the semi-automatic creation of such test cases. Alternatively, examples can be characterized based on different attributes, which allow a more fine-grained analysis of a model's strengths and weaknesses (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.489/">Fu et al., 2020</a>).</p><p><strong>Why is it important?</strong>  In order to make meaningful progress towards building more capable models machine learning models, we need to understand not only if a model outperforms a previous system but what kind of errors it makes and which phenomena it fails to capture.</p><p><strong>What's next?</strong>  By providing fine-grained diagnostics of model behaviour, it will be easier to identify a model's deficiencies and propose improvements that address them. Similarly, a fine-grained evaluation would allow a more nuanced comparison of the strengths and weaknesses of different methods.</p><h1 id="6-practical-concerns-of-large-lms">6) Practical concerns of large LMs</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/real_toxicity_prompts.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>Models generate toxic content based on seemingly innocuous prompts (<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.301/">Gehman et al., 2020</a>)</figcaption></figure><p><strong>What happened?</strong>  <a href="https://ruder.io/research-highlights-2019/#10-more-reliable-analysis-methods">Compared to 2019</a> where the analysis of language models (LMs) mainly focused on the syntactic, semantic, and world knowledge that such models capture—see <a href="https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00349">(Rogers et al., 2020)</a> for a great overview—recent analyses revealed a number of practical concerns. Pre-trained language models were found to be prone to generating toxic language (<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.301/">Gehman et al., 2020</a>) and leak information (<a href="https://arxiv.org/abs/2004.00053">Song &amp; Raghunathan, 2020</a>), to be susceptible to backdoors after fine-tuning, which let an attacker manipulate the model prediction (<a href="https://www.aclweb.org/anthology/2020.acl-main.249/">Kurita et al., 2020</a>; <a href="https://arxiv.org/abs/2010.12563">Wallace et al., 2020</a>), and to be vulnerable to model and data extraction attacks (<a href="https://openreview.net/forum?id=Byl5NREFDr">Krishna et al., 2020</a>; <a href="https://arxiv.org/abs/2012.07805">Carlini et al., 2020</a>). In addition, pre-trained models are well known to capture biases with regard to protected attributes such as gender (<a href="https://arxiv.org/abs/1607.06520">Bolukbasi et al., 2016</a>; <a href="https://arxiv.org/abs/2010.06032">Webster et al., 2020</a>)—see <a href="https://www.aclweb.org/anthology/P19-1159/">(Sun et al., 2019)</a> for an excellent survey on mitigating gender bias.</p><p><strong>Why is it important?</strong>  Large pre-trained models are trained by many institutions and are actively deployed in real-world scenarios. It is thus of practical importance that we are not only aware of their biases but what behaviour may have actually harmful consequences.</p><p><strong>What's next?</strong>  As larger and more powerful models are developed, it is important that such practical concerns as well as issues around bias and fairness are part of the development process from the start.  </p><h1 id="7-multilinguality">7) Multilinguality</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/language_resource_distribution.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>The unequal distribution of labeled and unlabeled data for languages around the world (<a href="https://www.aclweb.org/anthology/2020.acl-main.560/">Joshi et al., 2020</a>)</figcaption></figure><p><strong>What happened?</strong>  2020 had many highlights in multilingual NLP. The <a href="https://www.masakhane.io/">Masakhane</a> organisation whose mission is to strengthen NLP for African languages gave the <a href="https://www.youtube.com/watch?v=Xbc_g_OknqA">keynote</a> at the <a href="http://www.statmt.org/wmt20/">Fifth Conference on Machine Translation (WMT20)</a>, one of the most inspiring presentations of the last year. New general-purpose benchmarks for other languages emerged including XTREME (<a href="http://proceedings.mlr.press/v119/hu20b/hu20b.pdf">Hu et al., 2020</a>), XGLUE (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.484/">Liang et al., 2020</a>), IndoNLU (<a href="https://www.aclweb.org/anthology/2020.aacl-main.85/">Wilie et al., 2020</a>), IndicGLUE (<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.445/">Kakwani et al., 2020</a>). Existing datasets that were replicated in other languages—together with their non-English variants—include:</p><ul><li>SQuAD: XQuAD (<a href="https://www.aclweb.org/anthology/2020.acl-main.421/">Artetxe et al., 2020</a>), MLQA (<a href="https://www.aclweb.org/anthology/2020.acl-main.653/">Lewis et al., 2020</a>), FQuAD (<a href="https://arxiv.org/abs/2002.06071">d'Hoffschmidt et al., 2020</a>);</li><li>Natural Questions: TyDiQA (<a href="https://arxiv.org/abs/2003.05002">Clark et al., 2020</a>), MKQA (<a href="https://arxiv.org/abs/2007.15207">Longpre et al., 2020</a>);</li><li>MNLI: OCNLI (<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.314/">Hu et al., 2020</a>), FarsTail (<a href="https://arxiv.org/abs/2009.08820">Amirkhani et al., 2020</a>);</li><li>the CoNLL-09 dataset: X-SRL (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.321/">Daza and Frank, 2020</a>); and</li><li>the CNN/Daily Mail dataset: MLSUM (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.647/">Scialom et al., 2020</a>).</li></ul><p>Many of these datasets and many others in different languages are easily accessible via <a href="https://huggingface.co/datasets">Hugging Face datasets</a>. Powerful multilingual models that cover around 100 languages emerged including XML-R (<a href="https://www.aclweb.org/anthology/2020.acl-main.747/">Conneau et al., 2020</a>), RemBERT (<a href="https://openreview.net/forum?id=xpFFI_NtgpW">Chung et al., 2020</a>), InfoXLM (<a href="https://arxiv.org/abs/2007.07834">Chi et al., 2020</a>), and others (see the <a href="https://sites.research.google/xtreme">XTREME leaderboard</a> for an overview). A plethora of language-specific BERT models have been trained for languages beyond English such as AraBERT (<a href="https://www.aclweb.org/anthology/2020.osact-1.2/">Antoun et al., 2020</a>) and IndoBERT (<a href="https://www.aclweb.org/anthology/2020.aacl-main.85/">Wilie et al., 2020</a>); see (<a href="https://arxiv.org/abs/2003.02912">Nozza et al., 2020</a>; <a href="https://arxiv.org/abs/2012.15613">Rust et al., 2020</a>) for an overview. With efficient multilingual frameworks such as <a href="https://adapterhub.ml/">AdapterHub</a> (<a href="https://www.aclweb.org/anthology/2020.emnlp-demos.7/">Pfeiffer et al., 2020</a>), <a href="https://stanfordnlp.github.io/stanza/">Stanza</a> (<a href="https://www.aclweb.org/anthology/2020.acl-demos.14/">Qi et al., 2020</a>) and <a href="https://github.com/nlp-uoregon/trankit">Trankit</a> (<a href="https://arxiv.org/abs/2101.03289">Nguyen et al., 2020</a>) it has become easier than ever to apply and build models for many of the world's languages.</p><p>Finally, two position papers that inspired much of my thinking in this area this year are <em>The State and Fate of Linguistic Diversity and Inclusion in the NLP World</em> <a href="https://www.aclweb.org/anthology/2020.acl-main.560/">(Joshi et al., 2020)</a> and <em>Decolonising Speech and Language Technology</em> (<a href="https://www.aclweb.org/anthology/2020.coling-main.313/">Bird, 2020</a>). While the first highlights the urgent importance of working on languages beyond English, the second one cautions against treating language communities and their data as a commodity.</p><p><strong>Why is it important?</strong>  Working on NLP beyond English <a href="https://ruder.io/nlp-beyond-english/">has numerous benefits</a>: It poses interesting challenges for ML and NLP and enables having a large impact on society, among many others.</p><p><strong>What's next?</strong>  Given the availability of data and models in different languages, the stage is set to make meaningful progress on languages beyond English. I am most excited about developing models that tackle the most challenging settings and identifying in which cases the assumptions that underlie our current models fail.</p><h1 id="8-image-transformers">8) Image Transformers</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/vision_transformer.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>The Vision Transformer (<a href="https://openreview.net/forum?id=YicbFdNTTy">Dosovitskiy et al., 2020</a>) applies a Transformer encoder to flattened image patches</figcaption></figure><p><strong>What happened?</strong>  While Transformers have achieved large success in NLP, they were—up until recently—less successful in computer vision where convolutional neural networks (CNNs) still reigned supreme. While models early in the year such as DETR (<a href="https://arxiv.org/abs/2005.12872">Carion et al., 2020</a>) employed a CNN to compute image features, later models were completely convolution-free. Image GPT (<a href="http://proceedings.mlr.press/v119/chen20s/chen20s.pdf">Chen et al., 2020</a>) applied the GPT-2 recipe to pre-training directly from pixels and outperforms a supervised Wide ResNet. Later models all reshape an image into patches that are treated as "tokens". Vision Transformer (<a href="https://openreview.net/forum?id=YicbFdNTTy">Dosovitskiy et al., 2020</a>) is pre-trained on millions of labelled images—each consisting of such patches—outperforming state-of-the-art CNNs. The Image Processing Transformer (<a href="https://arxiv.org/abs/2012.00364">Chen et al., 2020</a>) pre-trains on corrupted ImageNet examples with a contrastive loss and achieves state-of-the-art performance on low-level image tasks. The Data-efficient image Transformer (<a href="https://arxiv.org/abs/2012.12877">Touvron et al., 2020</a>) is pre-trained on ImageNet via distillation. Interestingly, they observe that CNNs are better teachers. This is similar to findings for distilling an inductive bias into BERT (<a href="https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00345">Kuncoro et al., 2020</a>). In contrast in speech, Transformers have not been applied directly to the audio signal—to my knowledge—but typically receive the output of an encoder such as a CNN as input (<a href="https://arxiv.org/abs/2001.02674">Moritz et al., 2020</a>; <a href="https://arxiv.org/abs/2005.08100">Gulati et al., 2020</a>; <a href="https://arxiv.org/abs/2006.13979">Conneau et al., 2020</a>)</p><p><strong>Why is it important?</strong>  Transformers have less inductive bias compared to CNNs and RNNs. While being less theoretically powerful than RNNs (<a href="https://www.aclweb.org/anthology/P18-2117/">Weiss et al., 2018</a>; <a href="https://www.aclweb.org/anthology/2020.tacl-1.11/">Hahn et al., 2020</a>), given sufficient data and scale Transformers have been shown to eventually outperform their inductively biased competitors (cf. <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</a>).</p><p><strong>What's next?</strong>  We will likely see Transformers become more popular in computer vision. They will be applied particularly in scenarios where enough compute and data for unsupervised pre-training is available. In smaller scale settings, CNNs will likely still be the go-to approach and a strong baseline. </p><h1 id="9-ml-for-science">9) ML for science</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/alphafold.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>The self-attention-based architecture of AlphaFold (Credit: <a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">DeepMind blog</a>)</figcaption></figure><p><strong>What happened?</strong>  One of the highlights was <a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">AlphaFold</a> demonstrating ground-breaking performance in the biannual CASP challenge for protein folding. Beyond that, there have been several other notable developments in applying ML to problems in the natural sciences. MetNet (<a href="https://arxiv.org/abs/2003.12140">Sønderby et al., 2020</a>) outperformed numerical weather prediction for precipitation forecasting, <a href="https://openreview.net/forum?id=S1eZYeHFDS">Lample and Charton (2020)</a> solved differential equations using neural networks better than commercial computer algebra systems, and <a href="https://www.nature.com/articles/s41586-020-2939-8">Bellemare et al. (2020)</a> used reinforcement learning to navigate balloons in the stratosphere. </p><p>In addition, ML has been used extensively to help with the ongoing COVID-19 pandemic, e.g. to forecast COVID-19 spread (<a href="https://arxiv.org/abs/2007.03113">Kapoor et al., 2020</a>), <a href="https://deepmind.com/research/open-source/computational-predictions-of-protein-structures-associated-with-COVID-19">predict structures associated with COVID-19</a>, translate relevant data into 35 different languages (<a href="https://www.aclweb.org/anthology/2020.nlpcovid19-2.5/">Anastasopoulos et al., 2020</a>), and answer questions about COVID-19 in real-time (<a href="https://www.aclweb.org/anthology/2020.nlpcovid19-2.1/">Lee et al., 2020</a>). For an overview of COVID-19 related applications of NLP, check out the <a href="https://www.aclweb.org/anthology/volumes/2020.nlpcovid19-2/">Proceedings of the 1st Workshop on NLP for COVID-19</a>.</p><p><strong>Why is it important?</strong>  The natural sciences are arguably the most impactful application area for ML. Improvements touch many aspects of life and can have a profound impact on the world.</p><p><strong>What's next?</strong>  With progress in areas as central as protein folding, the speed of application of ML to the natural sciences will only accelerate. I am looking forward to many more fundamental advances that have a positive impact in the world.</p><h1 id="10-reinforcement-learning">10) Reinforcement learning</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/atari_performance.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>Performance of Agent57 and MuZero on Atari compared to state-of-the-art agents in terms of the number of games where they outperform the human benchmark throughout training (<a href="https://arxiv.org/abs/2003.13350">Badia et al., 2020</a>)</figcaption></figure><p><strong>What happened?</strong>  For the first time, a single deep RL agent—Agent57 (<a href="https://arxiv.org/abs/2003.13350">Badia et al., 2020</a>)—has achieved superhuman performance on all 57 Atari games, a long-standing benchmark in the deep reinforcement learning literature. The agent's versatility comes from a neural network that allows it to switch between exploratory and exploitative policies. Another milestone was the development of MuZero (<a href="https://www.nature.com/articles/s41586-020-03051-4">Schrittwieser et al., 2020</a>), which predicts the aspects of the environment that are most important for accurate planning. Without any knowledge of the game dynamics, it achieved state-of-the-art performance on Atari as well as superhuman performance on Go, chess, and shogi. Finally, Munchausen RL agents (<a href="https://papers.nips.cc/paper/2020/file/2c6a0bae0f071cbbf0bb3d5b11d90a82-Supplemental.pdf">Vieillard et al., 2020</a>) improved on state-of-the-art agents via a simple, theoretically founded modification.</p><p><strong>Why is it important?</strong>  Reinforcement learning algorithms have a multitude of practical implications (<a href="https://www.nature.com/articles/s41586-020-2939-8">Bellemare et al., 2020</a>). Improvements over the fundamental algorithms in this area can have a large practical impact by enabling better planning, environment modelling, and action prediction.</p><p><strong>What's next?</strong>  With classic benchmarks such as Atari essentially solved, researchers may look to more challenging settings to test their algorithms such as generalizing to out-of-distribution tasks, improving sample-efficiency, multi-task learning, etc. </p><h2 id="citation">Citation</h2><p>For attribution in academic contexts, please cite this work as:</p><pre><code>@misc{ruder2021researchhighlights,
  author = {Ruder, Sebastian},
  title = {{ML and NLP Research Highlights of 2020}},
  year = {2021},
  howpublished = {\url{http://ruder.io/research-highlights-2020}},
}</code></pre><p><em>Thanks to Sameer Singh whose <a href="https://twitter.com/sameer_/status/1347472491346763776">Twitter thread</a> reviewing NLP research in 2020 provided inspiration for this post.</em></p>]]></content:encoded></item><item><title><![CDATA[Why You Should Do NLP Beyond English]]></title><description><![CDATA[7000+ languages are spoken around the world but NLP research has mostly focused on English. This post outlines why you should work on languages other than English.]]></description><link>http://ruder.io/nlp-beyond-english/</link><guid isPermaLink="false">5ea9dc7bc4820fecc309c25c</guid><category><![CDATA[cross-lingual]]></category><category><![CDATA[natural language processing]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Sat, 01 Aug 2020 00:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2020/07/langscape-1.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2020/07/langscape-1.png" alt="Why You Should Do NLP Beyond English"><p>Natural language processing (NLP) research predominantly focuses on developing methods that work well for English despite the many positive benefits of working on other languages. These benefits range from an outsized societal impact to modelling a wealth of linguistic features to avoiding overfitting as well as interesting challenges for machine learning (ML).</p><p>There are<a href="https://www.ethnologue.com/"> around 7,000 languages</a> spoken around the world. The map above (see the interactive version at<a href="http://langscape.umd.edu/map.php"> Langscape</a>) gives an overview of languages spoken around the world, with each green circle representing a native language. Most of the world's languages are <a href="https://www.ethnologue.com/guides/ethnologue200">spoken</a> in<a href="https://www.ethnologue.com/guides/continents-most-indigenous-languages"> Asia, Africa, the Pacific region and the Americas</a>.</p><p>While we have seen exciting progress across many tasks in natural language processing (see<a href="https://paperswithcode.com/area/natural-language-processing"> Papers with Code</a> and<a href="http://nlpprogress.com/"> NLP Progress</a> for an overview) over the last years, most such results have been achieved in English and a small set of other high-resource languages.</p><p>In<a href="https://ruder.io/unsupervised-cross-lingual-learning/#the-nlp-resource-hierarchy"> a previous overview</a> of our<a href="https://tinyurl.com/xlingual"> ACL 2019 tutorial on Unsupervised Cross-lingual Representation Learning</a>, I've defined a resource hierarchy based on the availability of unlabelled data and labelled data online. In a recent<a href="https://arxiv.org/abs/2004.09095"> ACL 2020 paper</a>, Joshi et al. define a taxonomy similarly based on data availability, which you can see below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/06/language_data_distribution.png" class="kg-image" alt="Why You Should Do NLP Beyond English"><figcaption>Language resource distribution of <a href="https://arxiv.org/abs/2004.09095">Joshi et al. (2020).</a> The size and colour of a circle represent the number of languages and speakers respectively in each category. Colours (on the VIBGYOR spectrum; <strong>V</strong>iolet–<strong>I</strong>ndigo–<strong>B</strong>lue–<strong>G</strong>reen–<strong>Y</strong>ellow–<strong>O</strong>range–<strong>R</strong>ed) represent the total speaker population size from low (violet) to high (red).</figcaption></figure><p>Languages in categories 5 and 4 that lie at a sweet spot of having both large amounts of labelled and unlabelled data available to them are well-studied in the NLP literature. On the other hand, languages in the other groups have largely been neglected.</p><p>In this post, I will argue why you should work on languages other than English. Specifically, I will highlight reasons from a <a href="#the-societal-perspective">societal</a>, <a href="#the-linguistic-perspective">linguistic</a>, <a href="#the-ml-perspective">machine learning</a>, <a href="#the-cultural-and-normative-perspective">cultural and normative</a>, and <a href="#the-cognitive-perspective">cognitive</a> perspective.</p><h2 id="the-societal-perspective">The societal perspective</h2><p>Technology cannot be accessible if it is only available for English speakers with a <a href="https://en.wikipedia.org/wiki/Received_Pronunciation">standard accent</a>.</p><p>What language you speak determines your access to information, education, and even human connections. Even though we think of the Internet as open to everyone, there is a<a href="http://labs.theguardian.com/digital-language-divide/"> digital language divide</a> between dominant languages (mostly from the Western world) and others.<a href="https://w3techs.com/technologies/overview/content_language"> Only a few hundred languages are represented on the web</a> and speakers of minority languages are severely limited in the information available to them.</p><p>As many more languages are being written in informal contexts in chat apps and on social media, this divide extends to all levels of technology: At the most basic language technology level, low-resource languages lack keyboard support and spell checking (<a href="https://www.aclweb.org/anthology/L18-1656/">Soria et al., 2018</a>)—and keyboard support is even rarer for languages without a widespread written tradition (<a href="https://www.researchgate.net/publication/290279777_Keyboard_layouts_Lessons_from_the_me'phaa_and_sochiapam_Chinantec_designs">Paterson, 2015</a>). At a higher level, algorithms are biased and discriminate against speakers of <a href="https://qz.com/1141122/google-translates-gender-bias-pairs-he-with-hardworking-and-she-with-lazy-and-other-examples/">non-English</a> <a href="https://journals.sagepub.com/doi/10.1068/a44674">languages</a> or simply with <a href="https://www.washingtonpost.com/graphics/2018/business/alexa-does-not-understand-your-accent/">different</a> <a href="https://www.wired.com/2017/03/voice-is-the-next-big-platform-unless-you-have-an-accent/">accents</a>.</p><p>The latter is a problem because much existing work treats a high-resource language such as English as homogeneous. Our models consequently underperform on the plethora of related linguistic subcommunities, dialects, and accents (<a href="https://www.aclweb.org/anthology/D16-1120/">Blodgett et al., 2016</a>). In reality, the boundaries between language varieties are much blurrier than we make them out to be and language identification of similar languages and dialects is still a challenging problem (<a href="https://arxiv.org/abs/1804.08186">Jauhiainen et al., 2018</a>). For instance, even though Italian is the official language in Italy, there are around 34 regional languages and dialects spoken throughout the country.</p><p>A continuing lack of technological inclusion will not only exacerbate the language divide but it may also drive speakers of unsupported languages and dialects to high-resource languages with better technological support, further endangering such language varieties. To ensure that non-English language speakers are not left behind and at the same time to offset the existing imbalance, to lower language and literacy barriers, we need to apply our models to non-English languages.</p><h2 id="the-linguistic-perspective">The linguistic perspective</h2><p>Even though we claim to be interested in developing general language understanding methods, our methods are generally only applied to a single language, English.</p><p>English and the small set of other high-resource languages are in many ways not representative of the world's other languages. Many resource-rich languages belong to the Indo-European language family, are spoken mostly in the Western world, and are morphologically poor, i.e. information is mostly expressed syntactically, e.g. via a fixed word order and using <a href="https://en.wikipedia.org/wiki/Periphrasis">multiple separate words</a> rather than through variation at the word level.</p><p>For a more holistic view, we can take a look at the typological features of different languages. The<a href="https://wals.info/"> World Atlas of Language Structure</a> catalogues 192 typological features, i.e. structural and semantic properties of a language. For instance, one typological feature describes the typical <a href="https://wals.info/feature/81A#2/18.0/153.1">order of subject, object, and verb</a> in a language. Each feature has 5.93 categories on average. 48% of all feature categories exist only in the low-resource languages of groups 0–2 above and cannot be found in languages of groups 3–5 (<a href="https://arxiv.org/abs/2004.09095">Joshi et al., 2020</a>). Ignoring such a large subset of typological features means that our NLP models are potentially missing out on valuable information that can be useful for generalisation.</p><p>Working on languages beyond English may also help us gain new knowledge about the relationships between the languages of the world (<a href="https://www.aclweb.org/anthology/2020.acl-main.658/">Artetxe et al., 2020</a>). Conversely, it can help us reveal what linguistic features our models are able to capture. Specifically, you could use your knowledge of a particular language to probe aspects that differ from English such as the use of diacritics, extensive compounding, inflection, derivation, reduplication, agglutination, fusion, etc.</p><h2 id="the-ml-perspective">The ML perspective</h2><p>We encode assumptions into the architectures of our models that are based on the data we intend to apply them. Even though we intend our models to be general, many of their inductive biases are specific to English and languages similar to it.</p><p>The lack of any explicitly encoded information in a model does not mean that it is truly language agnostic. A classic example are n-gram language models, which perform significantly worse for languages with elaborate morphology and relatively free word order (<a href="https://journals.linguisticsociety.org/elanguage/lilt/article/view/2624.html">Bender, 2011</a>).</p><p>Similarly, neural models often overlook the complexities of morphologically rich languages (<a href="https://www.aclweb.org/anthology/2020.acl-main.660.pdf">Tsarfaty et al., 2020</a>): Subword tokenization performs poorly on languages with reduplication (<a href="https://arxiv.org/abs/1704.08352">Vania and Lopez, 2017</a>), byte pair encoding does not align well with morphology (<a href="https://arxiv.org/abs/2004.03720">Bostrom and Durrett, 2020</a>), and languages with larger vocabularies are more difficult for language models (<a href="https://www.aclweb.org/anthology/P19-1491/">Mielke et al., 2019</a>). Differences in grammar, word order, and syntax also cause problems for neural models (<a href="https://www.aclweb.org/anthology/W18-5412/">Ravfogel et al., 2018</a>;<a href="https://www.aclweb.org/anthology/N19-1253.pdf"> Ahmad et al., 2019</a>;<a href="https://arxiv.org/abs/2003.11080"> Hu et al., 2020</a>). In addition, we generally assume that pre-trained embeddings readily encode all relevant information, which may not be the case for all languages (<a href="https://www.aclweb.org/anthology/2020.acl-main.660.pdf">Tsarfaty et al., 2020</a>).</p><p>The above problems pose unique challenges for modelling structure—both on the word and the sentence level—, dealing with sparsity, few-shot learning, encoding relevant information in pre-trained representations, and transferring between related languages, among many other interesting directions. These challenges are not addressed by current methods and thus call for a new set of language-aware approaches.</p><p>Recent models have repeatedly matched human-level performance on increasingly difficult benchmarks—that is, in English using labelled datasets with thousands and unlabelled data with millions of examples. In the process, as a community we have overfit to the characteristics and conditions of English-language data. In particular, by focusing on high-resource languages, we have prioritised methods that work well only when large amounts of labelled and unlabelled data are available.</p><p>In contrast, most current methods break down when applied to the data-scarce conditions that are common for most of the world's languages. Even recent advances in pre-training language models that dramatically reduce the sample complexity for downstream tasks (<a href="https://www.aclweb.org/anthology/N18-1202/">Peters et al., 2018</a>; <a href="https://www.aclweb.org/anthology/P18-1031.pdf">Howard and Ruder, 2018</a>; <a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>; <a href="https://openreview.net/forum?id=r1xMH1BtvB">Clark et al., 2020</a>) require massive amounts of clean, unlabelled data, which is not available for most of the world's languages (<a href="https://www.aclweb.org/anthology/2020.acl-main.658/">Artetxe et al., 2020</a>). Doing well with few data is thus an ideal setting to test the limitations of current models—and evaluation on low-resource languages constitutes arguably its most impactful real-world application.</p><h2 id="the-cultural-and-normative-perspective">The cultural and normative perspective</h2><p>The data our models are trained on reveals not only the characteristics of the specific language but also sheds light on cultural norms and common sense knowledge.</p><p>However, such common sense knowledge may be different for different cultures. For instance, the notion of 'free' and 'non-free' varies cross-culturally where 'free' goods are ones that anyone can use without seeking permission, such as salt in a restaurant. Taboo topics are also different in different cultures. Furthermore, cultures vary in their assessment of relative power and social distance, among many other things (<a href="https://academic.oup.com/applij/article-abstract/4/2/91/167524?redirectedFrom=fulltext">Thomas, 1983</a>). In addition, many real-world situations such as ones included in the COPA dataset (<a href="https://ict.usc.edu/pubs/Choice%20of%20Plausible%20Alternatives-%20An%20Evaluation%20of%20Commonsense%20Causal%20Reasoning.pdf">Roemmele et al., 2011</a>) do not match the direct experience of many and equally do not reflect key situations that are obvious background knowledge for many people in the world (<a href="https://arxiv.org/abs/2005.00333">Ponti et al., 2020</a>).</p><p>Consequently, an agent that was only exposed to English data originating mainly in the Western world may be able to have a reasonable conversation with speakers from Western countries, but conversing with someone from a different culture may lead to pragmatic failures.</p><p>Beyond cultural norms and common sense knowledge, the data we train a model on also reflects the values of the underlying society. As an NLP researcher or practitioner, we have to ask ourselves whether we want our NLP system to exclusively share the values of a specific country or language community.</p><p>While this decision might be less important for current systems that mostly deal with simple tasks such as text classification, it will become more important as systems become more intelligent and need to deal with complex decision-making tasks.</p><h2 id="the-cognitive-perspective">The cognitive perspective</h2><p>Human children can acquire any natural language and their language understanding ability is remarkably consistent across all kinds of languages. In order to achieve human-level language understanding, our models should be able to show the same level of consistency across languages from different language families and typologies.</p><p>Our models should ultimately be able to learn abstractions that are not specific to the structure of any language but that can generalise to languages with different properties.</p><h2 id="what-you-can-do">What you can do</h2><p><strong>Datasets</strong>  If you create a new dataset, reserve half of your annotation budget for creating the same size dataset in another language.</p><p><strong>Evaluation</strong>  If you are interested in a particular task, consider evaluating your model on the same task in a different language. For an overview of some tasks, see <a href="http://nlpprogress.com/">NLP Progress</a> or our <a href="https://sites.research.google/xtreme">XTREME benchmark</a>.</p><p><strong>Bender Rule</strong>  <a href="https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/">State the language you are working on</a>.</p><p><strong>Assumptions</strong>  Be explicit about the signals your model uses and the assumptions it makes. Consider which are specific to the language you are studying and which might be more general.</p><p><strong>Language diversity</strong>  Estimate the language diversity of the sample of languages you are studying (<a href="https://arxiv.org/abs/2005.00333">Ponti et al., 2020</a>).</p><p><strong>Research</strong>  Work on methods that address challenges of low-resource languages. In the next post, I will outline interesting research directions and opportunities in multilingual NLP.</p><h2 id="citation">Citation</h2><p>For attribution in academic contexts, please cite this work as:</p><pre><code>@misc{ruder2020beyondenglish,
  author = {Ruder, Sebastian},
  title = {{Why You Should Do NLP Beyond English}},
  year = {2020},
  howpublished = {\url{http://ruder.io/nlp-beyond-english}},
}</code></pre><p></p><p><em>Thanks to Aida Nematzadeh, Laura Rimell, and Adhi Kuncoro for valuable feedback on drafts of this post.</em></p>]]></content:encoded></item><item><title><![CDATA[10 Tips for Research and a PhD]]></title><description><![CDATA[This post outlines 10 things that I did during my PhD and found particularly helpful in the long run.]]></description><link>http://ruder.io/10-tips-for-research-and-a-phd/</link><guid isPermaLink="false">5d440433808f272729699855</guid><category><![CDATA[advice]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Fri, 22 May 2020 13:23:32 GMT</pubDate><media:content url="http://ruder.io/content/images/2020/05/lessons_learned.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2020/05/lessons_learned.jpg" alt="10 Tips for Research and a PhD"><p>This advice should be most relevant to people studying machine learning (ML) and natural language processing (NLP) as that is what I did in my PhD. Having said that, this advice is not just limited to PhD students. If you are an independent researcher, want to start a PhD in the future or simply want to learn, then you will find most of this advice applicable.</p><p><strong>Pick and choose.</strong>  Everyone is different. You will have the most success if you adapt the particular advice to your situation and do what works for you. </p><p>TL;DR:</p><ol><li><a href="#1-read-broadly-">Read broadly.</a></li><li><a href="#2-work-on-two-things-">Work on two things.</a></li><li><a href="#3-be-ambitious-">Be ambitious.</a></li><li><a href="#4-collaborate-">Collaborate.</a></li><li><a href="#5-be-proactive-">Be proactive.</a></li><li><a href="#6-write-a-blog-">Write a blog.</a></li><li><a href="#7-keep-a-source-of-positive-energy-">Keep a source of positive energy.</a></li><li><a href="#8-play-to-your-strengths-">Play to your strengths.</a></li><li><a href="#9-intern-or-visit-a-university-">Intern or visit a university.</a></li><li><a href="#10-play-the-long-game-">Play the long game.</a></li></ol><h1 id="1-read-broadly-">1) Read broadly.</h1><p>While a PhD encourages you to delve deep into a specific topic, you can add value by making connections between different topics or entirely different fields. The papers that draw such connections can often be insightful. Many ideas in deep learning take inspiration from other fields such as biology (<a href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf">Hinton et al., 2014</a>), neuroscience (<a href="https://arxiv.org/abs/1611.05763">Wang et al., 2016</a>), physics (<a href="https://arxiv.org/abs/1902.04615">Cohen et al., 2019</a>), and many others.</p><p>In order to have a rich repertoire to draw on for inspiration, try to cultivate diverse interests. Look beyond your immediate horizon. Attend summer schools in other areas. Connect with people from other labs. Talk to people outside your subarea at conferences. Read papers from different disciplines.</p><p>ArXiv is a great source of research papers but staying up-to-date with the <a href="https://arxiv.org/help/subscribe">daily arXiv digest</a> feels like drinking from a firehose. Instead, I use services such as <a href="https://www.arxiv-sanity.com/">arXiv sanity preserver</a>, <a href="https://arxivist.com/">arXivist</a>, my Twitter feed, and recommendations from friends to stay up-to-date and to seek out different topics. I also generally prefer to read 10 papers superficially rather than one paper in-depth (as <a href="http://newsletter.ruder.io/issues/deep-learning-indaba-2018-edition-132825">suggested by Jeff Dean</a>). With a search-able paper management system (I use <a href="https://www.mendeley.com/">Mendeley</a>) you can always go back and reread the most relevant ones.</p><p>It can be helpful to dabble in different areas early in your PhD to get a sense for what interests you. Once you have found something, focus on the problems that you deeply care about. Think about the narrative you'd like to tell as part of your thesis.</p><h1 id="2-work-on-two-things-">2) Work on two things.</h1><p>While it is good to complete a project before starting a new one, working on a single project has downsides. If the project is not going well, your motivation and well-being may suffer. If you hit a roadblock, you can do nothing but grind until you resolve it. Developing such resilience is important but may at times come at a high mental cost.</p><p>Instead, I've found it useful to work on two projects at once to keep my sanity in check. If you hit a wall on one project, you can spend some time working on the other. This allows you to free up your mind and gain a new perspective, which may help you resolve the problem. If one of the projects is going well, this may also give you a boost to make progress on the other.</p><p>To minimise context-switching, I generally try to work on one project each day. It is also helpful if both projects are in similar areas so that you can apply what you learn on one project to the other one. </p><h1 id="3-be-ambitious-">3) Be ambitious.</h1><blockquote>“<em>Shoot for the moon. Even if you miss, you'll land among the stars.</em>”<br>—Norman Vincent Peale</blockquote><p>Another benefit of working on two projects is that it allows you to be more daring. You can work on a relatively safe project and one that is high-risk but also may be more impactful. The safe work ensures that you will graduate. The high-reward work may have a larger impact.</p><p>Ambitious projects demonstrate that you are creative and can come up with new ideas. Both are extremely valuable qualities. And even if such projects fail, they may lead you to discover unexpected insights that can lead to a publication.</p><p>Ambitious, however, does not mean that you should cater to the largest possible audience. A high impact can also be concentrated in a small community. A good indication of whether something you are working on is impactful is whether you'd be excited if it was published by someone else. In the end, you want to be known as someone that challenges the status quo and charts their own course.</p><h1 id="4-collaborate-">4) Collaborate.</h1><p>The PhD is often painted as a solitary affair, a lone journey on the quest towards knowledge. While you need to show substantial work that is your own in order to graduate, that does not mean that you are all on your own.</p><p>On the contrary, being able to collaborate is an important skill that you will need later on. Many projects with a large impact in ML and NLP such as <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">AlphaGo</a> or <a href="https://openai.com/blog/openai-five/">OpenAI Five</a> have been developed by a team. Whether you are part of a larger team or leading a group, you will have to collaborate with others.</p><p>Collaboration dynamics are more fluid compared to the adviser-PhD relationship, which is typically well defined. Collaborations are about <a href="http://colah.github.io/posts/2019-05-Collaboration/">building trust and mutual respect</a>. Successfully navigating collaborations takes practice. In collaborations, particularly if they are remote, it is important to communicate clearly and to set expectations.</p><p>If you are working on two projects, make one of them a collaboration. Collaborating with someone different from your advisor introduces you to a new perspective and will allow you to learn more than working on your own.</p><p>If you are based in a lab, collaborating with one of your lab mates is often the easiest choice. However, connecting and collaborating with people in other institutions may often be beneficial long-term.</p><h1 id="5-be-proactive-">5) Be proactive.</h1><p>This is probably the most important piece of advice. Don't restrict yourself to the people in your immediate circle.</p><p>Reach out to people. The main value of conferences is in bringing people together. Before a conference, look up who is going (by checking authors of accepted papers) and email them. Try to be respectful, briefly introduce yourself, and state why you'd like to meet them (a useful mnemonic is <a href="https://twitter.com/VikiLovesFACS/status/1202212904071811082">Inigo Montoya's Guide to Networking Success</a>). Most senior people make time for such meetings. Try to talk to many people and particularly seek out those who are not already well-known.</p><p>Outside of conferences, it is often useful to ask people who have worked in your area for research advice via email. It's amazing to see how many people in our field are genuinely helpful. <a href="https://tim.blog/2008/05/19/5-tips-for-e-mailing-busy-people/">Proper email etiquette</a> is important, however and makes it more likely that a busy researcher will respond. In particular, you should make it clear that you've done your research and explored alternative solutions before contacting them.</p><p>Beyond advice, such connections may lead to other opportunities further down the line: job offers, collaborations, mentorship, and even friendship. Many of my collaborations started through such connections—meetings at a conference, a cold email, a Twitter message. The important thing is that they are based on mutual interests and respect. So be conscious of other's people time. In addition, early career researchers with shared interests will often be much more open to collaborations than senior researchers who already have many commitments.</p><p>Being proactive also relates to how you view and talk about your research: Make it easy for other people to discover your work by highlighting it on your website, talking about it online, and writing a blog.</p><h1 id="6-write-a-blog-">6) Write a blog.</h1><p>Blogging has many advantages. It allows you to practice writing—and to learn to enjoy it. In order to finish your PhD, you will have to write a thesis, which can be an excruciating process. Blogging provides the training ground that prepares you for the thesis marathon.</p><p>From a research perspective, it allows you to practice <a href="https://www.nature.com/articles/d41586-019-02918-5">communicating and explaining things clearly</a>. Both are qualities that differentiate the best from mediocre research papers. In fact, <a href="https://www.cs.jhu.edu/~jason/advice/write-the-paper-first.html">clear writing</a> is important both to get your paper accepted and for high impact. In contrast to the hyper-compact format of research papers, a blog allows you to experiment and to find your own voice.</p><p>A blog can also be a great medium to present and share your work. A great blog post about a paper does not just reiterate its main findings but complements it. A blog can be <a href="https://worldmodels.github.io/">much</a> <a href="http://jalammar.github.io/illustrated-transformer/">more</a> <a href="https://distill.pub/2019/memorization-in-rnns/">flexible</a> than a paper: You can highlight interesting connections, provide the reader with a broad overview of the background literature and future directions, walk through an illustrative example, highlight code snippets or qualitative examples, show interactive visualisations, or perform an in-depth error analysis.</p><p>Another great way to start blogging is to discuss what you have just become knowledgeable about. Rachel Thomas puts this as "<a href="https://medium.com/@racheltho/why-you-yes-you-should-blog-7d2544ac1045">you are best positioned to help people one step behind you</a>". If you have just delved into a specialised area, why not save others the time and summarise the work and your insights. Most of my blog posts—from <a href="https://ruder.io/optimizing-gradient-descent/">gradient descent</a> to <a href="https://ruder.io/word-embeddings-1/">word embeddings</a>—started this way. If you have just learned how to do something cool, tell others about it. Conversely, if you want to learn about a certain topic but cannot find information about it online, consider creating that resource yourself. Starting your own blog <a href="https://www.fast.ai/2020/01/16/fast_template/">has never been easier</a>.</p><p>Having a blog is the single thing that has led to the most positive interactions throughout my PhD. ML and NLP have become so large that even if you write about a niche area, people will be interested. While I still feel anxious when I publish something, the response has always been worth it. In general, try to ignore unconstructive feedback and remember that the community appreciates genuine and honest voices.</p><h1 id="7-keep-a-source-of-positive-energy-">7) Keep a source of positive energy.</h1><p>External rewards such as paper acceptances are sparse, so leveraging intrinsic rewards is often necessary.</p><p>The most natural way to stay positive and energised in research is to work on something that excites you and to follow your curiosity. Depending on your funding or position, you might not be able to choose what you work on. In those cases, try to find a particular angle that excites you. Even an application of an existing algorithm can shed light on new and unsolved questions.</p><p>A PhD can be draining at the best of times. So it is important to build a support network that you can rely on. Surround yourself with positive people that support your ideas and ambitions. </p><p>At the same time, find an activity that you can fall back on to give you positive energy when things don't go as planned. This can be a collaboration, a side project, a hobby, exercise, meditation, or something else. For me, blogging filled this need. Compared to the long stretches of radio silence during peer review, writing, publishing and receiving feedback on a blog all within a couple of days feels liberating.</p><p>In the end, the most important resource is not the amount of compute you have, but your personal well-being. A crashed GPU can be rebooted; a burnt out GPU can't be fixed.</p><h1 id="8-play-to-your-strengths-">8) Play to your strengths.</h1><blockquote>“<em>The most value comes from doing something no else can do, or no one else has thought of.</em>”<br>—<a href="https://twitter.com/sama/status/1214274049074814976">Sam Altman</a></blockquote><p>With the increasing interest in ML and NLP, finding a fruitful undisturbed research topic can be challenging. A good strategy is to work on something that you are in the best position to tackle. Your ideal research topic sits at the intersection of work that is impactful, work that you are passionate about, and work that you are uniquely suited for.</p><p>What makes you uniquely suited can be one of many things: Your background; your knowledge of a particular technology, method, language, or data; your personal preferences. Do you come from a non-CS background? Use this as inspiration for your work. Are you a visually creative person? Supplement your blog and papers with graphs and analyses that will inspire others. Are you a strong coder? Implement technically challenging models. Are you great at maths? Prove your claims mathematically.</p><p>Another strength can be your network and the diversity of perspectives that you have access to. So locate others that complement your strengths, whether as advisors, mentors, or collaborators.</p><h1 id="9-intern-or-visit-a-university-">9) Intern or visit a university.</h1><p>The best way to make meaningful connections is to collaborate closely with people and to get to know them in-person. Internships and research visits are both excellent opportunities to expand your network as they enable you to work side-by-side with a group of talented people day-to-day.</p><p>They also allow you to get a feeling for how research is done in another environment. If you are considering whether to go into academia or industry, seeing first-hand how research is done in industry is an invaluable data point. A research visit or internship can also help you decide whether you would enjoy joining a lab or company at a later point.</p><p>Lastly, both are amazing learning experiences as you often will need to get familiar with a new tech stack or new research area. Through the guidance of a knowledgeable mentor different from your advisor, you will also be able to focus on different aspects of your personal growth.</p><h1 id="10-play-the-long-game-">10) Play the long game.</h1><p>Most of us are where we are because someone took a bet on us early on. My first research visit only happened because my host took a chance on me. So if you get the chance, pay it forward. Maximise not just the expected reward of yourself but of others around you.</p><p>While being at a big institution gives you access to an initial network, in the long term you want to develop a network of smart people that you can work with. One of the best ways to build a network is by being proactive and helping people as much as you can. This can be through writing blog posts or libraries, publishing tutorials and courses, doing podcasts, reimplementing models, or helping with open-source software. If you do this consistently, you will develop a reputation for being diligent and helpful and people will want to work with you.</p><p>Generally be kind to others. Assume good intentions. Be generous in giving praise and attribution. Don't hold grudges. In fact, being nice is one of the best things you can do to be successful (see Paul Graham's <a href="http://www.paulgraham.com/mean.html">Mean People Fail</a>). Being nice also has a recurring benefit as conferences are effectively—beyond the presentation and exchange of ideas—a yearly reunion of the friends you make along the way. </p><p>Take care of yourself. Work hard but get enough sleep and exercise. Take the time to learn new things. Work on things that you are not an expert at. In the end, always remind yourself that while a PhD is supposed to culminate in a thesis, the more important outcome of a PhD is a better version of yourself. </p><h1 id="references-and-inspiration">References and inspiration</h1><p>Finally, here are a few more pieces that served as inspiration:</p><ul><li>Andrey Karpathy's <a href="http://karpathy.github.io/2016/09/07/phd/">A Survival Guide to a PhD</a></li><li>The <a href="https://docs.google.com/document/d/18NoNdArdzDLJFQGBMVMsQ-iLOowP1XXDaSVRmYN0IyM/edit?usp=sharing">Frontiers in Natural Language Processing Expert Responses</a> at the Deep Learning Indaba 2018</li><li>John Schulman's <a href="http://joschu.net/blog/opinionated-guide-ml-research.html">An Opinionated Guide to ML Research</a></li><li>Andrey Kurenkov's <a href="https://www.andreykurenkov.com/writing/life/lessons-learned-from-failures/" rel="bookmark">Lessons Learned the Hard Way in Grad School (so far)</a></li><li>Volkan Cirik's <a href="https://www.cs.cmu.edu/~vcirik/blog/2019/phd-101/">PhD 101</a></li><li>Tim Dettmer's <a href="https://timdettmers.com/2020/03/10/how-to-pick-your-grad-school/">How to Pick Your Grad School</a></li><li>Isabelle Augenstein's <a href="https://medium.com/@isabelle.augenstein/increasing-well-being-in-academia-97f3ebc1599f">Increasing Well-Being in Academia</a></li><li>Richard Hamming's <a href="http://www.cs.virginia.edu/~robins/YouAndYourResearch.html">You and Your Research</a></li><li>Fei-Fei Li's <a href="https://bigaidream.gitbooks.io/tech-blog/content/2014/de-mystifying-good-research.html">De-Mystifying Good Research and Good Papers</a></li><li>Sam Altman's <a href="https://blog.samaltman.com/how-to-be-successful">How To Be Successful</a> and associated <a href="https://twitter.com/sama/status/1214274038933020672?lang=en">Twitter thread</a></li><li>Stuart K. Card's <a href="http://hci.stanford.edu/~cagatay/StuCard-WinPrizesGloryPhD.pdf">The PhD Thesis Deconstructed</a></li><li>Tweets from <a href="https://twitter.com/habdollahpouri/status/1201143604418187265">Himan Abdollahpouri</a>, <a href="https://twitter.com/chipro/status/1196889061799055360">Chip Huyen</a>, and many others</li></ul>]]></content:encoded></item><item><title><![CDATA[10 ML & NLP Research Highlights of 2019]]></title><description><![CDATA[This post gathers ten ML and NLP research directions that I found exciting and impactful in 2019.]]></description><link>http://ruder.io/research-highlights-2019/</link><guid isPermaLink="false">5dadee53c1f8023ac7c907cc</guid><category><![CDATA[natural language processing]]></category><category><![CDATA[cross-lingual]]></category><category><![CDATA[transfer learning]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Mon, 06 Jan 2020 08:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2020/01/videobert-1.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2020/01/videobert-1.png" alt="10 ML & NLP Research Highlights of 2019"><p>This post gathers ten ML and NLP research directions that I found exciting and impactful in 2019.</p><p>For each highlight, I summarise the main advances that took place this year, briefly state why I think it is important, and provide a short outlook to the future.</p><p>The full list of highlights is here:</p><ol><li><a href="#1-universal-unsupervised-pretraining">Universal unsupervised pretraining</a></li><li><a href="#2-lottery-tickets">Lottery tickets</a></li><li><a href="#3-the-neural-tangent-kernel">The Neural Tangent Kernel</a></li><li><a href="#4-unsupervised-multilingual-learning">Unsupervised multilingual learning</a></li><li><a href="#5-more-robust-benchmarks">More robust benchmarks</a></li><li><a href="#6-ml-and-nlp-for-science">ML and NLP for science</a></li><li><a href="#7-fixing-decoding-errors-in-nlg">Fixing decoding errors in NLG</a></li><li><a href="#8-augmenting-pretrained-models">Augmenting pretrained models</a></li><li><a href="#9-efficient-and-long-range-transformers">Efficient and long-range Transformers</a></li><li><a href="#10-more-reliable-analysis-methods">More reliable analysis methods</a></li></ol><h1 id="1-universal-unsupervised-pretraining">1) Universal unsupervised pretraining</h1><p><strong>What happened?</strong>  Unsupervised pretraining was prevalent in NLP this year, mainly driven by BERT (<a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>) and other variants. <a href="https://twitter.com/xwang_lk/status/1166187855456002049">A whole range of BERT variants</a> have been applied to multimodal settings, mostly involving images and videos together with text (for an example see the figure below). Unsupervised pretraining has also made inroads into domains where supervision had previously reigned supreme. In biology, Transformer language models have been pretrained on protein sequences (<a href="https://www.biorxiv.org/content/10.1101/622803v1.full">Rives et al., 2019</a>). In computer vision, approaches leveraged self-supervision including CPC (<a href="https://arxiv.org/abs/1905.09272">Hénaff et al., 2019</a>), MoCo (<a href="https://arxiv.org/abs/1911.05722">He et al., 2019</a>), and PIRL (<a href="http://ruder.io/research-highlights-2019/arxiv.org/abs/1912.01991">Misra &amp; van der Maaten, 2019</a>) as well as strong generators such as BigBiGAN (<a href="https://arxiv.org/abs/1907.02544">Donahue &amp; Simonyan, 2019</a>) to improve sample efficiency on ImageNet and image generation. In speech, representations learned with a multi-layer CNN (<a href="https://arxiv.org/abs/1904.05862">Schneider et al., 2019</a>) or bidirectional CPC (<a href="https://openreview.net/forum?id=HJe-blSYvH">Kawakami et al., 2019</a>) outperform state-of-the-art models with much less training data.</p><p><strong>Why is it important?</strong>  Unsupervised pretraining enables training models with much fewer labelled examples. This opens up new applications in many different domains where data requirements were previously prohibitive.</p><p><strong>What's next?</strong>  Unsupervised pretraining is here to stay. While the biggest advances have been achieved so far in individual domains, it will be interesting to see a focus towards tighter integration of multiple modalities.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/videobert.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>VideoBERT (<a href="https://arxiv.org/abs/1904.01766">Sun et al., 2019</a>), a recent multimodal variant of BERT that generates video "tokens" given a recipe (above) and predicts future tokens at different time scales given a video token (below).</figcaption></figure><h1 id="2-lottery-tickets">2) Lottery tickets</h1><p><strong>What happened?  </strong><a href="https://openreview.net/forum?id=rJl-b3RcF7">Frankle and Carbin</a> (2019) identified <em>winning tickets</em>, subnetworks in dense, randomly-initialised, feed-forward networks that are so well initialised that training them in isolation achieves similar accuracy to training the full network, as can be seen below. While the initial pruning procedure only worked on small vision tasks, later work (<a href="https://arxiv.org/abs/1903.01611">Frankle et al., 2019</a>) applied the pruning early in training instead of at initialisation, which makes it possible to find small subnetworks of deeper models. <a href="https://arxiv.org/abs/1906.02768">Yu et al. (2019)</a> find winning ticket initialisations also for LSTMs and Transformers in NLP and RL models. While winning tickets are still expensive to find, it is promising that they seem to be transferable across datasets and optimisers (<a href="https://papers.nips.cc/paper/8739-one-ticket-to-win-them-all-generalizing-lottery-ticket-initializations-across-datasets-and-optimizers.pdf">Morcos et al., 2019</a>) and domains (<a href="https://www.aclweb.org/anthology/D19-6117/">Desai et al., 2019</a>).</p><p><strong>Why is it important?</strong>  State-of-the-art neural networks are getting larger and more expensive to train and to use for prediction. Being able to consistently identify small subnetworks that achieve comparable performance enables training and inference with much fewer resources. This can speed up model iteration and opens up new applications in on-device and edge computing.</p><p><strong>What's next?</strong>  Identifying winning tickets is currently still too expensive to provide real benefits in low-resource settings. More robust one-shot pruning methods that are less susceptible to noise in the pruning process should mitigate this. Investigating what makes winning tickets special should also help us gain a better understanding of the initialisation and learning dynamics of neural networks. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/winning_tickets.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>Test accuracy of winning tickets (solid lines) vs. randomly sampled subnetworks (dashed lines) at different pruning ratios (<a href="https://openreview.net/forum?id=rJl-b3RcF7">Frankle &amp; Carbin, 2019</a>).</figcaption></figure><h1 id="3-the-neural-tangent-kernel">3) The Neural Tangent Kernel</h1><p><strong>What happened?  </strong>Somewhat counter-intuitively, very wide (more concretely, <em>infinitely</em> wide) neural networks are easier to study theoretically than narrow ones. It has been shown that in the infinite-width limit, neural networks can be approximated as linear models with a kernel, the Neural Tangent Kernel (NTK; <a href="https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">Jacot et al., 2018</a>). Refer to <a href="https://rajatvd.github.io/NTK/">this post </a>for an intuitive explanation of NTK including an illustration of its training dynamics (see the figure below). In practice, such models, have underperformed their finite-depth counterparts (<a href="https://openreview.net/pdf?id=B1g30j0qF7">Novak et al., 2019</a>; <a href="https://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers">Allen-Zhu et al., 2019</a>; <a href="http://papers.nips.cc/paper/9449-on-the-inductive-bias-of-neural-tangent-kernels">Bietti &amp; Mairal, 2019</a>), which limits applying the findings to standard methods. Recent work (<a href="https://arxiv.org/abs/1911.00809">Li et al., 2019</a>; <a href="https://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf">Arora et al., 2019</a>), however, has significantly reduced the performance gap to standard methods (see <a href="https://huyenchip.com/2019/12/18/key-trends-neurips-2019.html">Chip Huyen's post</a> for other related NeurIPS 2019 papers).</p><p><strong>Why is it important?</strong>  The NTK is perhaps the most powerful tool at our disposal to analyse the theoretical behaviour of neural networks. While it has its limitations, i.e. practical neural networks still perform better than their NTK counterparts, and insights so far have not translated into empirical gains, it may help us open the black box of deep learning.</p><p><strong>What's next?</strong>  The gap to standard methods seems to be mainly due to benefits of the finite width of such methods, which future work may seek to characterise. This will hopefully also help translating insights from the infinite-width limit to practical settings. Ultimately, the NTK may help us shed light on the training dynamics and generalisation behaviour of neural networks. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/ntk_dynamics.gif" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>Learning dynamics of linear models with an NTK with different α factors. NTKs are visualised as ellipses (credit: <a href="https://rajatvd.github.io/NTK/">Rajat's Blog</a>).</figcaption></figure><h1 id="4-unsupervised-multilingual-learning">4) Unsupervised multilingual learning</h1><p><strong>What happened?  </strong>Cross-lingual representations had mostly focused on the word level for many years (<a href="https://www.jair.org/index.php/jair/article/view/11640">see this survey</a>). Building on advances in unsupervised pretraining, this year saw the development of deep cross-lingual models such as <a href="https://github.com/google-research/bert/blob/master/multilingual.md">multilingual BERT</a>, XLM (<a href="https://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining.pdf">Conneau &amp; Lample, 2019</a>), and XLM-R (<a href="https://arxiv.org/abs/1911.02116">Conneau et al., 2019</a>). Even though these models do not use any explicit cross-lingual signal, they generalise surprisingly well across languages—even without a shared vocabulary or joint training (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>; <a href="https://openreview.net/forum?id=HJeT3yrtDr">Karthikeyan et al., 2019</a>; <a href="https://arxiv.org/abs/1911.01464">Wu et al., 2019</a>). For an overview, have a look at <a href="https://ruder.io/unsupervised-cross-lingual-learning/#unsupervised-deep-models">this post</a>. Such deep models also brought improvements in unsupervised MT (<a href="https://arxiv.org/abs/1905.02450">Song et al., 2019</a>; <a href="https://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining.pdf">Conneau &amp; Lample, 2019</a>), which hit its stride last year (see <a href="https://ruder.io/10-exciting-ideas-of-2018-in-nlp/#1-unsupervised-mt">highlights of 2018</a>) and saw improvements from a more principled combination of statistical and neural approaches (<a href="https://www.aclweb.org/anthology/P19-1019/">Artetxe et al., 2019</a>). Another exciting development is the bootstrapping of deep multilingual models from readily available pretrained representations in English (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>; <a href="https://openreview.net/forum?id=Bkle6T4YvB">Tran, 2020</a>), which can be seen below.</p><p><strong>Why is it important?</strong>  Ready-to-use cross-lingual representations enable training of models with fewer examples for languages other than English. Furthermore, if labelled data in English is available, these methods enable essentially free zero-shot transfer. They may finally help us gain a better understanding of the relationships between different languages.</p><p><strong>What's next?</strong>  It is still unclear why these methods work so well without any cross-lingual supervision. Gaining a better understanding of how these methods work will likely enable us to design more powerful methods and may also reveal insights about the structure of different languages. In addition, we should not only focus on zero-shot transfer but also consider learning from few labelled examples in the target language (see <a href="http://nlp.fast.ai/classification/2019/09/10/multifit.html">this post</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/monolingual_transfer.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>The four steps of the monolingual transfer approach of <a href="https://arxiv.org/abs/1910.11856">Artetxe et al. (2019)</a></figcaption></figure><h1 id="5-more-robust-benchmarks">5) More robust benchmarks</h1><blockquote>There is something rotten in the state of the art.<br>—<a href="https://arxiv.org/abs/1910.14599">Nie et al. (2019)</a> paraphrasing <a href="http://www.shakespeare-online.com/quickquotes/quickquotehamletdenmark.html">Shakespeare</a></blockquote><p><strong>What happened?  </strong>Recent NLP datasets such as HellaSWAG (<a href="https://www.aclweb.org/anthology/P19-1472/">Zellers et al., 2019</a>) are created to be difficult for state-of-the-art models to solve. Examples are filtered by humans to explicitly retain only those where state-of-the-art models fail (see below for an example). This process of human-in-the-loop adversarial curation can be repeated multiple times such as in the recent Adversarial NLI (<a href="https://arxiv.org/abs/1910.14599">Nie et al., 2019</a>) benchmark to enable the creation of datasets that are much more challenging for current methods.</p><p><strong>Why is it important?</strong>  Many researchers have observed that current NLP models do not learn what they are supposed to but instead adopt shallow heuristics and exploit superficial cues in the data (described as <a href="https://thegradient.pub/nlps-clever-hans-moment-has-arrived/">the Clever Hans moment</a>). As datasets become more robust, we would hope that models will be forced to eventually learn the true underlying relations in the data.</p><p><strong>What's next?</strong>  As models become better, most datasets will need to be continuously improved or will quickly become outdated. Dedicated infrastructure and tools will be necessary to facilitate this process. In addition, appropriate baselines should be run including simple methods and models using different variants of the data (such as with incomplete input) so that initial versions of datasets are as robust as possible.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/hellaswag.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>A multiple-choice sentence completion example from HellaSWAG that is difficult to answer for state-of-the-art models. Most hard examples lie in a "Goldilocks zone" of complexity, consisting roughly of three sentences of context and two generated sentences (<a href="https://www.aclweb.org/anthology/P19-1472/">Zellers et al., 2019</a>).</figcaption></figure><h1 id="6-ml-and-nlp-for-science">6) ML and NLP for science</h1><p><strong>What happened?  </strong>There have been some major advances of ML being applied to fundamental science problems. My highlights were the application of deep neural networks to <a href="https://www.nature.com/articles/d41586-019-01357-6">protein folding</a> and to the Many-Electron Schrödinger Equation (<a href="https://arxiv.org/abs/1909.02487">Pfau et al., 2019</a>). On the NLP side, it is exciting to see what impact even standard methods can have when combined with domain expertise. One study used word embeddings to analyse latent knowledge in the materials science literature (<a href="https://www.nature.com/articles/s41586-019-1335-8">Tshitoyan et al., 2019</a>), which can be used to predict which materials will have certain properties (see the figure below). In biology, a lot of data such as genes and proteins is sequential in nature. It is thus a natural fit for NLP methods such as LSTMs and Transformers, which have been applied to protein classification (<a href="https://www.biorxiv.org/content/10.1101/704874v2">Strodthoff et al., 2019</a>; <a href="https://www.biorxiv.org/content/10.1101/622803v2">Rives et al., 2019</a>).</p><p><strong>Why is it important?  </strong>Science is arguably one of the most impactful application domains for ML. Solutions can have a large impact on many other domains and can help solve real-world problems.</p><p><strong>What's next?</strong>  From modelling energy in physics problems (<a href="http://papers.nips.cc/paper/9672-hamiltonian-neural-networks.pdf">Greydanus et al., 2019</a>) to solving differential equations (<a href="https://openreview.net/forum?id=S1eZYeHFDS">Lample &amp; Charton, 2020</a>), ML methods are constantly being applied to new applications in science. It will be interesting to see what the most impactful of these will be in 2020.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/historical_validations_functional_material_predictions.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>Using word embeddings trained on abstracts from different time periods to predict which materials will be studied as ferroelectric (a), photovoltaics (b), and topological insulator (c) in future abstracts. Top 50 predictions are much more likely to be studied compared to all candidate materials (<a href="https://www.nature.com/articles/s41586-019-1335-8">Tshitoyan et al., 2019</a>).</figcaption></figure><h1 id="7-fixing-decoding-errors-in-nlg">7) Fixing decoding errors in NLG</h1><p><strong>What happened?</strong>  Despite ever more powerful models, natural language generation (NLG) models still frequently produce repetitions or gibberish as can be seen below. This was shown to be mainly a result of the maximum likelihood training. I was excited to see improvements that aim to ameliorate this and are orthogonal to advances in modelling. Such improvements came in the form of new sampling methods, such as nucleus sampling (<a href="https://openreview.net/forum?id=rygGQyrFvH">Holtzman et al., 2019</a>) and new loss functions (<a href="https://openreview.net/forum?id=SJeYe0NtvH">Welleck et al., 2019</a>). Another surprising finding was that better search does not lead to better generations: Current models rely to some extent on an imperfect search and beam search errors. In contrast, an exact search most often returns an empty translation in the case of machine translation (<a href="https://www.aclweb.org/anthology/D19-1331.pdf">Stahlberg &amp; Byrne, 2019</a>). This shows that advances in search and modelling must often go hand in hand.</p><p><strong>Why is it important? </strong> Natural language generation is one of the most general tasks in NLP. In NLP and ML research, most papers focus on improving the model, while other parts of the pipeline are typically neglected. For NLG, it is important to remind ourselves that our models still have flaws and that it may be possible to improve the output by fixing the search or the training process.</p><p><strong>What's next?</strong>  Despite more powerful models and successful applications of transfer learning to NLG (<a href="https://arxiv.org/abs/1905.02450">Song et al., 2019</a>; <a href="https://arxiv.org/abs/1901.08149">Wolf et al., 2019</a>), model predictions still contain many artefacts. Identifying and understanding the causes of such artefacts is an important research direction.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/text_degeneration.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>Repetitions (blue) and gibberish (red) produced by GPT-2 using beam search and pure (greedy) sampling (<a href="https://openreview.net/forum?id=rygGQyrFvH">Holtzman et al., 2019</a>).</figcaption></figure><h1 id="8-augmenting-pretrained-models">8) Augmenting pretrained models</h1><p><strong>What happened?  </strong>I was excited to see approaches that equip pretrained models with new capabilities. Some methods augment a pretrained model with a knowledge base in order to improve modelling of entity names (<a href="https://www.aclweb.org/anthology/N19-1117/">Liu et al., 2019</a>) and the recall of facts (<a href="https://www.aclweb.org/anthology/P19-1598/">Logan et al., 2019</a>). Others enable it to perform simple arithmetic reasoning (<a href="https://www.aclweb.org/anthology/D19-1609/">Andor et al., 2019</a>) by giving it access to a number of predefined executable programs. As most models have a weak inductive bias and learn most of their knowledge from data, another way to extend a pretrained model is by augmenting the training data itself, e.g. to capture common sense (<a href="https://www.aclweb.org/anthology/P19-1470/">Bosselut et al., 2019</a>) as can be seen below.</p><p><strong>Why is it important?  </strong>Models are becoming more powerful but <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5888218f39_11_238">there are many things</a> that a model cannot learn from text alone. Particularly when dealing with more complex tasks, the available data may be too limited to learn explicit reasoning using facts or common sense and a stronger inductive bias may often be necessary.</p><p><strong>What's next?</strong>  As models are being applied to more challenging problems, it will increasingly become necessary for modifications to be compositional. In the future, we might combine powerful pretrained models with learnable compositional programs (<a href="https://papers.nips.cc/paper/9608-learning-compositional-neural-programs-with-recursive-tree-search-and-planning">Pierrot et al., 2019</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/comet.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>A standard Transformer with multi-head attention. The model is trained to predict the object of a knowledge base triple given its subject and relation (<a href="https://www.aclweb.org/anthology/P19-1470/">Bosselut et al., 2019</a>).</figcaption></figure><h1 id="9-efficient-and-long-range-transformers">9) Efficient and long-range Transformers</h1><p><strong>What happened?  </strong>This year saw several improvements to the Transformer (<a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need">Vaswani et al., 2017</a>) architecture. The Transformer-XL (<a href="https://www.aclweb.org/anthology/P19-1285/">Dai et al., 2019</a>) and the Compressive Transformer (<a href="https://openreview.net/forum?id=SylKikSYDH">Rae et al., 2020</a>), which can be seen below enable it to better capture long-range dependencies. Many approaches sought to make the Transformer more efficient mostly using different—often sparse—attention mechanisms, such as adaptively sparse attention (<a href="https://www.aclweb.org/anthology/D19-1223/">Correia et al., 2019</a>), adaptive attention spans (<a href="https://www.aclweb.org/anthology/P19-1032/">Sukhbaatar et al., 2019</a>), product-key attention (<a href="https://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys">Lample et al., 2019)</a>, and locality-sensitive hashing (<a href="https://openreview.net/forum?id=rkgNKkHtvB">Kitaev et al., 2020</a>). On the Transformer-based pretraining front, there have been more efficient variants such as ALBERT (<a href="https://openreview.net/forum?id=H1eA7AEtvS">Lan et al., 2020</a>), which employs parameter sharing and ELECTRA (<a href="https://openreview.net/forum?id=r1xMH1BtvB">Clark et al., 2020</a>), which uses a more efficient pretraining task. There were also more efficient pretrained models that did not utilise a Transformer, such as the unigram document model VAMPIRE (<a href="https://www.aclweb.org/anthology/P19-1590/">Gururangan et al., 2019</a>) and the QRNN-based MultiFiT (<a href="https://www.aclweb.org/anthology/D19-1572/">Eisenschlos et al., 2019</a>). Another trend was the distillation of large BERT models into smaller ones (<a href="https://arxiv.org/abs/1903.12136">Tang et al., 2019</a>; <a href="https://www.aclweb.org/anthology/D19-1374/">Tsai et al., 2019</a>; <a href="https://arxiv.org/abs/1910.01108">Sanh et al., 2019</a>).</p><p><strong>Why is it important?</strong>  The Transformer architecture has been influential since its inception. It is a part of most state-of-the-art models in NLP and has been successfully applied to many other domains (see Sections <a href="#1-universal-unsupervised-pretraining">1</a> and <a href="#6-ml-and-nlp-for-science">6</a>). Any improvement to the Transformer architecture may thus have strong ripple effects.</p><p><strong>What's next?</strong>  It will take some time for these improvements to trickle down to the practitioner but given the prevalence and ease of use of pretrained models, more efficient alternatives will likely be adopted quickly. Overall, we will hopefully see a continuing focus on model architectures that emphasise efficiency, with sparsity being one of the key trends. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/compressive_transformer.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>The Compressive Transformer compresses (a fine-grained memory of) past activations into a coarser compressed memory (<a href="https://openreview.net/forum?id=SylKikSYDH">Rae et al., 2020</a>).</figcaption></figure><h1 id="10-more-reliable-analysis-methods">10) More reliable analysis methods</h1><p><strong>What happened?</strong>  A key trend for me this year was the increasing number of papers analysing models. In fact, several of my favourite papers this year were such analysis papers. An early highlight was the excellent survey of analysis methods by <a href="https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00254">Belinkov &amp; Glass (2019)</a>. This year was also the first one (in my memory) where many papers were dedicated to analysing a single model, BERT (such papers are known as <a href="http://newsletter.ruder.io/issues/bert-gpt-2-xlnet-naacl-icml-arxiv-eurnlp-180092">BERTology</a>). In this context, probes (see the figure below), which aim to understand whether a model captures morphology, syntax, etc. by predicting certain properties have become a common tool. I particularly appreciated papers that make probes more reliable (<a href="https://www.aclweb.org/anthology/N19-1225/">Liu et al., 2019</a>; <a href="https://www.aclweb.org/anthology/D19-1275/">Hewitt &amp; Liang, 2019</a>). Reliability is also a theme in the ongoing conversation on whether attention provides meaningful explanations (<a href="https://www.aclweb.org/anthology/N19-1357/">Jain &amp; Wallace, 2019</a>; <a href="https://www.aclweb.org/anthology/D19-1002/">Wiegreffe &amp; Pinter, 2019</a>; <a href="https://medium.com/@byron.wallace/thoughts-on-attention-is-not-not-explanation-b7799c4c3b24">Wallace, 2019</a>). The continuing interest in analysis methods is perhaps best exemplified by the new ACL 2020 track on <a href="https://acl2020.org/calls/papers/">Interpretability and Analysis of Models in NLP</a>.</p><p><strong>Why is it important?  </strong>State-of-the-art methods are used as black boxes. In order to develop better models and to use them in the real world, we need to understand why models make certain decisions. However, our current methods to explain models' predictions are still limited. </p><p><strong>What's next?  </strong>We need more work on explaining predictions that goes beyond visualisation, which is often unreliable. An important trend in this direction are human-written explanations that are being provided by more datasets (<a href="https://papers.nips.cc/paper/8163-e-snli-natural-language-inference-with-natural-language-explanations">Camburu et al., 2018</a>; <a href="https://www.aclweb.org/anthology/P19-1487/">Rajani et al., 2019</a>; <a href="https://arxiv.org/abs/1910.14599">Nie et al., 2019</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/probing_landscape.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>The probing setup used to study linguistic knowledge in representations (<a href="https://www.aclweb.org/anthology/N19-1225/">Liu et al., 2019</a>).</figcaption></figure>]]></content:encoded></item><item><title><![CDATA[Unsupervised Cross-lingual Representation Learning]]></title><description><![CDATA[This post expands on the ACL 2019 tutorial on Unsupervised Cross-lingual Representation Learning. It highlights key insights and takeaways and provides updates based on recent work, particularly unsupervised deep multilingual models.]]></description><link>http://ruder.io/unsupervised-cross-lingual-learning/</link><guid isPermaLink="false">5d514420808f2727296998ff</guid><category><![CDATA[cross-lingual]]></category><category><![CDATA[transfer learning]]></category><category><![CDATA[natural language processing]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Sat, 26 Oct 2019 11:35:18 GMT</pubDate><media:content url="http://ruder.io/content/images/2019/11/unsupervised_multilingual_overview.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2019/11/unsupervised_multilingual_overview.png" alt="Unsupervised Cross-lingual Representation Learning"><p>This post expands on the <a href="https://tinyurl.com/xlingual">ACL 2019 tutorial on Unsupervised Cross-lingual Representation Learning</a>.</p><p>The tutorial was organised by <a href="https://sites.google.com/site/ivanvulic/">Ivan Vulić</a>, <a href="https://anderssoegaard.github.io/">Anders Søgaard</a>, and me. In this post, I highlight key insights and takeaways and provide additional context and updates based on recent work. In particular, I cover <a href="#unsupervised-deep-models">unsupervised deep multilingual models</a> such as multilingual BERT. You can see the structure of this post below:</p><figure class="kg-card kg-image-card"><img src="http://ruder.io/content/images/2019/10/unsupervised_multilingual_overview.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"></figure><p>The slides of the tutorial are <a href="https://docs.google.com/presentation/d/1miPu0hkBFSRTAxP-Psg0xXOvAbVSJPhQXehMQdUI88I/edit?usp=sharing">available online</a>.</p><h1 id="introduction">Introduction</h1><p>Cross-lingual representation learning can be seen as an instance of transfer learning, similar to domain adaptation. The domains in this case are different languages. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/10/cross_lingual_learning_taxonomy.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>Cross-lingual learning in the transfer learning taxonomy (<a href="https://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=64">Ruder, 2019</a>)</figcaption></figure><p>Methods from domain adaptation have also been applied to cross-lingual transfer (<a href="https://arxiv.org/abs/1008.0716">Prettenhofer &amp; Stein, 2011</a>, <a href="https://pdfs.semanticscholar.org/f08f/269b715c31bfa02dc4df31f5561fd74222f0.pdf">Wan et al., 2011</a>). For a clearer distinction between domain adaptation and cross-lingual learning, have a look at <a href="https://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=63">this section</a>.</p><p>Viewing cross-lingual learning as a form of transfer learning can help us understand why it might be useful and when it might fail: </p><ol><li>Transfer learning is useful whenever the scenarios between which we transfer share some underlying structure. In a similar vein, languages share commonalities on many levels—from <a href="https://en.wikipedia.org/wiki/Loanword">loanwords</a> and <a href="https://en.wikipedia.org/wiki/Cognate">cognates</a> on the lexical level, to the <a href="https://universaldependencies.org/">structure of sentences</a> on the syntactic level, to the <a href="https://en.wikipedia.org/wiki/Semantic_primes">meaning of words</a> on the semantic level. Learning about the structure of one language may therefore help us do better when learning a second language.</li><li>Transfer learning struggles when the source and target settings are too different. Similarly, it is harder to transfer between languages that are dissimilar (e.g. that have different typological features in the <a href="https://wals.info/languoid">World Atlas of Language Structures</a>).</li></ol><p>Similar to the <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5888218f39_46_122">broader transfer learning landscape</a>, much of the work on cross-lingual learning over the last years has focused on learning representations of words. While there have been approaches that learn sentence-level representations, only recently are we witnessing the emergence of <a href="#unsupervised-deep-models">deep multilingual models</a>. Have a look at <a href="https://www.jair.org/index.php/jair/article/view/11640/26511">this survey</a> for an overview of the history of cross-lingual models. For more on transfer learning, check out <a href="http://ruder.io/state-of-transfer-learning-in-nlp/">this recent post</a>. Cross-lingual learning might be useful—but why should we care about applying NLP to other languages in the first place?</p><h2 id="the-digital-language-divide">The Digital Language Divide</h2><p>The language you speak shapes your experience of the world. This is known as the <a href="https://en.wikipedia.org/wiki/Linguistic_relativity">Sapir-Whorf hypothesis</a> and is contested. On the other hand, it is factual that the language you speak shapes your experience of the world <em>online</em>. What language you speak determines your access to information, education, and even human connections. In other words, even though we think of the Internet as being open to anyone, there is a <a href="http://labs.theguardian.com/digital-language-divide/">digital language divide</a> between dominant languages (mostly from the western world) and others.</p><p>As NLP technologies are becoming more commonplace, this divide extends to the technological level: At best, this means that non-English language speakers do not benefit from the latest features of digital assistants; at worst, algorithms are biased against or discriminate against speakers of non-English languages—we can see <a href="https://www.washingtonpost.com/graphics/2018/business/alexa-does-not-understand-your-accent/">occurrences</a> <a href="https://www.wired.com/2017/03/voice-is-the-next-big-platform-unless-you-have-an-accent/">of this</a> <a href="https://qz.com/1141122/google-translates-gender-bias-pairs-he-with-hardworking-and-she-with-lazy-and-other-examples/">already</a> <a href="https://journals.sagepub.com/doi/10.1068/a44674">today</a>. To ensure that non-English language speakers are not left behind and at the same time to offset the existing imbalance, we need to apply our models to non-English languages.</p><h2 id="the-nlp-resource-hierarchy">The NLP Resource Hierarchy</h2><p>In current machine learning, the amount of available training data is the main factor that influences an algorithm's performance. Data that is useful for current NLP models can range from unlabelled data in the form of documents online or articles on Wikipedia, labelled data in the form of large curated datasets, or parallel corpora used in machine translation.</p><p>We can roughly differentiate between languages with many and languages with few data resources in a hierarchy, which can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/10/nlp_resource_hierarchy.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>A conceptual view of the NLP resource hierarchy</figcaption></figure><p>This approximately corresponds with a language's presence online. Note that many languages cannot be assigned clearly to a single level of the hierarchy. Some languages with few speakers such as <a href="https://en.wikipedia.org/wiki/Aragonese_language">Aragonese</a> or <a href="https://en.wikipedia.org/wiki/Waray_language">Waray-Waray</a> have a <a href="https://meta.wikimedia.org/wiki/List_of_Wikipedias_by_speakers_per_article">disproportionally</a> <a href="https://meta.wikimedia.org/wiki/List_of_Wikipedias">large Wikipedia</a>, while for others such as <a href="https://en.wikipedia.org/wiki/Latvian_language">Latvian</a> <a href="https://www.statmt.org/europarl/">parallel data</a> is available due to their status as an official language. On the other hand, many languages with tens of millions of speakers such as <a href="https://en.wikipedia.org/wiki/Xhosa_language">Xhosa</a> and <a href="https://en.wikipedia.org/wiki/Zulu_language">Zulu</a> have barely more than <a href="https://meta.wikimedia.org/wiki/List_of_Wikipedias">1,000 Wikipedia articles</a>.</p><p>Nevertheless, the existence of such a hierarchy highlights why unsupervised cross-lingual representation learning is promising: for most of the world's languages, no labelled data is available and creating labelled data is expensive as native speakers are under-represented online. In order to train models for such languages, we need to make use of unlabelled data and transfer learning from high-resource languages.</p><h2 id="cross-lingual-representations">Cross-lingual Representations</h2><p>Whereas in the past it was necessary to learn language-specific models for every language and preprocessing and features needed to be applied to every language in isolation, cross-lingual representation learning enables us to apply the <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g56add75db5_0_104">transfer learning formula</a> of pretraining and adaptation <em>across languages</em>. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/10/transfer_learning_formula.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>The transfer learning formula (<a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g56add75db5_0_104">NAACL 2019 Transfer learning tutorial</a>)</figcaption></figure><!--kg-card-begin: markdown--><p>In this context, we will talk about a (high-resource) source language \(L_1\) where unlabelled data and labelled data for a particular task are available, and a (low-resource) target language \(L_2\)  where only unlabelled data is available. For cross-lingual transfer learning, we add one additional step to the transfer learning recipe above:</p>
<ol>
<li><strong>Pretraining</strong>: Learn <em>cross-lingual</em> representations that are shared across languages. This is what the rest of this post is about.</li>
<li><strong>Adaptation</strong>: Adapt the model to the labelled data of the desired task in \(L_1\) by learning task-specific parameters on top of the cross-lingual representations. The cross-lingual parameters are often kept fixed (this is the same as <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5888218f39_59_11">feature extraction</a>). As the task-specific parameters are learned on top of cross-lingual ones, they are expected to transfer to the other language.</li>
<li><strong>Zero-shot transfer</strong>: The model, which captures cross-lingual information, can be applied directly to perform inference on data of the target language.</li>
</ol>
<!--kg-card-end: markdown--><p>If labelled data in the target language is also available, then the model can be fine-tuned jointly on both languages in Step 2.</p><p>Throughout this post, I will focus on approaches that learn cross-lingual representations as the means for transferring across languages. Depending on the task, there are other ways of transferring information across languages such as by domain adaptation (as seen above), annotation projection (<a href="https://arxiv.org/pdf/1401.5694.pdf">Padó &amp; Lapata, 2009</a>; <a href="https://arxiv.org/abs/1707.02483">Ni et al., 2017</a>; <a href="https://www.aclweb.org/anthology/P19-1172/">Nicolai &amp; Yarowsky, 2019</a>), distant supervision (<a href="https://www.aclweb.org/anthology/D18-1061/">Plank &amp; Agić, 2018</a>) or machine translation (MT; <a href="https://www.aclweb.org/anthology/P16-1133/">Zhou et al., 2016</a>; <a href="https://arxiv.org/abs/1807.08998">Eger et al., 2018</a>).</p><h2 id="why-not-machine-translation">Why not Machine Translation?</h2><p>In light of the recent success of <a href="https://ai.googleblog.com/2019/10/exploring-massively-multilingual.html">massively multilingual</a> and unsupervised machine translation (<a href="https://www.aclweb.org/anthology/D18-1549/">Lample et al., 2018</a>; <a href="https://www.aclweb.org/anthology/P19-1019/">Artetxe et al., 2019</a>), an obvious question is why we would not just use MT to translate the training data in our source language to the target language and train a model on the translated data—or translate the test set and apply our source model directly to it, which often works better in practice. If an MT system is available, this can be a strong baseline (<a href="https://www.aclweb.org/anthology/D18-1269/">Conneau et al., 2018</a>) and unsupervised MT in particular can be useful for low-resource languages (<a href="https://www.aclweb.org/anthology/D18-1549/">Lample et al., 2018</a>).</p><p>However, an MT system for the desired language pair may not be easily available and can be expensive to train. In addition, models trained on machine-translated data often underperform <a href="#deep-unsupervised-models">state-of-the-art deep multilingual models</a>. MT struggles with distant language pairs and domain mismatches (<a href="https://www.aclweb.org/anthology/D19-1632/">Guzmán et al., 2019</a>). It is also not a fit for all tasks; the performance of translation-based models for question answering tasks depends heavily on the translation quality of named entities (<a href="https://www.aclweb.org/anthology/P19-1227/">Liu et al., 2019</a>). For sequence labelling tasks, MT requires projecting annotations across languages, itself a difficult problem (<a href="https://www.aclweb.org/anthology/L18-1344/">Akbik &amp; Vollgraf, 2018</a>).</p><p>In general, however, MT and the methods described in this post are not at odds but complementary. Cross-lingual word representations are used to initialise unsupervised MT (<a href="https://arxiv.org/abs/1902.01313">Artetxe et al., 2019</a>) and cross-lingual representations have been shown to improve neural MT performance (<a href="https://arxiv.org/abs/1901.07291">Lample &amp; Conneau, 2019</a>). Techniques from MT such as word alignment have also inspired much work in cross-lingual representation learning (<a href="https://www.jair.org/index.php/jair/article/view/11640/26511">Ruder et al., 2019</a>). Finally, a multilingual model benefits from being fine-tuned on translations of the labelled data in multiple languages (<a href="https://arxiv.org/abs/1911.02116">Conneau et al., 2019</a>).</p><h1 id="main-framework">Main Framework</h1><p>We now discuss how to learn unsupervised representations, starting at the word level. The main framework for learning cross-lingual word embeddings (CLWEs) consists of <em>mapping-based</em> or <em>post-hoc alignment</em> approaches. For an overview of other approaches, have a look at <a href="https://www.jair.org/index.php/jair/article/view/11640/26511">our survey</a> or <a href="https://www.morganclaypool.com/doi/abs/10.2200/S00920ED2V01Y201904HLT042">book</a> on the topic.</p><p>Mapping-based methods are currently the preferred approach due to their ease and efficiency of use and conceptual simplicity. They only require monolingual word embeddings trained in each language and then simply learn a linear projection that maps between the embedding spaces. The general framework can be seen in the Figure below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/10/projection_framework.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>A general framework for projection-based cross-lingual word embeddings (<a href="https://www.aclweb.org/anthology/P19-1070/">Glavaš et al., 2019</a>)</figcaption></figure><!--kg-card-begin: markdown--><p>Given monolingual embedding spaces \(\mathbf{X}_{L_1} = \{\mathbf{x}^i_{L_1} \}^{|V_{L_1}|}_{i=1} \) and \(\mathbf{X}_{L_2} = \{\mathbf{x}^j_{L_2} \}^{|V_{L_2}|}_{j=1} \) in two languages where \(\mathbf{x}\) is a word embedding and \(|V|\) is the size of the vocabulary, we perform the following steps (<a href="https://www.aclweb.org/anthology/P19-1070/">Glavaš et al., 2019</a>):</p>
<ol>
<li><strong>Construct a seed translation dictionary</strong>. The dictionary \(D = \{w^i_{L_1}, w^j_{L_2} \}^K\) contains \(K\) pairs of words and their translations.</li>
<li><strong>Create word-aligned monolingual subspaces</strong>. We create subspaces \(\mathbf{X}_S = \{\mathbf{x}^i_{L_1}\}^K_{i=1} \) and \(\mathbf{X}_T = \{\mathbf{x}^j_{L_2}\}^K_{j=1} \) where \(\mathbf{X}_S \in \mathbb{R}^{K \times d}\) and \(\mathbf{X}_T \in \mathbb{R}^{K \times d}\) where \(d\) is the dimensionality of the word embeddings. We do this simply by looking up the vectors of the words and their translations in the monolingual embedding spaces, \( \{w^i_{L_1} \} \) from \(\mathbf{X}_{L_1}\) and \( \{w^j_{L_2} \} \) from \(\mathbf{X}_{L_2}\) respectively.</li>
<li><strong>Learn a mapping between the subspaces to a common space</strong>. Specifically, we project the matrices \(\mathbf{X}_S\) and \(\mathbf{X}_T\) into a common space \(\mathbf{X}_{CL}\). In the general setting, we learn two projection matrices \(\mathbf{W}_{L_1} \in \mathbb{R}^{d \times d} \) and \(\mathbf{W}_{L_2} \in \mathbb{R}^{d \times d}\) to project \(\mathbf{X}_{L_1}\) and \(\mathbf{X}_{L_2}\) respectively to the shared space.</li>
</ol>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>We often treat one of the monolingual spaces \(\mathbf{X}_{L_1}\) (typically the English space) as the cross-lingual space \(\mathbf{X}_{CL}\) and only learn a mapping \(\mathbf{W}_{L_2}\) from \(\mathbf{X}_{L_2}\) to this space. The standard approach for learning this mapping is to minimise the Euclidean distance between the representations of words \( \mathbf{X}_T \) and their projected translations \( \mathbf{X}_S\mathbf{W} \) in the dictionary \(D\) (<a href="https://arxiv.org/abs/1309.4168">Mikolov et al., 2013</a>):</p>
<p>\(\mathbf{W}_{L_2} = \arg \min_{\mathbf{W}} \| \mathbf{X}_S\mathbf{W} - \mathbf{X}_T \|_2 \)</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>After having learned this mapping, we can now project a word embedding \(\mathbf{x}_{L_2}\) from \(\mathbf{X}_{L_2}\) simply as \(\mathbf{W}_{L_2} \mathbf{x}_{L_2} \) to the space of \(\ \mathbf{X}_{L_1}\). We can see how this mapping looks like geometrically below.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/mapping_visualisation.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>Visualisation of the mapping-based approach between Swahili and English subspaces (inspired by slides from Eneko Agirre &amp; Mikel Artetxe)</figcaption></figure><!--kg-card-begin: markdown--><p>More complex mappings (such as with a deep neural network) have been observed to lead to poorer performance (<a href="https://arxiv.org/abs/1309.4168">Mikolov et al., 2013</a>). Better word translation results can be achieved when \(\mathbf{W}_{L_2}\) is constrained to be orthogonal (<a href="https://www.aclweb.org/anthology/N15-1104/">Xing et al., 2015</a>, <a href="https://www.aclweb.org/anthology/D16-1250/">Artetxe et al., 2016</a>). Intuitively, this ensures that when \(\mathbf{W}_{L_2}\) vectors are projected to the \(\mathbf{X}_{CL}\) space that the structure of the monolingual embedding space is preserved. If \(\mathbf{W}_{L_2}\) is orthogonal, then the optimisation problem is known as the so-called Procrustes problem (<a href="https://link.springer.com/article/10.1007/BF02289451">Schönemann, 1966</a>), which has a closed form solution:</p>
<p>\(<br>
\begin{align}<br>
\begin{split}<br>
\mathbf{W}_{L_2} &amp; = \mathbf{UV}^\top, \text{with} \\<br>
\mathbf{U \Sigma V}^\top &amp; = SVD(\mathbf{X}_T \mathbf{X}_S{}^\top)<br>
\end{split}<br>
\end{align}<br>
\)</p>
<!--kg-card-end: markdown--><p>Almost all projection-based methods, whether they are supervised or unsupervised, solve the Procrustes problem to learn a linear mapping in Step 3. The main part where they differ is in how they obtain the initial seed dictionary in Step 1. Recent supervised approaches (<a href="https://www.aclweb.org/anthology/P17-1042/">Artetxe et al., 2017</a>) have been shown to achieve reasonable performance using dictionaries of only 25-40 translation pairs. The main idea that enables learning from such limited supervision is bootstrapping or self-learning. </p><h1 id="self-learning">Self-learning</h1><p>Starting from a seed dictionary, recent approaches employ a <a href="https://docs.google.com/presentation/d/1miPu0hkBFSRTAxP-Psg0xXOvAbVSJPhQXehMQdUI88I/edit#slide=id.g5d7e0f0964_0_0">self-learning procedure</a> (<a href="https://www.aclweb.org/anthology/P17-1042/">Artetxe et al., 2017</a>), which can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/10/self-learning_loop_wide.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>The self-learning loop (Credit: Eneko Agirre)</figcaption></figure><!--kg-card-begin: markdown--><p>We learn an initial mapping based on the seed dictionary and use this mapping to project \(\mathbf{X}_{L_2}\) vectors to the target space \(\mathbf{X}_{CL}\). In this cross-lingual space, \(L_1\) words should be close to their translations in \(L_2\). We can now use the \(L_1\) words and their nearest neighbours in \(L_2\) to build our new dictionary. In practice, people often only consider frequent words and mutual nearest neighbours, i.e. words that are nearest neighbours from both \(L_1\) and \(L_2\) directions (<a href="https://www.aclweb.org/anthology/P16-1024/">Vulić et al., 2016</a>; <a href="https://openreview.net/forum?id=H196sainb">Lample et al., 2018</a>).</p>
<!--kg-card-end: markdown--><p>The framework that is typically used in practice consists of additional components (<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16935/16781">Artetxe et al., 2018</a>) that can be seen in the Figure below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/cross-lingual_framework_landscape.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>The general unsupervised cross-lingual word embedding framework (<a href="https://arxiv.org/abs/1909.01638">Vulić et al., 2019</a>)</figcaption></figure><p>In particular, beyond the induction of the initial seed dictionary (C1), the learning of the projections, and the self-learning (C3), there are several pre- and post-processing steps that are often used in practice. You can have a look at <a href="https://docs.google.com/presentation/d/1miPu0hkBFSRTAxP-Psg0xXOvAbVSJPhQXehMQdUI88I/edit#slide=id.g5bb054856f_1_193">the slides</a> or refer to <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16935/16781">Artetxe et al. (2018)</a> for an overview.</p><h1 id="unsupervised-seed-induction">Unsupervised Seed Induction</h1><!--kg-card-begin: markdown--><p>Unsupervised seed induction aims to find a good initial dictionary that we can use to bootstrap the self-learning loop. We can differentiate approaches based on whether they learn an initial mapping by minimising the distance between the \(\mathbf{X}_{L_1}\) or \(\mathbf{X}_{L_2}\) spaces—often with an adversarial component—or whether they rely on a non-adversarial approach.</p>
<!--kg-card-end: markdown--><h2 id="adversarial-approaches">Adversarial approaches</h2><!--kg-card-begin: markdown--><p>Adversarial approaches are inspired by <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">generative adversarial networks (GANs)</a>. The generator is parameterised by our projection matrix \(\mathbf{W}_{L_2}\) while the discriminator is a separate neural network that tries to discriminate between true embeddings from \(\mathbf{X}_{L_2}\) and projected embeddings \(\mathbf{W}_{L_1} \mathbf{X}_{L_1}\) as can be seen below.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/10/adversarial_mapping.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>The adversarial approach for learning cross-lingual word embeddings</figcaption></figure><!--kg-card-begin: markdown--><p>In order to fool the discriminator, the generator has to transform \(\mathbf{X}_{L_1}\) in such a way that it matches the distribution of \(\mathbf{X}_{L_2}\). The underlying hypothesis is that the transformation that makes the distributions as similar as possible also puts words close to their translations in the cross-lingual space \(\mathbf{X}_{CL}\). GANs can be notoriously brittle. The first approach that worked more robustly in certain settings was proposed by <a href="https://arxiv.org/abs/1710.04087">Conneau et al. (2018)</a>.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>Other ways have been proposed to minimise the distance between the distributions of \(\mathbf{X}_{L_1}\) and \(\mathbf{X}_{L_2}\) that leverage Wasserstein GANs, Earth Mover's distance, and optimal transport. For more details about these approaches and their connections, have a look at <a href="https://docs.google.com/presentation/d/1miPu0hkBFSRTAxP-Psg0xXOvAbVSJPhQXehMQdUI88I/edit#slide=id.g5edc0b865a_3_4040">the corresponding slides</a>.</p>
<!--kg-card-end: markdown--><h2 id="weak-supervision-and-second-order-similarity">Weak supervision and second-order similarity</h2><p>Another strategy for obtaining a seed lexicon is through the use of heuristics. If the writing systems of two languages share the same alphabet, then there will be many words that are spelled the same in both languages, many of them named entities such as "New York" and "Barack Obama". A list of pairs of such identically spelled words can be used as a weakly supervised seed dictionary (<a href="https://www.aclweb.org/anthology/P18-1072/">Søgaard et al., 2018</a>). Even if languages do not share alphabets, many languages still use Arabic numerals, which can be used as a seed dictionary (<a href="https://www.aclweb.org/anthology/P17-1042/">Artetxe et al., 2017</a>).</p><p>Such a weakly supervised seed dictionary is available for most language pairs and works surprisingly well, often outperforming purely unsupervised approaches. Intuitively, despite being noisy, a dictionary consisting of identically spelled words provides a sufficient number of anchor points for learning an initial mapping.</p><p>Still, it would be useful to have a seed dictionary that is independent of a language's writing system. Rather than requiring that translations should be similar (e.g. spelled the same), we can go one step further and require that translations should be similar to other words across languages <em>in the same way</em> (<a href="https://www.aclweb.org/anthology/P18-1073/">Artetxe et al. 2018</a>). In other words, the similarity distributions of translations within each language should be similar.</p><p>Specifically, if we calculate the cosine similarity of one word with all other words in the same language, sort the values, and normalise, we obtain the following distributions for "two", "due" and "cane":</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/10/intralingual_similarity_distributions.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>Intralingual similarity distributions of three words (<a href="https://www.aclweb.org/anthology/P18-1073/">Artetxe et al., 2018</a>).</figcaption></figure><p>The intralingual similarity distributions of translations such as "two" and "due" should be more similar compared to other words such as "cane". Based on this insight, we can use the nearest neighbours of our similarity distributions across languages as the initial dictionary. This notion of second-order similarity (the similarity of similarity distributions) is a powerful concept and is also leveraged by distance metrics such as the <a href="https://docs.google.com/presentation/d/1miPu0hkBFSRTAxP-Psg0xXOvAbVSJPhQXehMQdUI88I/edit#slide=id.g5d7e0f0964_9_50">Gromov-Wasserstein distance</a>.</p><h1 id="systematic-comparisons">Systematic Comparisons</h1><p>Unsupervised cross-lingual word embeddings are theoretically interesting. They can teach us how to better leverage monolingual data across languages. We can also use them to learn more about how different languages relate to each other.</p><p>However, supervision in the form of translation pairs is available for many languages and weak supervision is readily available. Unsupervised methods are thus only of practical interest if they are able to outperform to their supervised counterparts.</p><p>Initial papers claimed that unsupervised methods are indeed competitive and even outperform the basic supervised Procrustes method. Recent studies, however, control for the <a href="#self-learning">additional components in the framework</a> and compare unsupervised and supervised methods on equal footing. The picture that has emerged is that current unsupervised methods generally underperform supervised methods with 500-1,000 seed translation pairs (<a href="https://www.aclweb.org/anthology/P19-1070/">Glavaš et al., 2019</a>; <a href="https://www.aclweb.org/anthology/D19-1449/">Vulić et al., 2019</a>) as can be seen in the Figure below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/bli_performance_vs_seed_dict_size_landscape.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>Comparison of BLI performances of a fully unsupervised method (UNSUPER), a supervised method (SUPER), and supervised methods with self-learning (+SL +NOD, +SL +SYM; <a href="https://www.aclweb.org/anthology/D19-1449/">Vulić et al., 2019</a>).</figcaption></figure><p>Moreover, even the most robust unsupervised mapping-based approaches fail for many language pairs. This has to do with the topology of the monolingual embedding spaces: Unsupervised methods rely on the assumption that the embedding spaces are approximately isomorphic, i.e. that they have similar structure  (<a href="https://www.aclweb.org/anthology/P18-1072/">Søgaard et al., 2018</a>). If this is not the case, then the unsupervised seed induction step fails.</p><p>The structure of embedding spaces can be different if embeddings belong to dissimilar languages but also if they were trained on different domains or using different algorithms (<a href="https://www.aclweb.org/anthology/P18-1072/">Søgaard et al., 2018</a>; <a href="https://www.aclweb.org/anthology/D18-1056/">Hartmann et al., 2018</a>). Different methods such as a combination of several independent linear maps (<a href="https://www.aclweb.org/anthology/D18-1047">Nakashole, 2018)</a>, iterative normalisation (<a href="https://arxiv.org/pdf/1906.01622.pdf">Zhang et al., 2019)</a>, or incrementally adding new languages to the multilingual space (<a href="https://www.aclweb.org/anthology/N19-1188/">Heymann et al., 2019</a>) have been proposed to align such non-isomorphic embedding spaces. Recent work has also shown that—controlling for additional components—the standard GAN approach is competitive with more advanced GANs such as Wasserstein GAN and with the second-order similarity initialisation (<a href="https://github.com/coastalcph/stepbystep/blob/master/poster.pdf">Hartmann et al., 2019</a>).</p><h1 id="unsupervised-deep-models">Unsupervised Deep Models</h1><p>Supervised deep models that learn from parallel sentences or documents have been proposed before (see Sections 7 and 8 of <a href="https://www.jair.org/index.php/jair/article/view/11640/26511">this survey</a>). In light of the success of pretrained language models, similar techniques have recently been applied to train unsupervised deep cross-lingual representations. </p><h2 id="joint-models">Joint models</h2><p>The most prominent example in this line of work is <a href="https://github.com/google-research/bert/blob/master/multilingual.md">multilingual BERT</a> (mBERT), a BERT-base model that was jointly trained on the corpora of 104 languages with a shared vocabulary of 110k subword tokens.</p><p>mBERT is trained using masked language modelling (MLM)—with no explicit supervision—but has nevertheless been shown to learn cross-lingual representations that generalise surprisingly well to other languages via zero-shot transfer (<a href="https://www.aclweb.org/anthology/P19-1493/">Pires et al., 2019</a>; <a href="https://www.aclweb.org/anthology/D19-1077/">Wu &amp; Dredze, 2019</a>). This generalisation ability has been attributed to three factors: 1) identical subwords in the shared vocabulary acting as anchor points for learning an alignment (similar to the weak supervision of CLWEs); 2) joint training across multiple languages that spreads this effect; and 3) deep cross-lingual representations that go beyond vocabulary memorisation and generalise across languages.</p><p>Recent work (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>; <a href="https://openreview.net/forum?id=HJeT3yrtDr">Anonymous et al., 2019</a>; <a href="https://arxiv.org/abs/1911.01464">Wu et al., 2019</a>), however, shows that a shared vocabulary is not required for learning unsupervised deep cross-lingual representations. <a href="https://arxiv.org/abs/1910.11856">Artetxe et al. (2019)</a> additionally demonstrate that joint training is unnecessary and identify the vocabulary size per language as an important factor: Multilingual models with larger vocabulary sizes consistently perform better.</p><p>Extensions to mBERT augment it with a supervised objective (<a href="https://arxiv.org/abs/1901.07291">Lample and Conneau, 2019</a>), which is inspired by <a href="https://www.jair.org/index.php/jair/article/view/11640/26511">bilingual skip-gram approaches</a> and can be seen in the Figure below. Others add auxiliary pre-training tasks such as cross-lingual word recovery and paraphrase detection (<a href="https://arxiv.org/abs/1909.00964">Huang et al., 2019</a>), encourage representations of translations to be similar (<a href="https://openreview.net/forum?id=r1xCMyBtPS">Anonymous, 2019</a>), and scale it up (<a href="https://arxiv.org/abs/1911.02116">Conneau et al., 2019</a>). Joint training has been applied not only to Transformers but also to LSTM-based methods where initialisation with CLWEs has been found useful (<a href="https://www.aclweb.org/anthology/N19-1392/">Mulcaire et al., 2019</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/translation_language_modelling.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>The supervised cross-lingual language modelling objective of <a href="https://arxiv.org/abs/1901.07291">Lample and Conneau (2019</a>)</figcaption></figure><h2 id="mapping-based-approaches">Mapping-based approaches</h2><p>The mapping-based approaches that we discussed previously have also been applied to the contextual representations of deep bilingual models. The main assumption behind these methods is that contextual representations—similar to their non-contextual counterparts—can also be aligned via a linear transformation. As can be seen in the Figure below, contextual representations cluster and have similar distributions across languages.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/contextual_aligned_landscape.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>PCA visualisation of contextual embeddings of the English word "bear" and its two possible Spanish translations, "tener" and "oso" in an aligned multilingual space (<a href="https://www.aclweb.org/anthology/N19-1162/">Schuster et al., 2019</a>)</figcaption></figure><!--kg-card-begin: markdown--><p>For contextual representations, the mapping can be done either on the token level or on the type level. On the token level, contextual representations of tokens in both languages can be aligned using word alignment information in parallel data (<a href="https://www.aclweb.org/anthology/N19-1391/">Aldarmaki &amp; Diab, 2019</a>). On the type level, an aggregate representation of a word's contextual representations such as its mean can be used as its type embeddings (<a href="https://www.aclweb.org/anthology/N19-1162/">Schuster et al., 2019</a>). Type embeddings across languages can then be aligned using the same mapping-based approaches as before. <a href="https://www.aclweb.org/anthology/K19-1004/">Liu et al. (2019)</a> perform further analyses and find that type-level alignment performs best. Mapping-based methods have also recently been applied to BERT-based representations (<a href="https://openreview.net/forum?id=S1l-C0NtwS">Anonymous et al., 2019</a>; <a href="https://arxiv.org/abs/1911.01464">Wu et al., 2019</a>).</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>Rather than learning a mapping between two existing pretrained models in \(L_1\) and \(L_2\), we can apply a model that was pretrained in a high-resource language \(L_1\) to a low-resource language \(L_2\) much more easily by simply learning token-level embeddings in \(L_2\) while keeping the model body fixed (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>, <a href="https://openreview.net/forum?id=Bkle6T4YvB">Anonymous et al., 2019</a>). The full process can be seen below. This alignment to a monolingual model may avoid the non-isomorphism of fixed monolingual vector spaces and the resulting model is competitive with jointly trained models.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/monolingual_transfer.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>The four steps of the monolingual transfer approach of <a href="https://arxiv.org/abs/1910.11856">Artetxe et al. (2019</a>)</figcaption></figure><h1 id="future-directions">Future Directions</h1><h2 id="benchmarks">Benchmarks</h2><p>A standard task for evaluating CLWEs is bilingual lexicon induction (BLI), which evaluates the quality of the cross-lingual embedding space by determining whether words are nearest neighbours of their translations in a test dictionary. Recently, several researchers have identified issues with the standard MUSE dictionaries (<a href="https://arxiv.org/abs/1710.04087">Conneau et al., 2018</a>) for this task, which only represent frequent words and are not morphologically diverse (<a href="https://www.aclweb.org/anthology/D19-1090/">Czarnowska et al., 2019</a>; <a href="https://www.aclweb.org/anthology/D19-1328/">Kementchedjhieva et al., 2019</a>). BLI performance on words with lower frequency ranks drops off drastically as can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/dictionary_frequency_ranks_landscape.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>BLI performance vs frequency rank of words in MUSE and new dictionaries (<a href="https://arxiv.org/abs/1909.02855">Czarnowska et al., 2019</a>)</figcaption></figure><p>We are ultimately interested in applying our methods to downstream tasks in low-resource languages. While BLI correlates well with certain downstream tasks for mapping-based approaches (<a href="https://www.aclweb.org/anthology/P19-1070/">Glavaš et al., 2019</a>), the correlation is weaker for methods that are not based on an orthogonal transformation. Downstream evaluation is thus important to accurately gauge the performance of novel methods in the future.</p><p>However, even existing cross-lingual downstream tasks have their problems. Cross-lingual document classification, a classic task for the evaluation of cross-lingual representations (<a href="https://www.aclweb.org/anthology/C12-1089/">Klementiev et al., 2012</a>), mostly requires superficial keyword-matching and CLWEs outperform deep representations (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>). A more recent multilingual benchmark, XNLI (<a href="https://www.aclweb.org/anthology/D18-1269/">Conneau et al., 2018</a>) may be plagued by similar artefacts as the MultiNLI dataset from which it has been derived (<a href="https://www.aclweb.org/anthology/N18-2017/">Gururangan et al., 2018</a>).</p><p>On the whole, we need more challenging benchmarks for evaluating multilingual models. Question answering might be a good candidate as it is a common probe for language understanding and has been shown to be harder to exploit. Recently, several cross-lingual question answering datasets have been proposed, including XQuAD (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>), which extends SQuAD 1.1. to ten other languages, the open-domain XQA (<a href="https://www.aclweb.org/anthology/P19-1227/">Liu et al., 2019</a>), and MLQA (<a href="https://arxiv.org/abs/1910.07475">Lewis et al., 2019</a>).</p><h2 id="understanding-representations">Understanding representations</h2><p>Benchmarks are also important for better understanding the representations that our models learn. Along these lines, it is still unclear what makes monolingual embedding spaces non-isomorphic. Even embedding spaces between similar languages are not strictly isomorphic as can be seen below. Similar issues might affect the spaces of deep contextual representations. Identifying the cause of this phenomenon may help us gain a better understanding of our learning algorithms and the biases they encode in their representations. It may also shed light on the semantic similarities and differences between languages.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/nn_graphs_freq_words_landscape.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>Nearest neighbour graphs of 10 most frequent nouns in English and their German translations (<a href="https://www.aclweb.org/anthology/P18-1072">Søgaard et al., 2018</a>)</figcaption></figure><p>There have been several studies analysing what CLWEs learn but it is still mostly unclear what representations learned by unsupervised deep multilingual models capture. As token-level alignment to a deep monolingual model achieves competitive results (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>), deep multilingual models may still mostly learn lexical alignment. If this is the case, we need better methods that incentivise learning deep multilingual representations. Parallel data improves performance in practice (<a href="https://arxiv.org/abs/1901.07291">Lample &amp; Conneau, 2019</a>), but for low-resource languages we would like to use as little supervision as possible.</p><p>Deep representations from an English BERT model can be transferred to another language without any modification (besides learning new token embeddings; <a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>). Consequently, it should be interesting to analyse what cross-lingual information monolingual models capture. In order to gain new insights, however, we need better benchmarks and evaluation protocols for the analysis of cross-lingual knowledge.</p><h2 id="practical-considerations">Practical considerations</h2><p>In the end, we'd like to learn cross-lingual representations in order to apply them to downstream tasks in low-resource languages. In light of this, we should not forget practical concerns of such low-resource scenarios. While zero-shot transfer is a focus of current methods, <a href="http://nlp.fast.ai/classification/2019/09/10/multifit.html">zero-shot transfer is not free</a>. It requires access to training data from the same task and distribution in a high-resource language. We should thus continue investing in approaches that can be trained with few samples in a target language and that combine the best of both the monolingual and cross-lingual worlds, for instance via cross-lingual bootstrapping that can be seen below (<a href="https://www.aclweb.org/anthology/D19-1572/">Eisenschlos et al., 2019</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/multifit_bootstrapping_landscape.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>Steps of a cross-lingual bootstrapping method for zero-shot cross-lingual transfer (<a href="https://www.aclweb.org/anthology/D19-1572/">Eisenschlos et al., 2019</a>)</figcaption></figure><p>In addition, recent deep multilingual models are large and expensive to train. Training smaller multilingual models, for instance via distillation (<a href="https://www.aclweb.org/anthology/D19-1374/">Tsai et al., 2019</a>), will be key for deployment in resource-constrained scenarios.</p><p>Recent wide-coverage parallel corpora such as JW-300 (<a href="https://www.aclweb.org/anthology/P19-1310/">Agić &amp; Vulić, 2019</a>) and WikiMatrix (<a href="https://arxiv.org/abs/1907.05791">Schwenk et al., 2019</a>) enable the training of massively multilingual or supervised systems for many new low-resource languages where parallel data was previously not available. However, these resources still do not cover languages where only few unlabelled data is available. In order to apply our methods to tasks in such languages, we need to develop approaches that can learn representations from a limited number of samples. Resources that provide limited data (mostly dictionaries) for a large number of languages are the <a href="https://asjp.clld.org">ASJP database</a>, <a href="https://panlex.org/">PanLex</a>, and <a href="https://babelnet.org/">BabelNet</a>.</p><p>Finally, while CLWEs underperform deep multilingual models on more challenging benchmarks (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>), they are still preferred for easier tasks such as document classification due to their efficiency and ease of use. In addition, they serve as a useful initialisation to kick-start the training of deep models (<a href="https://www.aclweb.org/anthology/N19-1392/">Mulcaire et al., 2019</a>; <a href="https://openreview.net/forum?id=Bkle6T4YvB">Anonymous et al., 2019</a>) and of unsupervised NMT models (<a href="https://arxiv.org/abs/1902.01313">Artetxe et al., 2019</a>). Consequently, they may be a useful foundation for the development of more resource and parameter-efficient deep multilingual models.</p><h2 id="citation">Citation</h2><p>If you found this post helpful, consider citing <a href="https://www.aclweb.org/anthology/P19-4007">the tutorial</a> as:</p><pre><code>@inproceedings{ruder2019unsupervised,
  title={Unsupervised Cross-Lingual Representation Learning},
  author={Ruder, Sebastian and S{\o}gaard, Anders and Vuli{\'c}, Ivan}, 
  booktitle={Proceedings of ACL 2019, Tutorial Abstracts},
  pages={31--38},
  year={2019}
}</code></pre>]]></content:encoded></item><item><title><![CDATA[The State of Transfer Learning in NLP]]></title><description><![CDATA[This post expands on the NAACL 2019 tutorial on Transfer Learning in NLP. It highlights key insights and takeaways and provides updates based on recent work.]]></description><link>http://ruder.io/state-of-transfer-learning-in-nlp/</link><guid isPermaLink="false">5c76ff7c1b9b0d18555b9eb3</guid><category><![CDATA[transfer learning]]></category><category><![CDATA[natural language processing]]></category><category><![CDATA[events]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Sun, 18 Aug 2019 15:22:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2019/08/transfer_learning_methods_small.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2019/08/transfer_learning_methods_small.png" alt="The State of Transfer Learning in NLP"><p>Update 16.10.2020: Added <a href="https://www.infoq.cn/article/zD5QkcIzF9253friWPVd">Chinese</a> and <a href="https://www.ibidemgroup.com/edu/traduccion-aprendizaje-pnl/">Spanish</a> translations.</p><p>This post expands on the <a href="http://tiny.cc/NAACLTransfer">NAACL 2019 tutorial on Transfer Learning in NLP</a>. The tutorial was organized by Matthew Peters, Swabha Swayamdipta, Thomas Wolf, and me. In this post, I highlight key insights and takeaways and provide updates based on recent work. You can see the structure of this post below:</p><figure class="kg-card kg-image-card"><img src="http://ruder.io/content/images/2019/08/agenda.png" class="kg-image" alt="The State of Transfer Learning in NLP"></figure><p>The <a href="http://tiny.cc/NAACLTransfer">slides</a>, a <a href="http://tiny.cc/NAACLTransferColab">Colaboratory notebook</a>, and <a href="http://tiny.cc/NAACLTransferCode">code</a> of the tutorial are available online.</p><h1 id="introduction">Introduction</h1><p>For an overview of what transfer learning is, have a look at <a href="http://ruder.io/transfer-learning/">this blog post</a>. Our go-to definition throughout this post will be the following, which is illustrated in the diagram below:</p><blockquote>Transfer learning is a means to extract knowledge from a source setting and apply it to a different target setting.</blockquote><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/transfer_learning_scenario.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>An illustration of the process of transfer learning.</figcaption></figure><p>In the span of little more than a year, transfer learning in the form of pretrained language models has <a href="https://thegradient.pub/nlp-imagenet/">become ubiquitous in NLP</a> and has contributed to the state of the art on a wide range of tasks. However, transfer learning is not a recent phenomenon in NLP. One illustrative example is progress on the task of Named Entity Recognition (NER), which can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/ner_results.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>Performance on Named Entity Recognition (NER) on CoNLL-2003 (English) over time.</figcaption></figure><p>Throughout its history, most of the major improvements on this task have been driven by different forms of transfer learning: from early self-supervised learning with auxiliary tasks (<a href="http://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf">Ando and Zhang, 2005</a>) and phrase &amp; word clusters (<a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35520.pdf">Lin and Wu, 2009</a>) to the language model embeddings (<a href="https://arxiv.org/pdf/1705.00108.pdf">Peters et al., 2017</a>) and pretrained language models (<a href="https://aclweb.org/anthology/N18-1202">Peters et al., 2018</a>; <a href="https://alanakbik.github.io/papers/coling2018.pdf">Akbik et al., 2018</a>; <a href="https://arxiv.org/abs/1903.07785">Baevski et al., 2019</a>) of recent years.</p><p>There are different types of transfer learning common in current NLP. These can be roughly classified along three dimensions based on a) whether the source and target settings deal with the same task; and b) the nature of the source and target domains; and c) the order in which the tasks are learned. A taxonomy that highlights the variations can be seen below:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/transfer_learning_taxonomy.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>A taxonomy for transfer learning in NLP (<a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=64">Ruder, 2019</a>).</figcaption></figure><p>Sequential transfer learning is the form that has led to the biggest improvements so far. The general practice is to pretrain representations on a large unlabelled text corpus using your method of choice and then to adapt these representations to a supervised target task using labelled data as can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/pretraining_adaptation.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>The general procedure of sequential transfer learning.</figcaption></figure><h3 id="major-themes">Major themes</h3><p>Several major themes can be observed in how this paradigm has been applied:</p><p><strong>From words to words-in-context</strong>  Over time, representations incorporate more context. Early approaches such as word2vec (<a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et al., 2013</a>) learned a single representation for every word independent of its context. Later approaches then scaled these representations to sentences and documents (<a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf">Le and Mikolov, 2014</a>; <a href="https://arxiv.org/abs/1705.02364">Conneau et al., 2017</a>). Current approaches learn word representations that change based on the word's context (<a href="http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf">McCann et al., 2017</a>; <a href="https://aclweb.org/anthology/N18-1202">Peters et al., 2018</a>). </p><p><strong>LM pretraining</strong>   Many successful pretraining approaches are based on variants of language modelling (LM). Advantages of LM are that it does not require any human annotation and that many languages have enough text available to learn reasonable models. In addition, LM is versatile and enables learning both sentence and word representations with a variety of objective functions.</p><p><strong>From shallow to deep</strong>  Over the last years, state-of-the-art models in NLP have become progressively deeper. Up to two years ago, the state of the art on most tasks was a 2-3 layer deep BiLSTM, with machine translation being an outlier with 16 layers (<a href="https://arxiv.org/abs/1609.08144">Wu et al., 2016</a>). In contrast, current models like BERT-Large and GPT-2 consist of 24 Transformer blocks and recent models are even deeper.</p><p><strong>Pretraining vs target task</strong>  The choice of pretraining and target tasks is closely intertwined. For instance, sentence representations are not useful for word-level predictions, while span-based pretraining is important for span-level predictions. On the whole, for the best target performance, it is beneficial to choose a similar pretraining task.</p><h1 id="pretraining">Pretraining</h1><h3 id="why-does-language-modelling-work-so-well">Why does language modelling work so well?</h3><p>The remarkable success of pretrained language models is surprising. One reason for the success of language modelling may be that it is a very difficult task, even for humans. To have any chance at solving this task, a model is required to learn about syntax, semantics, as well as certain facts about the world. Given enough data, a large number of parameters, and enough compute, a model can do a reasonable job. Empirically, language modelling works better than other pretraining tasks such as translation or autoencoding (<a href="https://arxiv.org/abs/1809.10040">Zhang et al. 2018</a>; <a href="https://www.aclweb.org/anthology/P19-1439">Wang et al., 2019</a>).</p><p>A recent predictive-rate distortion (PRD) analysis of human language (<a href="http://socsci.uci.edu/~rfutrell/papers/hahn2019estimating.pdf">Hahn and Futrell, 2019</a>) suggests that human language—and language modelling—has infinite statistical complexity but that it can be approximated well at lower levels. This observation has two implications: 1) We can obtain good results with comparatively small models; and 2) there is a lot of potential for scaling up our models. For both implications we have empirical evidence, as we can see in the next sections.</p><h3 id="sample-efficiency">Sample efficiency</h3><p>One of the main benefits of pretraining is that it reduces the need for annotated data. In practice, transfer learning has often been shown to achieve similar performance compared to a non-pretrained model with 10x fewer examples or more as can be seen below for ULMFiT (<a href="https://aclweb.org/anthology/P18-1031">Howard and Ruder, 2018</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/sample_efficiency.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>Performance of a model trained from scratch (blue) vs. two pretrained models fine-tuned on labelled target data (orange) as well as unlabelled target data (green) respectively (<a href="https://aclweb.org/anthology/P18-1031">Howard and Ruder, 2018</a>).</figcaption></figure><h3 id="scaling-up-pretraining">Scaling up pretraining</h3><p>Pretrained representations can generally be improved by jointly increasing the number of model parameters and the amount of pretraining data. Returns start to diminish as the amount of pretraining data grows huge. Current performance curves such as the one below, however, do not indicate that we have reached a plateau. We can thus expect to see even bigger models trained on more data. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/scaling_up_pretraining.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>Average GLUE score with different amounts of Common Crawl data for pretraining (<a href="https://arxiv.org/abs/1903.07785">Baevski et al., 2019</a>).&nbsp;</figcaption></figure><p>Recent examples of this trend are <a href="https://arxiv.org/abs/1907.12412">ERNIE 2.0</a>, <a href="https://arxiv.org/abs/1906.08237">XLNet</a>, <a href="https://devblogs.nvidia.com/training-bert-with-gpus/">GPT-2 8B</a>, and <a href="https://arxiv.org/abs/1907.11692">RoBERTa</a>. The latter in particular finds that simply training BERT for longer and on more data improves results, while GPT-2 8B reduces perplexity on a language modelling dataset (though only by a comparatively small factor).</p><h3 id="cross-lingual-pretraining">Cross-lingual pretraining</h3><p>A major promise of pretraining is that it can help us bridge the digital language divide and can enable us learn NLP models for more of the world's 6,000 languages. Much work on cross-lingual learning has focused on training separate word embeddings in different languages and learning to align them (<a href="https://www.jair.org/index.php/jair/article/view/11640">Ruder et al., 2019</a>). In the same vein, we can learn to align contextual representations (<a href="https://www.aclweb.org/anthology/N19-1162">Schuster et al., 2019</a>). Another common method is to share a subword vocabulary and train one model on many languages (<a href="https://www.aclweb.org/anthology/N19-1423">Devlin et al., 2019</a>; <a href="https://arxiv.org/abs/1812.10464">Artetxe and Schwenk, 2019</a>; <a href="https://www.aclweb.org/anthology/N19-1392">Mulcaire et al., 2019</a>; <a href="https://arxiv.org/abs/1901.07291">Lample and Conneau, 2019</a>). While this is easy to implement and is a strong cross-lingual baseline, it leads to under-representation of low-resource languages (<a href="https://www.aclweb.org/anthology/P19-1027">Heinzerling and Strube, 2019</a>). Multilingual BERT in particular has been the subject of much recent attention (<a href="https://www.aclweb.org/anthology/P19-1493">Pires et al., 2019</a>; <a href="https://arxiv.org/abs/1904.09077">Wu and Dredze, 2019</a>). Despite its strong zero-shot performance, dedicated monolingual language models often are competitive, while being more efficient (<a href="http://ruder.io/publications/">Eisenschlos et al., 2019</a>).</p><h3 id="practical-considerations">Practical considerations</h3><p>Pretraining is cost-intensive. Pretraining the Transformer-XL style model we used in the tutorial takes 5h–20h on 8 V100 GPUs (a few days with 1 V100) to reach a good perplexity. Sharing pretrained models is thus very important. Pretraining is relatively robust to the choice of hyper-parameters—apart from needing a learning rate warm-up for transformers. As a general rule, your model should not have enough capacity to overfit if your dataset is large enough. Masked language modeling (as in BERT) is typically 2-4 times slower to train than standard LM as masking only a fraction of words yields a smaller signal.</p><h1 id="what-is-in-a-representation">What is in a representation?</h1><p>Representations have been shown to be predictive of certain linguistic phenomena such as alignments in translation or syntactic hierarchies. Better performance has been achieved when pretraining with syntax; even when syntax is not explicitly encoded, representations still learn some notion of syntax (<a href="https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00019">Williams et al. 2018</a>). Recent work has furthermore shown that knowledge of syntax can be distilled efficiently into state-of-the-art models (<a href="https://www.aclweb.org/anthology/P19-1337">Kuncoro et al., 2019</a>). Network architectures generally determine what is in a representation. For instance, BERT has been observed to capture syntax (<a href="https://arxiv.org/abs/1905.05950">Tenney et al., 2019</a>; <a href="https://arxiv.org/abs/1901.05287">Goldberg, 2019</a>). Different architectures show different layer-wise trends in terms of what information they capture (<a href="https://www.aclweb.org/anthology/N19-1112">Liu et al., 2019</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/probing_setup.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>The general setup in probing tasks used to study linguistic knowledge within contextual word representations (<a href="https://www.aclweb.org/anthology/N19-1112">Liu et al., 2019</a>).</figcaption></figure><p>The information that a model captures also depends how you look at it: Visualizing activations or attention weights provides a bird's eye view of the model's knowledge, but focuses on a few samples; probes that train a classifier on top of learned representations in order to predict certain properties (as can be seen above) discover corpus-wide specific characteristics, but may introduce their own biases; finally, network ablations are great for improving the model, but may be task-specific.</p><h1 id="adaptation">Adaptation</h1><p>For adapting a pretrained model to a target task, there are several orthogonal directions we can make decisions on: architectural modifications, optimization schemes, and whether to obtain more signal.</p><h2 id="architectural-modifications">Architectural modifications</h2><p>For architectural modifications, the two general options we have are:</p><p><strong>a) Keep the pretrained model internals unchanged</strong>  This can be as simple as adding one or more linear layers on top of a pretrained model, which is commonly done with BERT. Instead, we can also use the model output as input to a separate model, which is often beneficial when a target task requires interactions that are not available in the pretrained embedding, such as span representations or modelling cross-sentence relations.</p><p><strong>b) Modify the pretrained model internal architecture</strong>  One reason why we might want to do this is in order to adapt to a structurally different target task such as one with several input sequences. In this case, we can use the pretrained model to initialize as much as possible of a structurally different target task model. We might also want to apply task-specific modifications such as adding skip or residual connections or attention. Finally, modifying the target task parameters may reduce the number of parameters that need to be fine-tuned by adding bottleneck modules (“adapters”) between the layers of the pretrained model (<a href="https://arxiv.org/abs/1902.00751">Houlsby et al., 2019</a>; <a href="https://arxiv.org/abs/1902.02671">Stickland and Murray, 2019</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/adapter_layer_small.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>An adapter layer (right) as used in a Transformer block (left) (<a href="https://arxiv.org/abs/1902.00751">Houlsby et al., 2019</a>).</figcaption></figure><h2 id="optimization-schemes">Optimization schemes</h2><p>In terms of optimizing the model, we can choose which weights we should update and how and when to update those weights.</p><h3 id="which-weights-to-update">Which weights to update</h3><p>For updating the weights, we can either tune or not tune (the pretrained weights):</p><p><strong>a) Do not change the pretrained weights (feature extraction)</strong>  In practice, a linear classifier is trained on top of the pretrained representations. The best performance is typically achieved by using the representation not just of the top layer, but learning a linear combination of layer representations (<a href="https://arxiv.org/abs/1802.05365">Peters et al., 2018</a>, <a href="https://arxiv.org/abs/1705.08142">Ruder et al., 2019</a>). Alternatively, pretrained representations can be used as features in a downstream model. When adding adapters, only the adapter layers are trained.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/feature_extraction.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>Use of a pretrained model as features in a separate downstream model.</figcaption></figure><p><strong>b) Change the pretrained weights (fine-tuning)</strong>  The pretrained weights are used as initialization for parameters of the downstream model. The whole pretrained architecture is then trained during the adaptation phase.</p><h3 id="how-and-when-to-update-the-weights">How and when to update the weights</h3><p>The main motivation for choosing the order and how to update the weights is that we want to avoid overwriting useful pretrained information and maximize positive transfer. Related to this is the concept of catastrophic forgetting (<a href="https://www.sciencedirect.com/science/article/pii/S0079742108605368">McCloskey &amp; Cohen, 1989</a>; <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.480.7627&amp;rep=rep1&amp;type=pdf">French, 1999</a>), which occurs if a model forgets the task it was originally trained on. In most settings, we only care about the performance on the target task, but this may differ depending on the application.</p><p>A guiding principle for updating the parameters of our model is to update them progressively from top-to-bottom in time, in intensity, or compared to a pretrained model:</p><p><strong>a) Progressively in time (freezing)</strong>  The main intuition is that training all layers at the same time on data of a different distribution and task may lead to instability and poor solutions. Instead, we train layers individually to give them time to adapt to the new task and data. This goes back to layer-wise training of early deep neural networks (<a href="https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">Hinton et al., 2006</a>; <a href="https://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf">Bengio et al., 2007</a>). Recent approaches (<a href="https://www.aclweb.org/anthology/D17-1169">Felbo et al., 2017</a>; <a href="https://arxiv.org/abs/1801.06146">Howard and Ruder, 2018</a>; <a href="https://arxiv.org/abs/1902.10547">Chronopoulou et al., 2019</a>) mostly vary in the combinations of layers that are trained together; all train all parameters jointly in the end. Unfreezing has not been investigated in detail for Transformer models.</p><p><strong>b) Progressively in intensity (lower learning rates)</strong>  We want to use lower learning rates to avoid overwriting useful information. Lower learning rates are particularly important in lower layers (as they capture more general information), early in training (as the model still needs to adapt to the target distribution), and late in training (when the model is close to convergence). To this end, we can use discriminative fine-tuning (<a href="https://aclweb.org/anthology/P18-1031">Howard and Ruder, 2018</a>), which decays the learning rate for each layer as can be seen below. In order to maintain lower learning rates early in training, a triangular learning rate schedule can be used, which is also known as learning rate warm-up in Transformers. <a href="https://arxiv.org/abs/1908.03265">Liu et al. (2019)</a> recently suggest that warm-up reduces variance in the early stage of training.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/discriminative_fine_tuning-1.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>Discriminative fine-tuning (<a href="https://aclweb.org/anthology/P18-1031">Howard and Ruder, 2018</a>).</figcaption></figure><p><strong>c) Progressively vs. a pretrained model (regularization)</strong>  One way to minimize catastrophic forgetting is to encourage target model parameters to stay close to the parameters of the pretrained model using a regularization term (<a href="https://www.aclweb.org/anthology/K17-1029">Wiese et al., CoNLL 2017</a>, <a href="https://www.pnas.org/content/114/13/3521">Kirkpatrick et al., PNAS 2017</a>).</p><h2 id="trade-offs-and-practical-considerations">Trade-offs and practical considerations</h2><p>In general, the more parameters you need to train from scratch the slower your training will be. Feature extraction requires adding more parameters than fine-tuning (<a href="https://arxiv.org/abs/1903.05987">Peters et al., 2019</a>), so is typically slower to train. Feature extraction, however, is more space-efficient when a model needs to be adapted to many tasks as it only requires storing one copy of the pretrained model in memory. Adapters strike a balance by adding a small number of additional parameters per task.</p><p>In terms of performance, no adaptation method is clearly superior in every setting. If source and target tasks are dissimilar, feature extraction seems to be preferable (<a href="https://arxiv.org/abs/1903.05987">Peters et al., 2019</a>). Otherwise, feature extraction and fine-tuning often perform similar, though this depends on the budget available for hyper-parameter tuning (fine-tuning may often require a more extensive hyper-parameter search). Anecdotally, Transformers are easier to fine-tune (less sensitive to hyper-parameters) than LSTMs and may achieve better performance with fine-tuning.</p><p>However, large pretrained models (e.g. BERT-Large) are prone to degenerate performance when fine-tuned on tasks with small training sets. In practice, the observed behavior is often “on-off”: the model either works very well or does not work at all as can be seen in the figure below. Understanding the conditions and causes of this behavior is an open research question.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/sentence_encoders_on_stilts_5k_examples.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>Distribution of task scores across 20 random restarts for BERT (red) and BERT that was fine-tuned on MNLI (green) when fine-tuning on no more than 5k examples for each task (<a href="https://arxiv.org/abs/1811.01088">Phang et al., 2018</a>).</figcaption></figure><h2 id="getting-more-signal">Getting more signal</h2><p>The target task is often a low-resource task. We can often improve the performance of transfer learning by combining a diverse set of signals:</p><p><strong>Sequential adaptation</strong>  If related tasks are available, we can fine-tune our model first on a related task with more data before fine-tuning it on the target task. This<br>helps particularly for tasks with limited data and similar tasks (<a href="https://arxiv.org/abs/1811.01088v2">Phang et al., 2018</a>) and improves sample efficiency on the target task (<a href="https://arxiv.org/abs/1901.11373">Yogatama et al., 2019</a>).</p><p><strong>Multi-task fine-tuning</strong>  Alternatively, we can also fine-tune the model jointly on related tasks together with the target task. The related task can also be an unsupervised auxiliary task. Language modelling is a good choice for this and has been shown to help even without pretraining (<a href="https://arxiv.org/abs/1704.07156">Rei et al., 2017</a>). The task ratio can optionally be annealed to de-emphasize the auxiliary task towards the end of training (<a href="https://arxiv.org/abs/1902.10547">Chronopoulou et al., NAACL 2019</a>). Language model fine-tuning is used as a separate step in ULMFiT (<a href="https://aclweb.org/anthology/P18-1031">Howard and Ruder, 2018</a>). Recently, multi-task fine-tuning has led to improvements even with many target tasks (<a href="https://arxiv.org/abs/1901.11504">Liu et al., 2019</a>, <a href="https://www.aclweb.org/anthology/P19-1439">Wang et al., 2019</a>).</p><p><strong>Dataset slicing</strong>  Rather than fine-tuning with auxiliary tasks, we can use <a href="https://dawn.cs.stanford.edu/2019/03/22/glue/">auxiliary heads that are trained only on particular subsets of the data</a>. To this end, we would first analyze the errors of the model, use heuristics to automatically identify challenging subsets of the training data, and then train auxiliary heads jointly with main head.</p><p><strong>Semi-supervised learning</strong>  We can also use semi-supervised learning methods to make our model's predictions more consistent by perturbing unlabelled examples. The perturbation can be noise, masking (<a href="https://arxiv.org/abs/1809.08370">Clark et al., 2018</a>), or data augmentation, e.g. back-translation (<a href="https://arxiv.org/abs/1904.12848">Xie et al., 2019</a>).</p><p><strong>Ensembling</strong>  To improve performance the predictions of models fine-tuned with different hyper-parameters, fine-tuned with different pretrained models, or trained on different target tasks or dataset splits may be combined.</p><p><strong>Distilling</strong>  Finally, large models or ensembles of models may be distilled into a single, smaller model. The model can also be a lot simpler (Tang et al., 2019) or have a different inductive bias (<a href="https://www.aclweb.org/anthology/P19-1337">Kuncoro et al., 2019</a>). Multi-task fine-tuning can also be combined with distillation (<a href="https://arxiv.org/abs/1907.04829">Clark et al., 2019</a>).</p><h1 id="down-stream-applications">Down-stream applications</h1><p>Pretraining large-scale models is costly, not only in terms of computation but also in terms of the environmental impact (<a href="https://www.aclweb.org/anthology/P19-1355">Strubell et al., 2019</a>). Whenever possible, it's best to use open-source models. If you need to train your own models, please share your pretrained models with the community.</p><h3 id="frameworks-and-libraries">Frameworks and libraries</h3><p>For sharing and accessing pretrained models, different options are available:</p><p><strong>Hubs</strong>  Hubs are central repositories that provide a common API for accessing pretrained models. The two most common hubs are <a href="https://www.tensorflow.org/hub">TensorFlow Hub</a> and <a href="https://pytorch.org/hub">PyTorch Hub</a>. Hubs are generally simple to use; however, they act more like a black-box as the source code of the model cannot be easily accessed. In addition, modifying the internals of a pretrained model architecture can be difficult.</p><p><strong>Author released checkpoints</strong>  Checkpoint files generally contain all the weights of a pretrained model. In contrast to hub modules, the model graph still needs to be created and model weights need to be loaded separately. As such, checkpoint files are more difficult to use than hub modules, but provide you with full control over the model internals.</p><p><strong>Third-party libraries</strong>  Some third-party libraries like <a href="https://allennlp.org/">AllenNLP</a>, <a href="https://github.com/fastai/fastai">fast.ai</a>, and <a href="https://github.com/huggingface/pytorch-transformers">pytorch-transformers</a> provide easy access to pretrained models. Such libraries typically enable fast experimentation and cover many standard use cases for transfer learning.</p><p>For examples of how such models and libraries can be used for downstream tasks, have a look at the <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g569f436ced_0_30">code snippets</a> in the slides, the <a href="http://tiny.cc/NAACLTransferColab">Colaboratory notebook</a>, and the <a href="http://tiny.cc/NAACLTransferCode">code</a>.</p><h1 id="open-problems-and-future-directions">Open problems and future directions</h1><p>There are many open problems and interesting future research directions. Below is just an updated selection. For more pointers, have a look at <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g569f436ced_0_34">the slides</a>.</p><h3 id="shortcomings-of-pretrained-language-models">Shortcomings of pretrained language models</h3><p>Pretrained language models are still bad at fine-grained linguistic tasks (<a href="https://arxiv.org/abs/1903.08855">Liu et al., 2019</a>), hierarchical syntactic reasoning (<a href="https://www.aclweb.org/anthology/P19-1337">Kuncoro et al., 2019</a>), and common sense (when you actually make it difficult; <a href="https://arxiv.org/abs/1905.07830">Zellers et al., 2019</a>). They still fail at natural language generation, in particular maintaining long-term dependencies, relations, and coherence. They also tend to overfit to surface form information when fine-tuned and can still mostly be seen as ‘rapid surface learners’.</p><p>As we have noted above, particularly large models that are fine-tuned on small amounts of data are difficult to optimize and suffer from high variance. Current pretrained language models are also very large. Distillation and pruning are two ways to deal with this.</p><h3 id="pretraining-tasks">Pretraining tasks</h3><p>While the language modelling objective has shown to be effective empirically, it has its weaknesses. Lately, we have seen that bidirectional context and modelling contiguous word sequences is particularly important. Maybe most importantly, language modelling encourages a focus on syntax and word co-occurrences and only provides a weak signal for capturing semantics and long-term context. We can take inspiration from other forms of self-supervision. In addition, we can design specialized pretraining tasks that explicitly learn certain relationships (<a href="https://arxiv.org/abs/1907.10529">Joshi et al., 2019</a>, <a href="https://arxiv.org/abs/1907.12412">Sun et al., 2019</a>).</p><p>On the whole, it is difficult to learn certain types of information from raw text. Recent approaches incorporate structured knowledge (<a href="http://arxiv.org/abs/1905.07129">Zhang et al., 2019</a>; <a href="https://www.aclweb.org/anthology/P19-1598">Logan IV et al., 2019</a>) or leverage multiple modalities (<a href="https://arxiv.org/abs/1904.01766">Sun et al., 2019</a>; <a href="https://arxiv.org/abs/1908.02265">Lu et al., 2019</a>) as two potential ways to mitigate this problem.</p><h2 id="citation">Citation</h2><p> If you found this post helpful, consider citing <a href="https://www.aclweb.org/anthology/N19-5004">the tutorial</a> as:</p><pre><code>@inproceedings{ruder2019transfer,
  title={Transfer Learning in Natural Language Processing},
  author={Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials},
  pages={15--18},
  year={2019}
}</code></pre><h3 id="translations">Translations</h3><p>This article has been translated into the following languages:</p><ul><li><a href="https://www.infoq.cn/article/zD5QkcIzF9253friWPVd">Chinese</a> (by Sambodhi)</li><li><a href="https://www.ibidemgroup.com/edu/traduccion-aprendizaje-pnl/">Spanish</a> (by Ibidem Group)</li></ul>]]></content:encoded></item><item><title><![CDATA[EurNLP]]></title><description><![CDATA[The first European NLP Summit (EurNLP) will take place in London on October 11, 2019. It is an opportunity to foster discussion and collaboration between researchers in and around Europe.]]></description><link>http://ruder.io/eurnlp/</link><guid isPermaLink="false">5ce51295d55ba83bebb51674</guid><category><![CDATA[events]]></category><category><![CDATA[natural language processing]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Thu, 04 Jul 2019 20:56:39 GMT</pubDate><media:content url="http://ruder.io/content/images/2019/07/EurNLP-ban_2.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2019/07/EurNLP-ban_2.jpg" alt="EurNLP"><p>The <a href="https://www.eurnlp.org/">first European NLP Summit (EurNLP)</a> will take place in London on October 11. Registration is <a href="https://eurnlp.splashthat.com/">open now</a>. Travel grants are available.</p><p>The Natural Language Processing community has seen unprecedented growth in recent years (see for instance the <a href="http://acl2019pcblog.fileli.unipi.it/?p=156">ACL 2019 Chairs blog</a>). As more people are entering the field and NLP research sprouts in more places, making meaningful connections and communicating effectively becomes more difficult.</p><p>To successfully scale our conferences, we require structures that enable us to integrate and to provide mentorship and advice to the next generation of researchers and engineers. In addition, we are in need of mechanisms that facilitate collaboration and the exchange of ideas in our community. </p><p>Conferences such as <a href="https://naacl2019.org/">NAACL 2019</a> have been fostering initiatives that encourage diversity and inclusion, such as mentoring, childcare, and live captions. In addition, meetups with demographic affinity groups (WiNLP, Queer in AI, Black in AI, etc.) and venues such as “birds-of-a-feather” tables facilitate meeting like-minded people.</p><p>Beyond the global exchange of information and ideas in the field, we believe that an effective way to foster such connections is on the regional level, to connect newcomers with experts in their vicinity and with each other. In order to keep the global community vital and healthy, we need to ensure that local talent is able to flourish and that role models and mentors are available to provide inspiration and feedback.</p><p>Europe has a vibrant and world-leading NLP community, with research hubs across the continent and positioned both in academia and industry. The aim of <a href="https://www.eurnlp.org/">EurNLP</a> (pronounced “your NLP”) is to bring our community closer together. We want to provide an informal focused get-together event to foster discussion and collaboration between researchers in and around Europe. EurNLP is supported by EACL and aimed to be complementary to scientific publication-oriented European events. It is also intended to be complementary to summer schools that are focused on educating the next generation and can now be found <a href="http://mlss.cc/">all around the globe</a>, including <a href="http://www.deeplearningindaba.com/">Africa</a>, <a href="https://khipu.ai/">South America</a>, <a href="https://www.sea-mls.com/">Southeast Asia</a>, and <a href="http://lxmls.it.pt">Europe</a>.</p><p>EurNLP is inspired by the highly successful West Coast NLP Summit (<a href="https://www.wecnlp.ai/">WeCNLP</a>). In contrast to a purely scientific symposium, EurNLP is an industry-academia partnership and hosted in cooperation with companies. In order to grow the European NLP community and solve real-world problems our society is facing, we must bridge academia and industry and connect fundamental and applied research.</p><p>The <a href="https://www.eurnlp.org/">first European NLP summit</a> will take place in London on October 11, 2019. The registration for EurNLP is <a href="https://eurnlp.splashthat.com/">open now</a>. Thanks to our industrial host Facebook and sponsorship from DeepMind, registration is not only free, but there are also travel grants available to students with an accepted abstract.</p><p>—The EurNLP Organisation Committee (Barbara Plank, Sebastian Ruder, Sebastian Riedel, Armand Joulin, Fabrizio Silvestri)</p>]]></content:encoded></item><item><title><![CDATA[NAACL 2019 Highlights]]></title><description><![CDATA[This post discusses highlights of NAACL 2019. It covers transfer learning, common sense reasoning, natural language generation, bias, non-English languages, and diversity and inclusion.]]></description><link>http://ruder.io/naacl2019/</link><guid isPermaLink="false">5cfd6e4dd55ba83bebb516be</guid><category><![CDATA[events]]></category><category><![CDATA[transfer learning]]></category><category><![CDATA[cross-lingual]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Sun, 09 Jun 2019 20:56:15 GMT</pubDate><media:content url="http://ruder.io/content/images/2019/06/transfer_learning_tutorial_room-4.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2019/06/transfer_learning_tutorial_room-4.jpg" alt="NAACL 2019 Highlights"><p>Update 19.04.20: Added a <a href="#translations">translation of this post in Spanish</a>.</p><p>This post discusses highlights of the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (<a href="https://naacl2019.org/">NAACL 2019</a>).</p><p>You can find past highlights of conferences <a href="http://ruder.io/tag/events/">here</a>. The conference accepted 424 papers (which you can find <a href="https://www.aclweb.org/anthology/events/naacl-2019/#n19-1">here</a>) and had 1575 participants (see the <a href="https://naacl2019.org/downloads/naacl2019-intro-slides.pdf">opening session slides</a> for more details). These are the topics that stuck out for me most:</p><ul><li><a href="#transfer-learning">Transfer learning</a></li><li><a href="#common-sense-reasoning">Common sense reasoning</a></li><li><a href="#natural-language-generation">Natural language generation</a></li><li><a href="#bias">Bias</a></li><li><a href="#non-english-languages">Non-English languages</a></li><li><a href="#diversity-and-inclusion">Diversity and inclusion</a></li></ul><h2 id="transfer-learning">Transfer learning</h2><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/06/transfer_learning_tutorial_room_2.jpg" class="kg-image" alt="NAACL 2019 Highlights"><figcaption>The room at the Transfer Learning in NLP tutorial (Image credit: <a href="https://twitter.com/soldni/status/1135269017512546304">Luca Soldaini</a>)</figcaption></figure><p>Interest in transfer learning remains high. The <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit?usp=sharing">Transfer Learning in NLP</a> tutorial (pictured above and organized by <a href="https://scholar.google.com/citations?user=K5nCPZwAAAAJ&amp;hl=en">Matthew Peters</a>, <a href="https://www.cs.cmu.edu/~sswayamd/">Swabha Swayamdipta</a>, <a href="http://thomwolf.io/">Thomas Wolf</a>, and me) was packed. NAACL 2019 awarded the best long paper award to <a href="https://www.aclweb.org/anthology/N19-1423">BERT</a>, arguably the most impactful recent transfer learning method. Despite its recency, conference papers already leveraged BERT for <a href="https://www.aclweb.org/anthology/N19-1035">aspect-based sentiment analysis</a>, <a href="https://www.aclweb.org/anthology/N19-1242">review reading comprehension</a>, <a href="https://www.aclweb.org/anthology/N19-1421">common sense reasoning</a>, and <a href="https://www.aclweb.org/anthology/N19-4013">open-domain question answering</a>. </p><p>At the <a href="https://repeval2019.github.io/">RepEval</a> workshop, Kristina Toutanova discussed how to <a href="https://arxiv.org/abs/1906.00300">use transfer learning for open-domain question answering</a>. With appropriate pretraining using an Inverse Cloze Task, the retriever and reader can be fine-tuned directly on QA pairs without an intermediate IR system. This demonstrates that <strong>a careful initialization + fine-tuning</strong> are two key ingredients for transfer learning and work even on challenging tasks. This has also been shown in the past for <a href="https://www.aclweb.org/anthology/P18-1073">learning cross-lingual word embeddings</a> and <a href="https://arxiv.org/abs/1804.07755">unsupervised MT</a>. She also made the point that <strong>single-vector sentence/paragraph representations are very useful for retrieval</strong>—and that we should continue to work on them. Overall, there are many exciting research directions in transfer learning in NLP, some of which we outlined at <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5882add69e_5_673">the end of our tutorial</a>. My other highlights include: </p><ul><li><strong>Single-step Auxiliary loss Transfer Learning</strong> (SiATL; <a href="https://www.aclweb.org/anthology/N19-1213">Chronopoulou et al.</a>), an "embarrassingly simple" approach that reduces some of the complexity of <a href="https://arxiv.org/abs/1801.06146">ULMFiT</a> via multi-task learning and exponentially decaying the auxiliary loss.</li><li><strong>AutoSeM</strong> (<a href="https://www.aclweb.org/anthology/N19-1355">Guo et al</a>.), a two-stage pipeline for multi-task learning that utilizes multi-armed bandits and Bayesian optimization to learn the best auxiliary task and the best task mixing ratio respectively.</li><li>An <strong>evaluation of contextual representation across 16 tasks</strong> (<a href="https://www.aclweb.org/anthology/N19-1112">Liu et al.</a>) that shows that they are bad at capturing fine-grained linguistic knowledge and higher layers in RNNs are more task-specific than in Transformers.</li></ul><h2 id="common-sense-reasoning">Common sense reasoning</h2><p>Language modelling is a pretraining task that has been shown to learn generally useful representations at scale. However, there are some things that are simply never written, even in billions of tokens.<strong> Overcoming this reporting bias is a key challenge</strong> in adapting language models to more complex tasks. To test reasoning with knowledge that is often left unsaid, the <a href="https://www.aclweb.org/anthology/N19-1421">best resource paper</a> used the common sense knowledge base ConceptNet as “seed”. They created <a href="https://www.tau-nlp.org/commonsenseqa">CommonsenseQA</a>, a dataset of multiple-choice questions where most answers have the same relation to the target concept (see below).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/06/commonsenseqa_examples.png" class="kg-image" alt="NAACL 2019 Highlights"><figcaption>Example question-answer pairs in CommonsenseQA (Source: <a href="https://www.aclweb.org/anthology/N19-1421">Talmor et al.</a>)</figcaption></figure><p>This requires the model to use common sense rather than just relational or co-occurrence information to answer the question. BERT achieves 55.9% accuracy on this dataset—and is estimated to achieve around 75% with 100k examples—still well below human performance 88.9%. What does it take to get to those 88.9%? Most likely <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5888218f39_11_238"><strong>structured knowledge, interactive and multimodal learning</strong></a>. In his talk at the <a href="https://sites.google.com/view/sivl2019/home">Workshop on Shortcomings in Vision and Language</a> (SiLV), Yoav Artzi discussed <a href="https://yoavartzi.com/slides/2019_06_06_sivl_naacl.pdf">language diversity in grounded NLU</a>, noting that we need to move from synthetic to more realistic images for learning grounded representations.</p><p>Another prerequisite for natural language understanding is compositional reasoning. The <a href="https://nlitutorial.github.io/">Deep Learning for Natural Language Inference tutorial</a> discussed natural language inference, a common benchmark for evaluating such forms of reasoning in-depth. I particularly liked the following papers:</p><ul><li><strong>A label consistency framework for procedural text comprehension</strong> (<a href="https://www.aclweb.org/anthology/N19-1244">Du et al.</a>) that encourages consistency between predictions from descriptions of the same process. This is a clever way to use intuition and additional data to incorporate an inductive bias into the model.</li><li><strong>Discrete Reasoning Over the content of Paragraphs</strong> (DROP; <a href="https://www.aclweb.org/anthology/N19-1246">Dua et al.</a>), which requires models to resolve references in a question and perform discrete operations (e.g. addition, counting, sorting) over multiple referents in the text.</li></ul><h2 id="natural-language-generation">Natural language generation</h2><p>At the <a href="https://neuralgen.io/">NeuralGen</a> workshop, Graham Neubig <a href="http://www.phontron.com/slides/neubig19neuralgen.pdf">discussed methods to optimize a non-differentiable objective function</a> such as BLEU directly, including minimum risk training and REINFORCE and tricks to deal with their instability and get them to work. While we had touched on <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g512941aa9e_0_36">transfer learning for natural language generation</a> (NLG) in our tutorial, Sasha Rush provided many more details and <a href="http://nlp.seas.harvard.edu/slides/Pre-training%20for%20Generation.pdf">discussed different methods of using language models</a> to improve NLG quality. Another way to improve sample quality is to focus on decoding. Yejin Choi discussed a <a href="https://arxiv.org/abs/1904.09751">new sampling method</a> that samples from the head of the distribution and leads to better text quality. She also discussed the generation of fake news and how large pretrained language models such as <a href="https://arxiv.org/abs/1905.12616">Grover</a> can be used to defend against them. </p><p>Generative adversarial networks (GANs) are a popular way to generate images, but so far have underperformed for language. The <a href="https://drive.google.com/drive/folders/1E4uHe4_TD4yDJws3t1kXJQanUFJiqpBB">Deep Adversarial Learning for NLP tutorial</a> argued that we should not give up on them as the <strong>unsupervised or self-supervised learning done by GANs has many applications in NLP</strong>.</p><p>Another compelling aspect of generation is to enable multiple agents to communicate effectively. Besides providing a window into how language emerges, it may be necessary for interactive learning and to transfer knowledge among agents. Angeliki Lazaridou discussed in her SiLV workshop talk that deep reinforcement learning tools seem to work well for this setting but argued that <strong>better biases are needed</strong>. In addition, it is still <strong>difficult to <a href="https://arxiv.org/abs/1612.07182">interface emergent language to natural language</a></strong>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/06/huse_trade-offs.png" class="kg-image" alt="NAACL 2019 Highlights"><figcaption>Trade-offs between quality and diversity of different models (circles) on NLG tasks (Image credit: <a href="https://www.aclweb.org/anthology/N19-1169">Hashimoto et al.</a>)</figcaption></figure><p>I also enjoyed the following papers:</p><ul><li><strong>Human Unified with Statistical Evaluation</strong> (HUSE; <a href="https://www.aclweb.org/anthology/N19-1169">Hashimoto et al.</a>), a new metric for natural language generation that can consider both diversity and quality and yields a Pareto frontier by trading off one of the two (see above). Methods such as temperature annealing result in higher quality, but reduce diversity.</li><li><strong>Separating planning from realization</strong> (<a href="https://arxiv.org/abs/1904.03396">Moryossef et al.</a>) can improve the quality of generated text from structured data such as RDF triplets as there are often multiple ways structured information can be realized in text.</li><li><strong>Decoupling syntax and surface form generation</strong> (<a href="https://arxiv.org/abs/1804.07707">Cao &amp; Clark</a>) is another way to deal with the underspecified problem of text generation from structured data (in this case, abstract meaning representations). </li><li><strong>A systematic analysis that probes how useful the visual modality actually is for multimodal translation</strong> (<a href="https://www.aclweb.org/anthology/N19-1422">Caglayan et al.</a>) and was awarded the best short paper award. It observes that models with less textual information more strongly rely on the visual context, contrary to current beliefs.</li></ul><h2 id="bias">Bias</h2><p>The <a href="https://naacl2019.org/calls/papers/#theme-topics">theme of the conference</a> was model bias. The diverse sets of keynotes fit very well into this theme. The first keynote by <a href="http://randomwalker.info/">Arvind Narayanan</a> in particular highlighted one under-appreciated aspect of bias, i.e. that we can <strong>leverage the bias in our models to improve our understanding of human culture</strong>.</p><p>On the whole, there is a <strong>fine line between desirable and undesirable bias</strong>. We often try to encode inductive bias about how the world works, such as <a href="http://ruder.io/emnlp-2018-highlights/index.html#inductive-bias">objects being invariant to translation</a>. On the other hand, we do not want our models to learn superficial cues or relations that are not part of our possibly idealized perception of the world, such as <a href="https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf">gender bias</a>. Ultimately, super-human performance should not just entail that models outperform humans quantitively but also that they are less biased and fallible.</p><p>Lastly, we should be conscious that <strong>technology has lasting impact in the real world</strong>. As one vivid example of this, <a href="https://textio.com/team/">Kieran Snyder</a> recounted in her keynote the time when she had to design a sorting algorithm for <a href="https://en.wikipedia.org/wiki/Sinhala_language">Sinhala</a> (see below). Sorting Sinhalese names was necessary for the Sri Lankan government to be able to search for survivors in the aftermath of the 2004 tsunami. Her decision on how to alphabetize the language later became part of an official government policy.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/06/sinhala_vowels.jpg" class="kg-image" alt="NAACL 2019 Highlights"><figcaption>Vowels in Sinhala (Image credit: <a href="https://www.omniglot.com/writing/sinhala.htm">Omniglot</a>)</figcaption></figure><p>Some of my favourite papers on bias include:</p><ul><li><strong>Debiasing methods only superficially remove bias in word embeddings </strong>(<a href="https://www.aclweb.org/anthology/N19-1061">Gonen &amp; Goldberg</a>); bias is still reflected in—and can be recovered from—the distances in the debiased embeddings.</li><li><strong>An evaluation of bias in contextualized word embeddings</strong> (<a href="https://www.aclweb.org/anthology/N19-1064">Zhao et al.</a>) finds that ELMo syntactically and unequally encodes gender information and—more importantly—that this bias is inherited by downstream models, such as a coreference system. </li></ul><h2 id="non-english-languages">Non-English languages</h2><p>On the topic of different languages, during the conference, the <a href="https://twitter.com/emilymbender/status/1135691925674217473">“Bender Rule”</a>—named after <a href="https://faculty.washington.edu/ebender/">Emily Bender</a> who is known for her advocacy for multilingual language processing, among other things—was <a href="https://twitter.com/EvpokPadding/status/1136649868800352262">frequently</a> <a href="https://twitter.com/amitmoryossef/status/1136359765758697478">invoked</a> <a href="https://twitter.com/adinamwilliams/status/1136313903988822016">after</a> <a href="https://twitter.com/EmmaSManning/status/1136395448313401346">presentations</a>. In short, the rule states: "<strong>Always name the language(s) you are working on.</strong>" Not explicitly identifying the language under consideration leads to English being perceived as the default and as proxy for other languages, which is problematic in many ways (see <a href="http://faculty.washington.edu/ebender/papers/Bender-SDSS-2019.pdf">Emily's slides</a> for a thorough rationale).</p><p>In this vein, some of my favourite papers from the conference investigate how the performance of our models changes as we apply them other languages:</p><ul><li><strong>Polyglot contextual representations </strong>(<a href="https://www.aclweb.org/anthology/N19-1392">Mulcaire et al.</a>) that are trained on English and an additional language by initializing word embeddings with cross-lingual representations. For some settings (Chinese SRL, Arabic NER), cross-lingual training yields large improvements.</li><li><strong>A study on transfer of dependency parsers trained on English to 30 other languages </strong>(<a href="https://www.aclweb.org/anthology/N19-1253">Ahmad et al.</a>) finds that RNNs trained on English transfer well to languages close to English, but self-attention models transfer better to distant languages.</li><li><strong>An unsupervised POS tagger for low-resource languages</strong> (<a href="https://www.aclweb.org/anthology/N19-1252">Cardenas et al.</a>) that "deciphers" Brown cluster ids in order to generate the POS sequence and achieves state-of-the-art performance on Sinhalese (see above).</li></ul><h2 id="diversity-and-inclusion">Diversity and inclusion</h2><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/06/badge_stickers.jpg" class="kg-image" alt="NAACL 2019 Highlights"><figcaption>Badge stickers at NAACL 2019 (Image credit: <a href="https://twitter.com/natschluter/status/1135963599216750593">Natalie Schluter</a>)</figcaption></figure><p>As the community is growing it is important that new members feel included and that their voices are heard. NAACL 2019 put into effect a wide range of initiatives in this regard, from thoughtful touches such as badge stickers (see above) to matching newcomers with mentors and “big siblings”, to fundamental ones such as childcare (see below) and <a href="https://naacl2019.org/captions/">live captions</a>. I particularly appreciated the <a href="https://twitter.com/NAACLHLT/status/1134181082151227393">live tweeting</a>, which made the conference accessible to people who could not attend.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/06/childcare_room-1.jpg" class="kg-image" alt="NAACL 2019 Highlights"><figcaption>Childcare room at NAACL 2019 (Image credit: <a href="https://twitter.com/KieranSnyder/status/1135924564771450880">Kieran Snyder</a>)</figcaption></figure><h2 id="translations">Translations</h2><p>This post has been translated into the following languages: </p><ul><li><a href="https://www.traductor-jurado.org/blog/traduccion-lenguaje-natural/">Spanish</a></li></ul><p><em>Cover image: The room at the <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit?usp=sharing">Transfer Learning in NLP</a> tutorial (Image credit: <a href="https://twitter.com/thedansimonson/status/1135275735730597889">Dan Simonson</a>)</em></p>]]></content:encoded></item><item><title><![CDATA[Neural Transfer Learning for Natural Language Processing (PhD thesis)]]></title><description><![CDATA[This post discusses my PhD thesis Neural Transfer Learning for Natural Language Processing and some new material presented in it.]]></description><link>http://ruder.io/thesis/</link><guid isPermaLink="false">5c76ff7c1b9b0d18555b9eb6</guid><category><![CDATA[transfer learning]]></category><category><![CDATA[natural language processing]]></category><category><![CDATA[multi-task learning]]></category><category><![CDATA[domain adaptation]]></category><category><![CDATA[cross-lingual]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Sat, 23 Mar 2019 18:00:43 GMT</pubDate><media:content url="http://ruder.io/content/images/2019/03/transfer_learning_taxonomy-1.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2019/03/transfer_learning_taxonomy-1.png" alt="Neural Transfer Learning for Natural Language Processing (PhD thesis)"><p></p><p>I finally got around to submitting <a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf">my thesis</a>. The thesis touches on the four areas of transfer learning that are most prominent in current Natural Language Processing (NLP): <strong>domain adaptation</strong>, <strong>multi-task learning</strong>, <strong>cross-lingual learning</strong>, and <strong>sequential transfer learning</strong>.</p><p>Most of the work in the thesis has been previously presented (see <a href="http://ruder.io/publications/">Publications</a>). Nevertheless, there are some new parts as well. The most notable are:</p><ol><li><strong>a background chapter</strong> (<a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=28">§2</a>) that lays out key concepts in terms of probability and information theory, machine learning, neural networks, and NLP and connects these to their usage in subsequent chapters;</li><li><strong>a taxonomy for transfer learning for NLP</strong> (<a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=64">§3.1.3</a>, see below) that adapts the taxonomy of <a href="http://www.cs.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf">Pan and Yang (2010)</a> to contemporary settings in NLP;</li><li><strong>an updated review of multi-task learning</strong> (<a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=65">§3.2</a>) that discusses more recent advances and choices in multi-task learning;</li><li><strong>reviews of sequential transfer learning and domain adaptation</strong> (<a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=81">§3.3</a> and <a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=104">§3.4</a>) that identify common themes in each of the research areas;</li><li>and <strong>future directions</strong> (<a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=273">§8.3</a>) in each area that particularly excite me.</li></ol><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/03/transfer_learning_taxonomy.png" class="kg-image" alt="Neural Transfer Learning for Natural Language Processing (PhD thesis)"><figcaption>A taxonomy for transfer learning for NLP.</figcaption></figure><p>Whenever possible, I've tried to draw connections between methods used in different areas of transfer learning. It's a longer read but I hope it may still be helpful to some of you. You can download the complete thesis <a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf">here</a>.</p><p>If you found some material in the thesis helpful, I'd appreciate if you could cite it using the below BibTex:</p><!--kg-card-begin: markdown--><pre><code>@PhdThesis{Ruder2019Neural,
  title={Neural Transfer Learning for Natural Language Processing},
  author={Ruder, Sebastian},
  year={2019},
  school={National University of Ireland, Galway}
}
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item></channel></rss>
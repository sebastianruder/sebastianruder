<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Sebastian Ruder]]></title><description><![CDATA[I'm a research scientist in the Language team at DeepMind. I blog about natural language processing, machine learning, and deep learning.]]></description><link>http://ruder.io/</link><image><url>http://ruder.io/favicon.png</url><title>Sebastian Ruder</title><link>http://ruder.io/</link></image><generator>Ghost 3.11</generator><lastBuildDate>Sun, 04 Apr 2021 10:45:01 GMT</lastBuildDate><atom:link href="http://ruder.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Recent Advances in Language Model Fine-tuning]]></title><description><![CDATA[This article provides an overview of recent methods to fine-tune large pre-trained language models.]]></description><link>http://ruder.io/recent-advances-lm-fine-tuning/</link><guid isPermaLink="false">60226bf5a96736463841e6a2</guid><category><![CDATA[language models]]></category><category><![CDATA[natural language processing]]></category><category><![CDATA[transfer learning]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Wed, 24 Feb 2021 09:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2021/02/fine-tuning_methods.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2021/02/fine-tuning_methods.png" alt="Recent Advances in Language Model Fine-tuning"><p>Fine-tuning a pre-trained language model (LM) has become the de facto standard for doing transfer learning in natural language processing. Over the last three years (<a href="https://thegradient.pub/nlp-imagenet/">Ruder, 2018</a>), fine-tuning (<a href="https://www.aclweb.org/anthology/P18-1031/">Howard &amp; Ruder, 2018</a>) has superseded the use of feature extraction of pre-trained embeddings (<a href="https://www.aclweb.org/anthology/N18-1202/">Peters et al., 2018</a>) while pre-trained language models are favoured over models trained on translation (<a href="https://papers.nips.cc/paper/2017/file/20c86a628232a67e7bd46f76fba7ce12-Paper.pdf">McCann et al., 2018</a>), natural language inference (<a href="https://www.aclweb.org/anthology/D17-1070/">Conneau et al., 2017</a>), and other tasks due to their increased sample efficiency and performance (<a href="https://www.aclweb.org/anthology/W18-5448/">Zhang and Bowman, 2018</a>). The empirical success of these methods has led to the development of ever larger models (<a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>; <a href="https://jmlr.org/papers/v21/20-074.html">Raffel et al., 2020</a>). Recent models are so large in fact that they can achieve reasonable performance <em>without</em> any parameter updates (<a href="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown et al., 2020</a>). The limitations of this zero-shot setting (see <a href="#text-to-text-fine-tuning">this section</a>), however, make it likely that in order to achieve the best performance or stay reasonably efficient, <strong>fine-tuning will continue to be the modus operandi when using large pre-trained LMs in practice.</strong></p><p>In the standard transfer learning setup (see below; see <a href="https://ruder.io/state-of-transfer-learning-in-nlp/">this post</a> for a general overview), a model is first pre-trained on large amounts of unlabelled data using a language modelling loss such as masked language modelling (MLM; <a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>). The pre-trained model is then fine-tuned on labelled data of a downstream task using a standard cross-entropy loss.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/02/pretraining_finetuning.png" class="kg-image" alt="Recent Advances in Language Model Fine-tuning"><figcaption>The standard pre-training—fine-tuning setting (adapted from <a href="https://tiny.cc/NAACLTransfer">(Ruder et al., 2019)</a>)</figcaption></figure><p>While pre-training is compute-intensive, fine-tuning can be done comparatively inexpensively. Fine-tuning is more important for the practical usage of such models as individual pre-trained models are downloaded—and fine-tuned—millions of times (see <a href="https://huggingface.co/models?sort=downloads">the Hugging Face models repository</a>). Consequently, fine-tuning is the main focus of this post. In particular, I will highlight the most recent advances that have shaped or are likely to change the way we fine-tune language models, which can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/02/fine-tuning_methods_overview.png" class="kg-image" alt="Recent Advances in Language Model Fine-tuning"><figcaption>Overview of fine-tuning methods discussed in this post.</figcaption></figure><h2 id="adaptive-fine-tuning">Adaptive fine-tuning</h2><p>Even though pre-trained language models are more robust in terms of out-of-distribution generalisation than previous models (<a href="https://www.aclweb.org/anthology/2020.acl-main.244/">Hendrycks et al., 2020</a>), they are still poorly equipped to deal with data that is substantially different from the one they have been pre-trained on. Adaptive fine-tuning is a way to bridge such a shift in distribution by fine-tuning the model on data that is closer to the distribution of the target data. Specifically, adaptive fine-tuning involves fine-tuning the model on additional data prior to task-specific fine-tuning, which can be seen below. Importantly, the model is fine-tuned with the pre-training objective, so adaptive fine-tuning only requires unlabelled data.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/02/adaptive_fine-tuning-1.png" class="kg-image" alt="Recent Advances in Language Model Fine-tuning"><figcaption>Adaptive fine-tuning as part of the standard transfer learning setting. A pre-trained model is trained with the pre-training loss (typically masked language modelling) on data that is closer to the target distribution.</figcaption></figure><!--kg-card-begin: markdown--><p>Formally, given a target domain $\mathcal{D}_T$ consisting of a feature space $\mathcal{X}$ and a marginal probability distribution over the feature space $P(X)$ where $X = \{x_1, \ldots, x_n \} \in \mathcal{X}$ (<a href="https://ieeexplore.ieee.org/document/5288526">Pan and Yang, 2009</a>; <a href="https://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=62">Ruder, 2019</a>), adaptive fine-tuning allows us to learn about both the feature space $\mathcal{X}$ and the distribution of the target data $P(X)$.</p>
<!--kg-card-end: markdown--><p>Variants of adaptive fine-tuning—domain, task, and language-adaptive fine-tuning—have been used to adapt a model to data of the target domain, target task, and target language respectively. <a href="https://papers.nips.cc/paper/2015/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf">Dai and Le (2015)</a> first showed the benefits of domain-adaptive fine-tuning. <a href="https://www.aclweb.org/anthology/P18-1031/">Howard and Ruder (2018)</a> later demonstrated improved sample efficiency by fine-tuning on in-domain data as part of ULMFiT. They also proposed task-adaptive fine-tuning, which fine-tunes the model with the pre-training objective on the task training data. As the pre-training loss provides richer information for modelling the target data compared to the cross-entropy over one-hot task labels, task-adaptive fine-tuning is useful beyond regular fine-tuning. Alternatively, adaptive and regular fine-tuning can be done jointly via multi-task learning (<a href="https://www.aclweb.org/anthology/N19-1213/">Chronopoulou et al., 2019</a>).</p><p>Domain and task-adaptive fine-tuning have recently been applied to the latest generation of pre-trained models (<a href="https://www.aclweb.org/anthology/P19-1335/">Logeswaran et al., 2019</a>; <a href="https://www.aclweb.org/anthology/D19-1433/">Han and Eisenstein, 2019</a>; <a href="https://www.aclweb.org/anthology/P19-1373/">Mehri et al., 2019</a>). <a href="https://www.aclweb.org/anthology/2020.acl-main.740/">Gururangan et al. (2020)</a> show that adapting to data of the target domain and target task are complementary. Recently, <a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/">Pfeiffer et al. (2020)</a> proposed language-adaptive fine-tuning to adapt a model to new languages.</p><p>An adaptively fine-tuned model is specialised to a particular data distribution, which it will be able to model well. However, this comes at the expense of its ability to be a general model of language. <strong>Adaptive fine-tuning is thus most useful when high performance on (potentially multiple) tasks of a single domain is important</strong> and can be computationally inefficient if a pre-trained model should be adapted to a large number of domains.</p><h2 id="behavioural-fine-tuning">Behavioural fine-tuning</h2><!--kg-card-begin: markdown--><p>While adaptive fine-tuning enables us to specialise our model to $\mathcal{D}_T$, it does not teach us anything directly about the target task. Formally, a target task $\mathcal{T}_T$ consists of a label space $\mathcal{Y}$, a prior distribution $P(Y)$ where $Y = \{y_1, \ldots, y_n \} \in \mathcal{Y}$, and a conditional probability distribution $P(Y | X)$. Alternatively, we can teach a model capabilities useful for doing well on the target task by fine-tuning it on relevant tasks, as can be seen below. We will refer to this setting as <em>behavioural fine-tuning</em> as it focuses on learning useful behaviours and to distinguish it from adaptive fine-tuning.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/02/behavioural_fine-tuning.png" class="kg-image" alt="Recent Advances in Language Model Fine-tuning"><figcaption>Behavioural fine-tuning of a pre-trained model. The pre-trained model is trained with task-specific supervised or self-supervised objectives on tasks that are relevant for the target task.</figcaption></figure><p>One way to teach a model relevant capabilities is to fine-tune it on relevant labelled data of a related task prior to task-specific fine-tuning (<a href="https://arxiv.org/abs/1811.01088">Phang et al., 2018</a>). This so-called intermediate-task training works best with tasks that require high-level inference and reasoning capabilities (<a href="https://www.aclweb.org/anthology/2020.acl-main.467/">Pruksachatkun et al., 2020</a>; <a href="https://www.aclweb.org/anthology/2020.aacl-main.56/">Phang et al., 2020</a>). Behavioural fine-tuning with labelled data has been used to teach a model information about named entities (<a href="https://www.aclweb.org/anthology/K19-1063/">Broscheit, 2019)</a>, paraphrasing (<a href="https://www.aclweb.org/anthology/D19-1542/">Arase and Tsujii, 2019</a>), syntax (<a href="https://arxiv.org/abs/2008.06788">Glavaš and Vulić, 2020</a>), answer sentence selection (<a href="https://arxiv.org/abs/1911.04118">Garg et al., 2020</a>), and question answering (<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.171/">Khashabi et al., 2020)</a>. <a href="https://arxiv.org/abs/2101.11038">Aghajanyan et al. (2021)</a> fine-tune on around 50 labelled datasets in a massively multi-task setting and observe that a large, diverse collection of tasks is important for good transfer performance. </p><p>As supervised data of such high-level reasoning tasks is generally hard to obtain, we can instead train on objectives that teach the model capabilities that are relevant for the downstream task but which can still be learned in a self-supervised manner. For instance, <a href="https://arxiv.org/abs/2101.08231">Dou and Neubig (2021)</a> fine-tune a model for word alignment with an objective that teaches it to identify parallel sentences, among others. <a href="https://www.aclweb.org/anthology/2020.acl-main.704/">Sellam et al. (2020)</a> fine-tune BERT for quality evaluation with a range of sentence similarity signals. In both cases, a diversity of learning signals is important.</p><p>Another effective way is to frame the target task as a form of masked language modelling. To this end, <a href="https://www.aclweb.org/anthology/2020.tacl-1.33/">Ben-David et al. (2020)</a> fine-tune a model for sentiment domain adaptation with a pivot-based objective. Others propose pre-training objectives, which can be used similarly during fine-tuning: <a href="https://arxiv.org/abs/2101.00438">Ram et al. (2021)</a> pre-train a model for QA with a span selection task while <a href="https://www.aclweb.org/anthology/2020.emnlp-main.38/">Bansal et al. (2020)</a> pre-train a model for few-shot learning by automatically generating cloze-style multi-class classification tasks.</p><!--kg-card-begin: markdown--><p><strong>Distinguishing between adaptive and behavioural fine-tuning encourages us to consider the inductive biases we aim to instill in our model and whether they relate to properties of the domain $\mathcal{D}$ or the task $\mathcal{T}$</strong>. Disentangling the role of domain and task is important as information about a domain can often be learned using limited unlabelled data (<a href="https://www.aclweb.org/anthology/2020.coling-main.603/">Ramponi and Plank, 2020</a>) while the acquisition of high-level natural language understanding skills with current methods generally requires billions of pre-training data samples (<a href="https://arxiv.org/abs/2011.04946">Zhang et al., 2020</a>).</p>
<p>The distinction between task and domain becomes fuzzier, however, when we frame tasks in terms of the pre-training objective. A sufficiently general pre-training task such as MLM may provide useful information for learning $P(Y | X)$ but likely does not contain every signal important for the task. For instance, models pre-trained with MLM struggle with modelling negations, numbers, or named entities (<a href="https://www.aclweb.org/anthology/2020.tacl-1.54/">Rogers et al., 2020</a>).</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>Similarly, the use of data augmentation entangles the roles of $\mathcal{D}$ and $\mathcal{T}$ as it allows us to encode the desired capabilities directly in the data. For instance, by fine-tuning a model on text where gendered words are replaced with those of the opposite gender, a model can be made more robust to gender bias (<a href="https://www.aclweb.org/anthology/N18-2003/">Zhao et al., 2018</a>; <a href="https://www.aclweb.org/anthology/N19-1064/">Zhao et al., 2019</a>; <a href="https://arxiv.org/abs/2101.09688">Manela et al., 2021</a>).</p>
<!--kg-card-end: markdown--><h2 id="parameter-efficient-fine-tuning">Parameter-efficient fine-tuning</h2><p>When a model needs to be fine-tuned in many settings such as for a large number of users, it is computationally expensive to store a copy of a fine-tuned model for every scenario. Consequently, recent work has focused on keeping most of the model parameters fixed and fine-tuning a small number of parameters per task. In practice, this enables storing a single copy of a large model and many much smaller files with task-specific modifications.</p><p>The first approaches in this line of work are based on adapters (<a href="https://proceedings.neurips.cc/paper/2017/hash/e7b24b112a44fdd9ee93bdf998c6ca0e-Abstract.html">Rebuffi et al., 2017</a>), small bottleneck layers that are inserted between the layers of a pre-trained model (<a href="http://proceedings.mlr.press/v97/houlsby19a.html">Houlsby et al., 2019</a>; <a href="http://proceedings.mlr.press/v97/stickland19a.html">Stickland and Murray, 2019</a>) whose parameters are fixed. Adapters render common settings such as storing multiple checkpoints during training as well as more advanced techniques such as checkpoint averaging (<a href="https://arxiv.org/abs/1803.05407">Izmailov et al., 2018</a>), snapshot ensembling (<a href="https://openreview.net/forum?id=BJYwwY9ll">Huang et al., 2017</a>) and temporal ensembling (<a href="https://openreview.net/forum?id=BJ6oOfqge">Laine and Aila, 2017</a>) much more space-efficient. Using adapters, a general-purpose model can be efficiently adapted to many settings such as different languages (<a href="https://www.aclweb.org/anthology/D19-1165/">Bapna and Firat, 2019</a>). <a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/">Pfeiffer et al. (2020)</a> recently demonstrated that adapters are modular and can be combined via stacking, which enables learning specialised representations in isolation. This is particularly useful when working with the previously discussed methods: <strong>an adaptively or behaviourally fine-tuned adapter can be evaluated without any task-specific fine-tuning by stacking a trained task adapter on top of it</strong>. This setting can be seen below where a task adapter trained on named entity recognition (NER) is stacked on either an English (left) or Quechua language adapter (right).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/02/modular_adapters.png" class="kg-image" alt="Recent Advances in Language Model Fine-tuning"><figcaption>Task and language adapters inserted in a Transformer block in the MAD-X framework (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/">Pfeiffer et al., 2020</a>). Adapters learn encapsulated representations and can be replaced with each other, enabling zero-shot transfer.</figcaption></figure><!--kg-card-begin: markdown--><p>While adapters modify the model's activations without changing the underlying parameters, another line of work modifies the pre-trained parameters directly. To illustrate this set of methods, we can view fine-tuning as learning how to perturb the parameters of a pre-trained model. Formally, in order to obtain the parameters of a fine-tuned model $\theta_{\text{fine-tuned}} \in \mathbb{R}^D$ where $D$ is the dimensionality of the model, we learn a task-specific parameter vector $\theta_{\text{task}} \in \mathbb{R}^D$ that captures how to change the pre-trained model parameters $\theta_{\text{pre-trained}} \in \mathbb{R}^D$. The fine-tuned parameters are the result of applying the task-specific permutations to the pre-trained parameters:<br>
\begin{equation}<br>
\theta_{\text{fine-tuned}} = \theta_{\text{pre-trained}} + \theta_{\text{task}}<br>
\end{equation}</p>
<p>Instead of storing a copy of $\theta_{\text{fine-tuned}}$ for every task, we can store a single copy of $\theta_{\text{pre-trained}}$ and a copy of $\theta_{\text{task}}$ for every task. This setting is cheaper if we can parameterise $\theta_{\text{task}}$ more efficiently. To this end, <a href="https://arxiv.org/abs/2012.07463">Guo et al. (2020)</a> learn $\theta_{\text{task}}$ as a sparse vector. <a href="https://arxiv.org/abs/2012.13255">Aghajanyan et al. (2020)</a> set $\theta_{\text{task}} = \theta_\text{low} \textbf{M}$ where $\theta_\text{low}$ is a low-dimensional vector and $\textbf{M}$ is a random linear projection (in their case, the FastFood transform (<a href="https://openreview.net/forum?id=ryup8-WCW">Li et al., 2018</a>)).</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>Alternatively, we can apply modifications only to a subset of the pre-trained parameters. A classic method in computer vision (<a href="http://proceedings.mlr.press/v32/donahue14.html">Donahue et al., 2014</a>) fine-tunes only the last layer of the model. Let $\theta_{\text{pre-trained}}$ be the collection of pre-trained parameters across all $L$ layers of the model, i.e. $\theta_{\text{pre-trained}} = \bigcup\limits_{l=1}^{L} \theta_{\text{pre-trained}}^l$ where $\theta_{\text{pre-trained}}^l$ is the parameter vector associated with the $l$-th layer, with analogous notation for $\theta_{\text{fine-tuned}}$ and $\theta_{\text{task}}$. Fine-tuning only the last layer is then equivalent to:<br>
\begin{equation}<br>
\begin{split}<br>
\theta_{\text{fine-tuned}} = &amp; (\bigcup\limits_{l=1}^{L-1} \theta_{\text{pre-trained}}^l) \\<br>
&amp; \cup (\theta_{\text{pre-trained}}^L + \theta_{\text{task}}^L)<br>
\end{split}<br>
\end{equation}</p>
<!--kg-card-end: markdown--><p>While this works less well in NLP (<a href="https://www.aclweb.org/anthology/P18-1031/">Howard &amp; Ruder, 2018</a>), there are other subsets of parameters that are more effective to fine-tune. For instance, <a href="https://nlp.biu.ac.il/~yogo/bitfit.pdf">Ben-Zaken et al. (2020)</a> achieve competitive performance by only fine-tuning a model's bias parameters. </p><p>Another line of work prunes parameters of the pre-trained model during fine-tuning. Such methods use different criteria for pruning weights such as based on zero-th or first-order information about a weight's importance (<a href="https://papers.nips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf">Sanh et al., 2020</a>). As there is limited support of sparse architectures with current hardware, approaches that are <em>structurally</em> sparse, i.e. where updates are concentrated in a limited set of layers, matrices, or vectors are currently preferable. For instance, the last few layers of pre-trained models have been shown to be of limited use during fine-tuning and can be randomly reinitialised (<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.125/">Tamkin et al., 2020</a>; <a href="https://openreview.net/forum?id=cO1IH43yUF">Zhang et al., 2021</a>) or even completely removed (<a href="https://openreview.net/forum?id=xpFFI_NtgpW">Chung et al., 2021</a>).</p><!--kg-card-begin: markdown--><p>While pruning methods focus on reducing the total number of parameters of task-specific models, most of the other methods focus on reducing the number of <em>trainable</em> parameters—while maintaining a copy of $\theta_{\text{pre-trained}}$. The most recent of the latter approaches generally match the performance of full fine-tuning while training around 0.5% of the model's parameters per task (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/">Pfeiffer et al., 2020</a>; <a href="https://arxiv.org/abs/2012.07463">Guo et al., 2020</a>; <a href="https://nlp.biu.ac.il/~yogo/bitfit.pdf">Ben-Zaken et al., 2020</a>).</p>
<!--kg-card-end: markdown--><p>There is increasing evidence that large pre-trained language models learn representations that compress NLP tasks well (<a href="https://openreview.net/forum?id=ryup8-WCW">Li et al., 2018</a>; <a href="https://www.aclweb.org/anthology/2020.repl4nlp-1.18/">Gordon et al., 2020</a>; <a href="https://arxiv.org/abs/2012.13255">Aghajanyan et al., 2020</a>). This practical evidence coupled with their <strong>convenience, availability (<a href="https://www.aclweb.org/anthology/2020.emnlp-demos.7/">Pfeiffer et al., 2020</a>) as well as recent empirical successes make these methods promising both for conducting experiments as well as in practical settings.</strong></p><h2 id="text-to-text-fine-tuning">Text-to-text fine-tuning</h2><p>Another development in transfer learning is a move from masked language models such as BERT (<a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>) and RoBERTa (<a href="https://arxiv.org/abs/1907.11692">Liu et al., 2019</a>) to autoregressive models of language such as T5 (<a href="https://jmlr.org/papers/v21/20-074.html">Raffel et al., 2019</a>) and GPT-3 (<a href="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown et al., 2020</a>). While both sets of methods can be used to assign likelihood scores to text (<a href="https://www.aclweb.org/anthology/2020.acl-main.240/">Salazar et al., 2020</a>), autoregressive LMs are easier to sample from. In contrast, masked LMs are generally restricted to fill-in-the-blank settings, e.g. (<a href="https://www.aclweb.org/anthology/D19-1250/">Petroni et al., 2019</a>).</p><p>The standard way to use masked LMs for fine-tuning is to replace the output layer used for MLM with a randomly initialised task-specific head that is learned on the target task (<a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>). Alternatively, the pre-trained model's output layer can be reused by recasting a task as MLM in a cloze-style format (<a href="https://www.aclweb.org/anthology/2020.tacl-1.48/">Talmor et al., 2020</a>; <a href="https://arxiv.org/abs/2001.07676">Schick and Schütze, 2021</a>). Analogously, autoregressive LMs generally cast the target task in a text-to-text format (<a href="https://arxiv.org/abs/1806.08730">McCann et al., 2018</a>; <a href="https://jmlr.org/papers/v21/20-074.html">Raffel et al., 2020</a>; <a href="https://openreview.net/forum?id=US-TP-xnXI">Paolini et al., 2021</a>). In both settings, <strong>the models are able to benefit from all their pre-trained knowledge and do not need to learn any new parameters from scratch, which improves their sample efficiency.</strong></p><p>In the extreme when no parameters are fine-tuned, framing a target task in terms of the pre-training objective enables zero-shot or few-shot learning using a task-specific prompt and a small number of examples of a task (<a href="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown et al., 2020</a>). However, while such few-shot learning is possible, it is not the most effective way to use such models (<a href="https://arxiv.org/abs/2009.07118">Schick and Schütze, 2020</a>; see <a href="https://ruder.io/research-highlights-2020/#3-few-shot-learning">this post</a> for a brief overview). Learning without updates requires a huge model as the model needs to rely entirely on its existing knowledge. The amount of information available to the model is also restricted by its context window and the prompts shown to the model need to be carefully engineered.</p><p>Retrieval augmentation (see <a href="https://ruder.io/research-highlights-2020/#2-retrieval-augmentation">this post</a> for an overview) can be used to off-load the storage of external knowledge and symbolic approaches could be used to teach a model task-specific rules akin to (<a href="https://openreview.net/forum?id=SkeuexBtDr">Awasthi et al., 2020</a>). Pre-trained models will also become larger and more powerful and may be behaviourally fine-tuned to be good at the zero-shot setting. However, without fine-tuning a model is ultimately limited in its ability to adapt to a new task.</p><p>Consequently, <strong>for most practical settings the best path forward arguably is to fine-tune all or a subset of the model's parameters using the methods described in the previous sections.</strong> <strong>In addition, we will increasingly see an emphasis of pre-trained models' generative capabilities.</strong> While current methods generally focus on modifying a model's natural language input such as via automatic prompt design (<a href="https://arxiv.org/abs/2009.07118">Schick and Schütze, 2020</a>; <a href="https://arxiv.org/abs/2012.15723">Gao et al., 2020</a>; <a href="https://arxiv.org/abs/2010.15980">Shin et al., 2020</a>), the most effective way to modulate the output of such models will likely act directly on their hidden representations (<a href="https://openreview.net/forum?id=H1edEyBKDS">Dathathri et al., 2020</a>; see <a href="https://lilianweng.github.io/lil-log/2021/01/02/controllable-neural-text-generation.html">Lillian Weng's post</a> for an overview of methods for controllable generation). </p><h2 id="mitigating-fine-tuning-instabilities">Mitigating fine-tuning instabilities</h2><p>A practical problem with fine-tuning pre-trained models is that performance can vary drastically between different runs, particularly on small datasets (<a href="https://arxiv.org/abs/1811.01088">Phang et al., 2018</a>). <a href="https://arxiv.org/abs/2002.06305">Dodge et al., 2020</a> find that both the weight initialisation of the output layer and the order of the training data contribute to variation in performance. As instabilities are generally apparent early in training, they recommend stopping the least promising runs early after 20-30% of training. <a href="https://openreview.net/forum?id=nzpLWnVAyah">Mosbach et al. (2021</a>) additionally recommend using small learning rates and to increase the number of epochs when fine-tuning BERT.</p><p>A number of recent methods seek to mitigate instabilities during fine-tuning by relying on adversarial or trust region-based approaches (<a href="https://openreview.net/forum?id=BygzbyHFvB">Zhu et al., 2019</a>; <a href="https://www.aclweb.org/anthology/2020.acl-main.197/">Jiang et al., 2020</a>; <a href="https://openreview.net/forum?id=OQ08SN70M1V">Aghajanyan et al., 2021</a>). Such methods generally augment the fine-tuning loss with a regularisation term that bounds the divergence between update steps.</p><p>In light of the previous section, we can make another recommendation for minimising instabilities during fine-tuning: <strong>Avoid using a randomly initialised output layer on the target task for small datasets by framing the target task as a form of LM or use behavioural fine-tuning to fine-tune the output layer prior to task-specific fine-tuning.</strong> While text-to-text models are thus more robust to fine-tuning on small datasets, they suffer from instabilities in the few-shot setting and are sensitive to the prompt and few-shot examples (<a href="https://arxiv.org/abs/2102.09690">Zhao et al., 2021</a>).</p><p>Overall, as models are increasingly applied to challenging tasks with fewer training examples, it is crucial to develop methods that are robust to possible variations and that can be reliably fine-tuned.</p><h2 id="citation">Citation</h2><p>For attribution in academic contexts, please cite this work as:</p><pre><code>@misc{ruder2021lmfine-tuning,
  author = {Ruder, Sebastian},
  title = {{Recent Advances in Language Model Fine-tuning}},
  year = {2021},
  howpublished = {\url{http://ruder.io/recent-advances-lm-fine-tuning}},
}</code></pre>]]></content:encoded></item><item><title><![CDATA[ML and NLP Research Highlights of 2020]]></title><description><![CDATA[This post summarizes progress in 10 exciting and impactful directions in ML and NLP in 2020.]]></description><link>http://ruder.io/research-highlights-2020/</link><guid isPermaLink="false">5fcb79c34be0c26392ea1525</guid><category><![CDATA[transfer learning]]></category><category><![CDATA[natural language processing]]></category><category><![CDATA[language models]]></category><category><![CDATA[reinforcement learning]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Tue, 19 Jan 2021 09:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2021/01/lra_analysis-2.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2021/01/lra_analysis-2.png" alt="ML and NLP Research Highlights of 2020"><p>The selection of areas and methods is heavily influenced by my own interests; the selected topics are biased towards representation and transfer learning and towards natural language processing (NLP). I tried to cover the papers that I was aware of but likely missed many relevant ones—feel free to highlight them in the comments below. In all, I discuss the following highlights:</p><ol><li><a href="#1-scaling-up-and-down">Scaling up—and down</a></li><li><a href="#2-retrieval-augmentation">Retrieval augmentation</a></li><li><a href="#3-few-shot-learning">Few-shot learning</a></li><li><a href="#4-contrastive-learning">Contrastive learning</a></li><li><a href="#5-evaluation-beyond-accuracy">Evaluation beyond accuracy</a></li><li><a href="#6-practical-concerns-of-large-lms">Practical concerns of large LMs</a></li><li><a href="#7-multilinguality">Multilinguality</a></li><li><a href="#8-image-transformers">Image Transformers</a></li><li><a href="#9-ml-for-science">ML for science</a></li><li><a href="#10-reinforcement-learning">Reinforcement learning</a></li></ol><h1 id="1-scaling-up-and-down">1) Scaling up—and down</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/large_models.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>Model sizes of language models from 2018–2020 (Credit: <a href="https://www.stateof.ai/">State of AI Report 2020</a>)</figcaption></figure><p><strong>What happened?</strong>  2020 saw the development of ever larger language and dialogue models such as Meena (<a href="https://arxiv.org/abs/2001.09977">Adiwardana et al., 2020</a>), <a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">Turing-NLG</a>, BST (<a href="https://arxiv.org/abs/2004.13637">Roller et al., 2020</a>), and GPT-3 (<a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown et al., 2020</a>). At the same time, researchers have become more aware of how expensive and energy-hungry these models can be (<a href="https://www.aclweb.org/anthology/P19-1355/">Strubell et al., 2019</a>) and work that focuses on making them smaller has gained momentum: Recent approaches rely on pruning (<a href="https://arxiv.org/abs/2004.03844">Sajjad et al., 2020</a>; <a href="https://openreview.net/forum?id=SylO2yStDr">Fan et al., 2020a</a>; <a href="https://papers.nips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf">Sanh et al., 2020</a>), quantization (<a href="https://openreview.net/forum?id=dV19Yyi1fS3">Fan et al., 2020b</a>), distillation (<a href="https://arxiv.org/abs/1910.01108">Sanh et al., 2019</a>; <a href="https://www.aclweb.org/anthology/2020.acl-main.195/">Sun et al., 2020</a>), and compression (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.633/">Xu et al., 2020</a>). Other approaches focused on making the Transformer architecture itself more efficient. Models in this line include the Performer (<a href="https://openreview.net/forum?id=Ua6zuk0WRH">Choromanski et al., 2020</a>) and Big Bird (<a href="https://papers.nips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf">Zaheer et al., 2020</a>), which can be seen in the cover image above. The image shows performance (y axis), speed (x axis) and memory footprint (circle size) of different models on the Long Range Arena benchmark (<a href="https://openreview.net/forum?id=qVyeW-grC2k">Tay et al., 2020</a>). </p><p>Tools such as the experiment-impact-tracker (<a href="https://arxiv.org/abs/2002.05651">Henderson et al., 2020</a>) have made it easier to track the energy efficiency of models. They have also facilitated competitions and benchmarks that evaluate models primarily based on their efficiency such as the <a href="https://sites.google.com/view/sustainlp2020/home?utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">SustaiNLP workshop</a> at EMNLP 2020, the <a href="https://ai.google.com/research/NaturalQuestions/efficientqa?utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">Efficient QA competition</a> at NeurIPS 2020, and HULK (<a href="https://arxiv.org/abs/2002.05829">Zhou et al., 2020</a>).</p><p><strong>Why is it important?</strong>  Scaling up models allows us to keep pushing the boundaries of what current models can do. In order to deploy and use them in real-world scenarios, however, they need to be efficient. Ultimately, both directions benefit each other: Compressing large models yields efficient models with strong performance (<a href="https://arxiv.org/abs/2002.11794">Li et al., 2020</a>) while more efficient methods may lead to stronger, larger models (<a href="https://arxiv.org/abs/2003.10555">Clark et al., 2020</a>).</p><p><strong>What's next?</strong>  I am hopeful that—in light of the increasing interest in efficiency and the availability of tools—it will become more common not only to report a model's performance and number of parameters but also its energy efficiency. This should contribute to a more holistic evaluation that may help to bridge the gap to real-world ML use cases.</p><h1 id="2-retrieval-augmentation">2) Retrieval augmentation</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/retrieval_augmented_lm.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>Unsupervised pre-training with REALM (<a href="https://arxiv.org/abs/2002.08909">Guu et al., 2020</a>); retriever and encoder are jointly pre-trained</figcaption></figure><p><strong>What happened?</strong>  Large models have been shown to have learned a surprising amount of world knowledge from their pre-training data, which allows them to reproduce facts (<a href="https://www.aclweb.org/anthology/2020.tacl-1.28.pdf">Jiang et al., 2020</a>) and answer questions even without access to external context (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.437.pdf">Roberts et al., 2020</a>). However, storing such knowledge implicitly in the parameters of a model is inefficient and requires ever larger models to retain more information. Instead, recent approaches jointly trained retrieval models and large language models, which led to strong results on knowledge-intensive NLP tasks such as open-domain question answering (<a href="https://arxiv.org/abs/2002.08909">Guu et al., 2020</a>; <a href="https://arxiv.org/abs/2005.11401">Lewis et al., 2020</a>) and language modelling (<a href="https://openreview.net/forum?id=HklBjCEKvH">Khandelwal et al., 2020</a>). The main advantage of these methods is that they integrate retrieval directly into language model pre-training, which allows language models to be much more efficient by being able to off-load the recall of facts and focus on learning the more challenging aspects of natural language understanding. Consequently, the best systems in the NeurIPS 2020 EfficientQA competition (<a href="https://arxiv.org/abs/2101.00133">Min et al., 2020</a>) all relied on retrieval.</p><p><strong>Why is it important?</strong>  Retrieval was the standard in many generative tasks, such as text summarization or dialogue and has largely been superseded by abstractive generation (<a href="https://arxiv.org/abs/1707.02268">Allahyari et al., 2017</a>). Retrieval-augmented generation enables combining the best of both worlds: the factual correctness and faithfulness of retrieved segments and the relevancy and composition of generated text.</p><p><strong>What's next?</strong>  Retrieval-augmented generation should be particularly useful for dealing with failure cases that have plagued generative neural models in the past, such as dealing with hallucinations (<a href="https://www.aclweb.org/anthology/P19-1256/">Nie et al., 2019</a>). It may also help make systems more interpretable by directly providing evidence for their prediction.</p><h1 id="3-few-shot-learning">3) Few-shot learning</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/prompt-based_fine-tuning.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>Prompt-based fine-tuning uses templated prompts and demonstrations (<a href="https://arxiv.org/abs/2012.15723">Gao et al., 2020</a>)</figcaption></figure><p><strong>What happened?</strong>  Over the last years, driven by advances in pre-training, the number of training examples to perform a given task has progressively gone down (<a href="https://www.aclweb.org/anthology/N18-1202/">Peters et al., 2018</a>; <a href="https://www.aclweb.org/anthology/P18-1031/">Howard et al., 2018</a>). We are now at a stage where tens of examples can be used to demonstrate a given task (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.38/">Bansal et al., 2020</a>). A very natural paradigm for few-shot learning is to reframe a task as language modelling. The most prominent instantiation of this, the in-context learning approach of GPT-3 (<a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown et al., 2020</a>) performs a prediction based on a few demonstrations of input–output pairs in the model's context and a prompt without any gradient updates. This setting, however, has a few limitations: It requires a huge model—without any updates the model needs to rely on its existing knowledge—, the amount of knowledge that the model can use is restricted by its context window, and prompts need to be hand-engineered.</p><p>Recent work has sought to make such few-shot learning more effective by using a smaller model, integrating fine-tuning, and automatically generating natural language prompts (<a href="https://arxiv.org/abs/2009.07118">Schick and Schütze, 2020</a>; <a href="https://arxiv.org/abs/2012.15723">Gao et al., 2020</a>; <a href="https://arxiv.org/abs/2010.15980">Shin et al., 2020</a>). Such work is closely related to the broader area of controllable neural text generation, which broadly seeks to leverage the generative capabilities of powerful pre-trained models. For an excellent overview, check out <a href="https://lilianweng.github.io/lil-log/2021/01/02/controllable-neural-text-generation.html">Lilian Weng's blog post</a>.</p><p>Few-shot learning enables rapid adaptation of a model to many tasks. However, updating all model parameters for each task is wasteful. Instead, it is preferable to perform localized updates that concentrate changes in a small set of parameters. There have been a few approaches that make such efficient fine-tuning more practical including using adapters (<a href="http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf">Houlsby et al., 2019</a>; <a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/">Pfeiffer et al., 2020a</a>,<a href="https://www.aclweb.org/anthology/2020.emnlp-demos.7/">b</a>; <a href="https://www.aclweb.org/anthology/2020.emnlp-main.180.pdf">Üstün et al., 2020</a>), adding a sparse parameter vector (<a href="https://arxiv.org/abs/2012.07463">Guo et al., 2020</a>), and only modifying bias values (<a href="https://nlp.biu.ac.il/~yogo/bitfit.pdf">Ben-Zaken et al., 2020</a>).</p><p><strong>Why is it important?</strong>  Being able to teach a model a task based on only a few examples greatly reduces the barrier to entry for applying ML and NLP models in practice. This opens up applications where data is very expensive to collect and enables adapting models swiftly to new domains.</p><p><strong>What's next?</strong>  For many real-world scenarios, it is possible to collect thousands of training examples. Models should thus be able to scale seamlessly from learning from a few to learning from thousands of examples and should not be limited by e.g. their context length. Given that models have achieved super-human performance on many popular tasks such as <a href="https://super.gluebenchmark.com/leaderboard">SuperGLUE</a> when fine-tuned on entire training datasets, enhancing their few-shot performance is a natural area for improvement.</p><h1 id="4-contrastive-learning">4) Contrastive learning</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/instance_discrimination.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>Instance discrimination compares features from different transformations of the same images to each other (<a href="https://proceedings.neurips.cc/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf">Caron et al., 2020</a>)</figcaption></figure><p><strong>What happened?</strong>  Contrastive learning—learning to differentiate a positive example from negative samples, often from a noise distribution—such as using negative sampling or noise contrastive estimation is a staple of representation learning and self-supervised learning and a prominent part of classic approaches such as word2vec (<a href="https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Mikolov et al., 2013</a>). More recently, contrastive learning gained popularity in self-supervised representation learning in computer vision and speech (<a href="https://arxiv.org/abs/1807.03748">van den Oord, 2018</a>; <a href="https://arxiv.org/abs/1905.09272">Hénaff et al., 2019</a>). The recent generation of increasingly powerful self-supervised approaches for visual representation learning rely on contrastive learning using an instance discrimination task: different images are treated as negative pairs and views of the same image are treated as positive pairs. Recent approaches have further refined this general framework: SimCLR (<a href="http://proceedings.mlr.press/v119/chen20j/chen20j.pdf">Chen et al., 2020</a>) defines the contrastive loss over augmented examples, Momentum Contrast (<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf">He et al., 2020</a>) seeks to ensure a large and consistent set of pairs, SwAV (<a href="https://proceedings.neurips.cc/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf">Caron et al., 2020</a>) leverages online clustering, and BYOL only employs positive pairs (<a href="https://arxiv.org/abs/2006.07733">Grill et al., 2020</a>). <a href="https://arxiv.org/abs/2011.10566">Chen and He (2020</a>) have furthermore proposed a simpler formulation that relates to the previous methods.</p><p>Recently, <a href="https://openreview.net/forum?id=tC6iW2UUbJf">Zhao et al. (2020)</a> find that data augmentation is essential for contrastive learning. This might indicate why <em>unsupervised</em> contrastive learning has not been successful with large pre-trained models in NLP where data augmentation is less common. They also hypothesize that the reason instance discrimination may work better than supervised pre-training in computer vision is that it does not try to make the features of all instances from a class similar but retains the information from each instance. This is less of a problem in NLP where unsupervised pre-training involves classification over thousands of word types. In NLP, <a href="https://openreview.net/forum?id=cu7IUiOhujH">Gunel et al. (2020)</a> recently employ contrastive learning for <em>supervised</em> fine-tuning. </p><p><strong>Why is it important?</strong>  The cross-entropy objective between one-hot labels and a model's output logits commonly used in language modelling has several limitations such as generalizing poorly to imbalanced classes (<a href="http://papers.neurips.cc/paper/8435-learning-imbalanced-datasets-with-label-distribution-aware-margin-loss.pdf">Cao et al., 2019</a>). Contrastive learning is an alternative, complementary paradigm that may help ameliorate some of these deficits.</p><p><strong>What's next?</strong>  Contrastive learning combined with masked language modelling may enable us to learn representations that are richer and more robust. It could help model outliers and rare syntactic and semantic phenomena, which are a challenge for current NLP models.</p><h1 id="5-evaluation-beyond-accuracy">5) Evaluation beyond accuracy</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/checklist_negation.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>A CheckList template and tests probing for an understanding of negation in sentiment analysis (<a href="https://www.aclweb.org/anthology/2020.acl-main.442/">Ribeiro et al., 2020</a>)</figcaption></figure><p><strong>What happened?</strong>  State-of-the-art models in NLP have achieved superhuman performance across many tasks. Whether or not we believe that such models can achieve true natural language understanding (<a href="https://arxiv.org/abs/1901.11373">Yogatama et al., 2019</a>; <a href="https://www.aclweb.org/anthology/2020.acl-main.463/">Bender and Koller, 2020</a>), we know that current models are not close to this elusive goal. However, the simple performance metrics of our tasks fail to capture the limitations of existing models. There are two key themes in this area: a) curating examples that are difficult for current models; and b) going beyond simple metrics such as accuracy towards more fine-grained evaluation.</p><p>Regarding the former, the common methodology is to use adversarial filtering (<a href="https://www.aclweb.org/anthology/D18-1009/">Zellers et al., 2018</a>) during dataset creation to filter out examples that are predicted correctly by current models. Recent work proposes more efficient adversarial filtering methods (<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6399/6255">Sakaguchi et al., 2020</a>; <a href="http://proceedings.mlr.press/v119/bras20a/bras20a.pdf">Le Bras et al., 2020</a>) and an iterative dataset creation process (<a href="https://www.aclweb.org/anthology/2020.acl-main.441/">Nie et al., 2020</a>; <a href="https://transacl.org/ojs/index.php/tacl/article/view/2129/649">Bartolo et al., 2020</a>) where examples are filtered and models are re-trained over multiple rounds. A subset of such evolving benchmarks are available in <a href="https://dynabench.org/">Dynabench</a>.</p><p>The methods that regard the second point are similar in spirit. However, rather than creating examples that target a specific model, examples are used to probe for phenomena common to a task of interest. Commonly, minimal pairs—also known as counterfactual examples or contrast sets—(<a href="https://openreview.net/forum?id=Sklgs0NFvr">Kaushik et al., 2020</a>;  <a href="https://arxiv.org/abs/2004.02709">Gardner et al., 2020</a>; <a href="https://transacl.org/ojs/index.php/tacl/article/view/2013/527">Warstadt et al., 2020</a>) are created, which perturb examples in a minimal way and often change the gold label. <a href="https://www.aclweb.org/anthology/2020.acl-main.442/">Ribeiro et al. (2020)</a> formalized some of the underlying intuitions in their CheckList framework, which enables the semi-automatic creation of such test cases. Alternatively, examples can be characterized based on different attributes, which allow a more fine-grained analysis of a model's strengths and weaknesses (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.489/">Fu et al., 2020</a>).</p><p><strong>Why is it important?</strong>  In order to make meaningful progress towards building more capable models machine learning models, we need to understand not only if a model outperforms a previous system but what kind of errors it makes and which phenomena it fails to capture.</p><p><strong>What's next?</strong>  By providing fine-grained diagnostics of model behaviour, it will be easier to identify a model's deficiencies and propose improvements that address them. Similarly, a fine-grained evaluation would allow a more nuanced comparison of the strengths and weaknesses of different methods.</p><h1 id="6-practical-concerns-of-large-lms">6) Practical concerns of large LMs</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/real_toxicity_prompts.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>Models generate toxic content based on seemingly innocuous prompts (<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.301/">Gehman et al., 2020</a>)</figcaption></figure><p><strong>What happened?</strong>  <a href="https://ruder.io/research-highlights-2019/#10-more-reliable-analysis-methods">Compared to 2019</a> where the analysis of language models (LMs) mainly focused on the syntactic, semantic, and world knowledge that such models capture—see <a href="https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00349">(Rogers et al., 2020)</a> for a great overview—recent analyses revealed a number of practical concerns. Pre-trained language models were found to be prone to generating toxic language (<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.301/">Gehman et al., 2020</a>) and leak information (<a href="https://arxiv.org/abs/2004.00053">Song &amp; Raghunathan, 2020</a>), to be susceptible to backdoors after fine-tuning, which let an attacker manipulate the model prediction (<a href="https://www.aclweb.org/anthology/2020.acl-main.249/">Kurita et al., 2020</a>; <a href="https://arxiv.org/abs/2010.12563">Wallace et al., 2020</a>), and to be vulnerable to model and data extraction attacks (<a href="https://openreview.net/forum?id=Byl5NREFDr">Krishna et al., 2020</a>; <a href="https://arxiv.org/abs/2012.07805">Carlini et al., 2020</a>). In addition, pre-trained models are well known to capture biases with regard to protected attributes such as gender (<a href="https://arxiv.org/abs/1607.06520">Bolukbasi et al., 2016</a>; <a href="https://arxiv.org/abs/2010.06032">Webster et al., 2020</a>)—see <a href="https://www.aclweb.org/anthology/P19-1159/">(Sun et al., 2019)</a> for an excellent survey on mitigating gender bias.</p><p><strong>Why is it important?</strong>  Large pre-trained models are trained by many institutions and are actively deployed in real-world scenarios. It is thus of practical importance that we are not only aware of their biases but what behaviour may have actually harmful consequences.</p><p><strong>What's next?</strong>  As larger and more powerful models are developed, it is important that such practical concerns as well as issues around bias and fairness are part of the development process from the start.  </p><h1 id="7-multilinguality">7) Multilinguality</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/language_resource_distribution.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>The unequal distribution of labeled and unlabeled data for languages around the world (<a href="https://www.aclweb.org/anthology/2020.acl-main.560/">Joshi et al., 2020</a>)</figcaption></figure><p><strong>What happened?</strong>  2020 had many highlights in multilingual NLP. The <a href="https://www.masakhane.io/">Masakhane</a> organisation whose mission is to strengthen NLP for African languages gave the <a href="https://www.youtube.com/watch?v=Xbc_g_OknqA">keynote</a> at the <a href="http://www.statmt.org/wmt20/">Fifth Conference on Machine Translation (WMT20)</a>, one of the most inspiring presentations of the last year. New general-purpose benchmarks for other languages emerged including XTREME (<a href="http://proceedings.mlr.press/v119/hu20b/hu20b.pdf">Hu et al., 2020</a>), XGLUE (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.484/">Liang et al., 2020</a>), IndoNLU (<a href="https://www.aclweb.org/anthology/2020.aacl-main.85/">Wilie et al., 2020</a>), IndicGLUE (<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.445/">Kakwani et al., 2020</a>). Existing datasets that were replicated in other languages—together with their non-English variants—include:</p><ul><li>SQuAD: XQuAD (<a href="https://www.aclweb.org/anthology/2020.acl-main.421/">Artetxe et al., 2020</a>), MLQA (<a href="https://www.aclweb.org/anthology/2020.acl-main.653/">Lewis et al., 2020</a>), FQuAD (<a href="https://arxiv.org/abs/2002.06071">d'Hoffschmidt et al., 2020</a>);</li><li>Natural Questions: TyDiQA (<a href="https://arxiv.org/abs/2003.05002">Clark et al., 2020</a>), MKQA (<a href="https://arxiv.org/abs/2007.15207">Longpre et al., 2020</a>);</li><li>MNLI: OCNLI (<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.314/">Hu et al., 2020</a>), FarsTail (<a href=" FarsTail: A Persian Natural Language Inference Dataset">Amirkhani et al., 2020</a>);</li><li>the CoNLL-09 dataset: X-SRL (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.321/">Daza and Frank, 2020</a>); and</li><li>the CNN/Daily Mail dataset: MLSUM (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.647/">Scialom et al., 2020</a>).</li></ul><p>Many of these datasets and many others in different languages are easily accessible via <a href="https://huggingface.co/datasets">Hugging Face datasets</a>. Powerful multilingual models that cover around 100 languages emerged including XML-R (<a href="https://www.aclweb.org/anthology/2020.acl-main.747/">Conneau et al., 2020</a>), RemBERT (<a href="https://openreview.net/forum?id=xpFFI_NtgpW">Chung et al., 2020</a>), InfoXLM (<a href="https://arxiv.org/abs/2007.07834">Chi et al., 2020</a>), and others (see the <a href="https://sites.research.google/xtreme">XTREME leaderboard</a> for an overview). A plethora of language-specific BERT models have been trained for languages beyond English such as AraBERT (<a href="https://www.aclweb.org/anthology/2020.osact-1.2/">Antoun et al., 2020</a>) and IndoBERT (<a href="https://www.aclweb.org/anthology/2020.aacl-main.85/">Wilie et al., 2020</a>); see (<a href="https://arxiv.org/abs/2003.02912">Nozza et al., 2020</a>; <a href="https://arxiv.org/abs/2012.15613">Rust et al., 2020</a>) for an overview. With efficient multilingual frameworks such as <a href="https://adapterhub.ml/">AdapterHub</a> (<a href="https://www.aclweb.org/anthology/2020.emnlp-demos.7/">Pfeiffer et al., 2020</a>), <a href="https://stanfordnlp.github.io/stanza/">Stanza</a> (<a href="https://www.aclweb.org/anthology/2020.acl-demos.14/">Qi et al., 2020</a>) and <a href="https://github.com/nlp-uoregon/trankit">Trankit</a> (<a href="https://arxiv.org/abs/2101.03289">Nguyen et al., 2020</a>) it has become easier than ever to apply and build models for many of the world's languages.</p><p>Finally, two position papers that inspired much of my thinking in this area this year are <em>The State and Fate of Linguistic Diversity and Inclusion in the NLP World</em> <a href="https://www.aclweb.org/anthology/2020.acl-main.560/">(Joshi et al., 2020)</a> and <em>Decolonising Speech and Language Technology</em> (<a href="https://www.aclweb.org/anthology/2020.coling-main.313/">Bird, 2020</a>). While the first highlights the urgent importance of working on languages beyond English, the second one cautions against treating language communities and their data as a commodity.</p><p><strong>Why is it important?</strong>  Working on NLP beyond English <a href="https://ruder.io/nlp-beyond-english/">has numerous benefits</a>: It poses interesting challenges for ML and NLP and enables having a large impact on society, among many others.</p><p><strong>What's next?</strong>  Given the availability of data and models in different languages, the stage is set to make meaningful progress on languages beyond English. I am most excited about developing models that tackle the most challenging settings and identifying in which cases the assumptions that underlie our current models fail.</p><h1 id="8-image-transformers">8) Image Transformers</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/vision_transformer.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>The Vision Transformer (<a href="https://openreview.net/forum?id=YicbFdNTTy">Dosovitskiy et al., 2020</a>) applies a Transformer encoder to flattened image patches</figcaption></figure><p><strong>What happened?</strong>  While Transformers have achieved large success in NLP, they were—up until recently—less successful in computer vision where convolutional neural networks (CNNs) still reigned supreme. While models early in the year such as DETR (<a href="https://arxiv.org/abs/2005.12872">Carion et al., 2020</a>) employed a CNN to compute image features, later models were completely convolution-free. Image GPT (<a href="http://proceedings.mlr.press/v119/chen20s/chen20s.pdf">Chen et al., 2020</a>) applied the GPT-2 recipe to pre-training directly from pixels and outperforms a supervised Wide ResNet. Later models all reshape an image into patches that are treated as "tokens". Vision Transformer (<a href="https://openreview.net/forum?id=YicbFdNTTy">Dosovitskiy et al., 2020</a>) is pre-trained on millions of labelled images—each consisting of such patches—outperforming state-of-the-art CNNs. The Image Processing Transformer (<a href="https://arxiv.org/abs/2012.00364">Chen et al., 2020</a>) pre-trains on corrupted ImageNet examples with a contrastive loss and achieves state-of-the-art performance on low-level image tasks. The Data-efficient image Transformer (<a href="https://arxiv.org/abs/2012.12877">Touvron et al., 2020</a>) is pre-trained on ImageNet via distillation. Interestingly, they observe that CNNs are better teachers. This is similar to findings for distilling an inductive bias into BERT (<a href="https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00345">Kuncoro et al., 2020</a>). In contrast in speech, Transformers have not been applied directly to the audio signal—to my knowledge—but typically receive the output of an encoder such as a CNN as input (<a href="https://arxiv.org/abs/2001.02674">Moritz et al., 2020</a>; <a href="https://arxiv.org/abs/2005.08100">Gulati et al., 2020</a>; <a href="https://arxiv.org/abs/2006.13979">Conneau et al., 2020</a>)</p><p><strong>Why is it important?</strong>  Transformers have less inductive bias compared to CNNs and RNNs. While being less theoretically powerful than RNNs (<a href="https://www.aclweb.org/anthology/P18-2117/">Weiss et al., 2018</a>; <a href="https://www.aclweb.org/anthology/2020.tacl-1.11/">Hahn et al., 2020</a>), given sufficient data and scale Transformers have been shown to eventually outperform their inductively biased competitors (cf. <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</a>).</p><p><strong>What's next?</strong>  We will likely see Transformers become more popular in computer vision. They will be applied particularly in scenarios where enough compute and data for unsupervised pre-training is available. In smaller scale settings, CNNs will likely still be the go-to approach and a strong baseline. </p><h1 id="9-ml-for-science">9) ML for science</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/alphafold.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>The self-attention-based architecture of AlphaFold (Credit: <a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">DeepMind blog</a>)</figcaption></figure><p><strong>What happened?</strong>  One of the highlights was <a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">AlphaFold</a> demonstrating ground-breaking performance in the biannual CASP challenge for protein folding. Beyond that, there have been several other notable developments in applying ML to problems in the natural sciences. MetNet (<a href="https://arxiv.org/abs/2003.12140">Sønderby et al., 2020</a>) outperformed numerical weather prediction for precipitation forecasting, <a href="https://openreview.net/forum?id=S1eZYeHFDS">Lample and Charton (2020)</a> solved differential equations using neural networks better than commercial computer algebra systems, and <a href="https://www.nature.com/articles/s41586-020-2939-8">Bellemare et al. (2020)</a> used reinforcement learning to navigate balloons in the stratosphere. </p><p>In addition, ML has been used extensively to help with the ongoing COVID-19 pandemic, e.g. to forecast COVID-19 spread (<a href="https://arxiv.org/abs/2007.03113">Kapoor et al., 2020</a>), <a href="https://deepmind.com/research/open-source/computational-predictions-of-protein-structures-associated-with-COVID-19">predict structures associated with COVID-19</a>, translate relevant data into 35 different languages (<a href="https://www.aclweb.org/anthology/2020.nlpcovid19-2.5/">Anastasopoulos et al., 2020</a>), and answer questions about COVID-19 in real-time (<a href="https://www.aclweb.org/anthology/2020.nlpcovid19-2.1/">Lee et al., 2020</a>). For an overview of COVID-19 related applications of NLP, check out the <a href="https://www.aclweb.org/anthology/volumes/2020.nlpcovid19-2/">Proceedings of the 1st Workshop on NLP for COVID-19</a>.</p><p><strong>Why is it important?</strong>  The natural sciences are arguably the most impactful application area for ML. Improvements touch many aspects of life and can have a profound impact on the world.</p><p><strong>What's next?</strong>  With progress in areas as central as protein folding, the speed of application of ML to the natural sciences will only accelerate. I am looking forward to many more fundamental advances that have a positive impact in the world.</p><h1 id="10-reinforcement-learning">10) Reinforcement learning</h1><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2021/01/atari_performance.png" class="kg-image" alt="ML and NLP Research Highlights of 2020"><figcaption>Performance of Agent57 and MuZero on Atari compared to state-of-the-art agents in terms of the number of games where they outperform the human benchmark throughout training (<a href="https://arxiv.org/abs/2003.13350">Badia et al., 2020</a>)</figcaption></figure><p><strong>What happened?</strong>  For the first time, a single deep RL agent—Agent57 (<a href="https://arxiv.org/abs/2003.13350">Badia et al., 2020</a>)—has achieved superhuman performance on all 57 Atari games, a long-standing benchmark in the deep reinforcement learning literature. The agent's versatility comes from a neural network that allows it to switch between exploratory and exploitative policies. Another milestone was the development of MuZero (<a href="https://www.nature.com/articles/s41586-020-03051-4">Schrittwieser et al., 2020</a>), which predicts the aspects of the environment that are most important for accurate planning. Without any knowledge of the game dynamics, it achieved state-of-the-art performance on Atari as well as superhuman performance on Go, chess, and shogi. Finally, Munchausen RL agents (<a href="https://papers.nips.cc/paper/2020/file/2c6a0bae0f071cbbf0bb3d5b11d90a82-Supplemental.pdf">Vieillard et al., 2020</a>) improved on state-of-the-art agents via a simple, theoretically founded modification.</p><p><strong>Why is it important?</strong>  Reinforcement learning algorithms have a multitude of practical implications (<a href="https://www.nature.com/articles/s41586-020-2939-8">Bellemare et al., 2020</a>). Improvements over the fundamental algorithms in this area can have a large practical impact by enabling better planning, environment modelling, and action prediction.</p><p><strong>What's next?</strong>  With classic benchmarks such as Atari essentially solved, researchers may look to more challenging settings to test their algorithms such as generalizing to out-of-distribution tasks, improving sample-efficiency, multi-task learning, etc. </p><h2 id="citation">Citation</h2><p>For attribution in academic contexts, please cite this work as:</p><pre><code>@misc{ruder2021researchhighlights,
  author = {Ruder, Sebastian},
  title = {{ML and NLP Research Highlights of 2020}},
  year = {2021},
  howpublished = {\url{http://ruder.io/research-highlights-2020}},
}</code></pre><p><em>Thanks to Sameer Singh whose <a href="https://twitter.com/sameer_/status/1347472491346763776">Twitter thread</a> reviewing NLP research in 2020 provided inspiration for this post.</em></p>]]></content:encoded></item><item><title><![CDATA[Why You Should Do NLP Beyond English]]></title><description><![CDATA[7000+ languages are spoken around the world but NLP research has mostly focused on English. This post outlines why you should work on languages other than English.]]></description><link>http://ruder.io/nlp-beyond-english/</link><guid isPermaLink="false">5ea9dc7bc4820fecc309c25c</guid><category><![CDATA[cross-lingual]]></category><category><![CDATA[natural language processing]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Sat, 01 Aug 2020 00:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2020/07/langscape-1.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2020/07/langscape-1.png" alt="Why You Should Do NLP Beyond English"><p>Natural language processing (NLP) research predominantly focuses on developing methods that work well for English despite the many positive benefits of working on other languages. These benefits range from an outsized societal impact to modelling a wealth of linguistic features to avoiding overfitting as well as interesting challenges for machine learning (ML).</p><p>There are<a href="https://www.ethnologue.com/"> around 7,000 languages</a> spoken around the world. The map above (see the interactive version at<a href="http://langscape.umd.edu/map.php"> Langscape</a>) gives an overview of languages spoken around the world, with each green circle representing a native language. Most of the world's languages are <a href="https://www.ethnologue.com/guides/ethnologue200">spoken</a> in<a href="https://www.ethnologue.com/guides/continents-most-indigenous-languages"> Asia, Africa, the Pacific region and the Americas</a>.</p><p>While we have seen exciting progress across many tasks in natural language processing (see<a href="https://paperswithcode.com/area/natural-language-processing"> Papers with Code</a> and<a href="http://nlpprogress.com/"> NLP Progress</a> for an overview) over the last years, most such results have been achieved in English and a small set of other high-resource languages.</p><p>In<a href="https://ruder.io/unsupervised-cross-lingual-learning/#the-nlp-resource-hierarchy"> a previous overview</a> of our<a href="https://tinyurl.com/xlingual"> ACL 2019 tutorial on Unsupervised Cross-lingual Representation Learning</a>, I've defined a resource hierarchy based on the availability of unlabelled data and labelled data online. In a recent<a href="https://arxiv.org/abs/2004.09095"> ACL 2020 paper</a>, Joshi et al. define a taxonomy similarly based on data availability, which you can see below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/06/language_data_distribution.png" class="kg-image" alt="Why You Should Do NLP Beyond English"><figcaption>Language resource distribution of <a href="https://arxiv.org/abs/2004.09095">Joshi et al. (2020).</a> The size and colour of a circle represent the number of languages and speakers respectively in each category. Colours (on the VIBGYOR spectrum; <strong>V</strong>iolet–<strong>I</strong>ndigo–<strong>B</strong>lue–<strong>G</strong>reen–<strong>Y</strong>ellow–<strong>O</strong>range–<strong>R</strong>ed) represent the total speaker population size from low (violet) to high (red).</figcaption></figure><p>Languages in categories 5 and 4 that lie at a sweet spot of having both large amounts of labelled and unlabelled data available to them are well-studied in the NLP literature. On the other hand, languages in the other groups have largely been neglected.</p><p>In this post, I will argue why you should work on languages other than English. Specifically, I will highlight reasons from a <a href="#the-societal-perspective">societal</a>, <a href="#the-linguistic-perspective">linguistic</a>, <a href="#the-ml-perspective">machine learning</a>, <a href="#the-cultural-and-normative-perspective">cultural and normative</a>, and <a href="#the-cognitive-perspective">cognitive</a> perspective.</p><h2 id="the-societal-perspective">The societal perspective</h2><p>Technology cannot be accessible if it is only available for English speakers with a <a href="https://en.wikipedia.org/wiki/Received_Pronunciation">standard accent</a>.</p><p>What language you speak determines your access to information, education, and even human connections. Even though we think of the Internet as open to everyone, there is a<a href="http://labs.theguardian.com/digital-language-divide/"> digital language divide</a> between dominant languages (mostly from the Western world) and others.<a href="https://w3techs.com/technologies/overview/content_language"> Only a few hundred languages are represented on the web</a> and speakers of minority languages are severely limited in the information available to them.</p><p>As many more languages are being written in informal contexts in chat apps and on social media, this divide extends to all levels of technology: At the most basic language technology level, low-resource languages lack keyboard support and spell checking (<a href="https://www.aclweb.org/anthology/L18-1656/">Soria et al., 2018</a>)—and keyboard support is even rarer for languages without a widespread written tradition (<a href="https://www.researchgate.net/publication/290279777_Keyboard_layouts_Lessons_from_the_me'phaa_and_sochiapam_Chinantec_designs">Paterson, 2015</a>). At a higher level, algorithms are biased and discriminate against speakers of <a href="https://qz.com/1141122/google-translates-gender-bias-pairs-he-with-hardworking-and-she-with-lazy-and-other-examples/">non-English</a> <a href="https://journals.sagepub.com/doi/10.1068/a44674">languages</a> or simply with <a href="https://www.washingtonpost.com/graphics/2018/business/alexa-does-not-understand-your-accent/">different</a> <a href="https://www.wired.com/2017/03/voice-is-the-next-big-platform-unless-you-have-an-accent/">accents</a>.</p><p>The latter is a problem because much existing work treats a high-resource language such as English as homogeneous. Our models consequently underperform on the plethora of related linguistic subcommunities, dialects, and accents (<a href="https://www.aclweb.org/anthology/D16-1120/">Blodgett et al., 2016</a>). In reality, the boundaries between language varieties are much blurrier than we make them out to be and language identification of similar languages and dialects is still a challenging problem (<a href="https://arxiv.org/abs/1804.08186">Jauhiainen et al., 2018</a>). For instance, even though Italian is the official language in Italy, there are around 34 regional languages and dialects spoken throughout the country.</p><p>A continuing lack of technological inclusion will not only exacerbate the language divide but it may also drive speakers of unsupported languages and dialects to high-resource languages with better technological support, further endangering such language varieties. To ensure that non-English language speakers are not left behind and at the same time to offset the existing imbalance, to lower language and literacy barriers, we need to apply our models to non-English languages.</p><h2 id="the-linguistic-perspective">The linguistic perspective</h2><p>Even though we claim to be interested in developing general language understanding methods, our methods are generally only applied to a single language, English.</p><p>English and the small set of other high-resource languages are in many ways not representative of the world's other languages. Many resource-rich languages belong to the Indo-European language family, are spoken mostly in the Western world, and are morphologically poor, i.e. information is mostly expressed syntactically, e.g. via a fixed word order and using <a href="https://en.wikipedia.org/wiki/Periphrasis">multiple separate words</a> rather than through variation at the word level.</p><p>For a more holistic view, we can take a look at the typological features of different languages. The<a href="https://wals.info/"> World Atlas of Language Structure</a> catalogues 192 typological features, i.e. structural and semantic properties of a language. For instance, one typological feature describes the typical <a href="https://wals.info/feature/81A#2/18.0/153.1">order of subject, object, and verb</a> in a language. Each feature has 5.93 categories on average. 48% of all feature categories exist only in the low-resource languages of groups 0–2 above and cannot be found in languages of groups 3–5 (<a href="https://arxiv.org/abs/2004.09095">Joshi et al., 2020</a>). Ignoring such a large subset of typological features means that our NLP models are potentially missing out on valuable information that can be useful for generalisation.</p><p>Working on languages beyond English may also help us gain new knowledge about the relationships between the languages of the world (<a href="https://www.aclweb.org/anthology/2020.acl-main.658/">Artetxe et al., 2020</a>). Conversely, it can help us reveal what linguistic features our models are able to capture. Specifically, you could use your knowledge of a particular language to probe aspects that differ from English such as the use of diacritics, extensive compounding, inflection, derivation, reduplication, agglutination, fusion, etc.</p><h2 id="the-ml-perspective">The ML perspective</h2><p>We encode assumptions into the architectures of our models that are based on the data we intend to apply them. Even though we intend our models to be general, many of their inductive biases are specific to English and languages similar to it.</p><p>The lack of any explicitly encoded information in a model does not mean that it is truly language agnostic. A classic example are n-gram language models, which perform significantly worse for languages with elaborate morphology and relatively free word order (<a href="https://journals.linguisticsociety.org/elanguage/lilt/article/view/2624.html">Bender, 2011</a>).</p><p>Similarly, neural models often overlook the complexities of morphologically rich languages (<a href="https://www.aclweb.org/anthology/2020.acl-main.660.pdf">Tsarfaty et al., 2020</a>): Subword tokenization performs poorly on languages with reduplication (<a href="https://arxiv.org/abs/1704.08352">Vania and Lopez, 2017</a>), byte pair encoding does not align well with morphology (<a href="https://arxiv.org/abs/2004.03720">Bostrom and Durrett, 2020</a>), and languages with larger vocabularies are more difficult for language models (<a href="https://www.aclweb.org/anthology/P19-1491/">Mielke et al., 2019</a>). Differences in grammar, word order, and syntax also cause problems for neural models (<a href="https://www.aclweb.org/anthology/W18-5412/">Ravfogel et al., 2018</a>;<a href="https://www.aclweb.org/anthology/N19-1253.pdf"> Ahmad et al., 2019</a>;<a href="https://arxiv.org/abs/2003.11080"> Hu et al., 2020</a>). In addition, we generally assume that pre-trained embeddings readily encode all relevant information, which may not be the case for all languages (<a href="https://www.aclweb.org/anthology/2020.acl-main.660.pdf">Tsarfaty et al., 2020</a>).</p><p>The above problems pose unique challenges for modelling structure—both on the word and the sentence level—, dealing with sparsity, few-shot learning, encoding relevant information in pre-trained representations, and transferring between related languages, among many other interesting directions. These challenges are not addressed by current methods and thus call for a new set of language-aware approaches.</p><p>Recent models have repeatedly matched human-level performance on increasingly difficult benchmarks—that is, in English using labelled datasets with thousands and unlabelled data with millions of examples. In the process, as a community we have overfit to the characteristics and conditions of English-language data. In particular, by focusing on high-resource languages, we have prioritised methods that work well only when large amounts of labelled and unlabelled data are available.</p><p>In contrast, most current methods break down when applied to the data-scarce conditions that are common for most of the world's languages. Even recent advances in pre-training language models that dramatically reduce the sample complexity for downstream tasks (<a href="https://www.aclweb.org/anthology/N18-1202/">Peters et al., 2018</a>; <a href="https://www.aclweb.org/anthology/P18-1031.pdf">Howard and Ruder, 2018</a>; <a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>; <a href="https://openreview.net/forum?id=r1xMH1BtvB">Clark et al., 2020</a>) require massive amounts of clean, unlabelled data, which is not available for most of the world's languages (<a href="https://www.aclweb.org/anthology/2020.acl-main.658/">Artetxe et al., 2020</a>). Doing well with few data is thus an ideal setting to test the limitations of current models—and evaluation on low-resource languages constitutes arguably its most impactful real-world application.</p><h2 id="the-cultural-and-normative-perspective">The cultural and normative perspective</h2><p>The data our models are trained on reveals not only the characteristics of the specific language but also sheds light on cultural norms and common sense knowledge.</p><p>However, such common sense knowledge may be different for different cultures. For instance, the notion of 'free' and 'non-free' varies cross-culturally where 'free' goods are ones that anyone can use without seeking permission, such as salt in a restaurant. Taboo topics are also different in different cultures. Furthermore, cultures vary in their assessment of relative power and social distance, among many other things (<a href="https://academic.oup.com/applij/article-abstract/4/2/91/167524?redirectedFrom=fulltext">Thomas, 1983</a>). In addition, many real-world situations such as ones included in the COPA dataset (<a href="https://ict.usc.edu/pubs/Choice%20of%20Plausible%20Alternatives-%20An%20Evaluation%20of%20Commonsense%20Causal%20Reasoning.pdf">Roemmele et al., 2011</a>) do not match the direct experience of many and equally do not reflect key situations that are obvious background knowledge for many people in the world (<a href="https://arxiv.org/abs/2005.00333">Ponti et al., 2020</a>).</p><p>Consequently, an agent that was only exposed to English data originating mainly in the Western world may be able to have a reasonable conversation with speakers from Western countries, but conversing with someone from a different culture may lead to pragmatic failures.</p><p>Beyond cultural norms and common sense knowledge, the data we train a model on also reflects the values of the underlying society. As an NLP researcher or practitioner, we have to ask ourselves whether we want our NLP system to exclusively share the values of a specific country or language community.</p><p>While this decision might be less important for current systems that mostly deal with simple tasks such as text classification, it will become more important as systems become more intelligent and need to deal with complex decision-making tasks.</p><h2 id="the-cognitive-perspective">The cognitive perspective</h2><p>Human children can acquire any natural language and their language understanding ability is remarkably consistent across all kinds of languages. In order to achieve human-level language understanding, our models should be able to show the same level of consistency across languages from different language families and typologies.</p><p>Our models should ultimately be able to learn abstractions that are not specific to the structure of any language but that can generalise to languages with different properties.</p><h2 id="what-you-can-do">What you can do</h2><p><strong>Datasets</strong>  If you create a new dataset, reserve half of your annotation budget for creating the same size dataset in another language.</p><p><strong>Evaluation</strong>  If you are interested in a particular task, consider evaluating your model on the same task in a different language. For an overview of some tasks, see <a href="http://nlpprogress.com/">NLP Progress</a> or our <a href="https://sites.research.google/xtreme">XTREME benchmark</a>.</p><p><strong>Bender Rule</strong>  <a href="https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/">State the language you are working on</a>.</p><p><strong>Assumptions</strong>  Be explicit about the signals your model uses and the assumptions it makes. Consider which are specific to the language you are studying and which might be more general.</p><p><strong>Language diversity</strong>  Estimate the language diversity of the sample of languages you are studying (<a href="https://arxiv.org/abs/2005.00333">Ponti et al., 2020</a>).</p><p><strong>Research</strong>  Work on methods that address challenges of low-resource languages. In the next post, I will outline interesting research directions and opportunities in multilingual NLP.</p><h2 id="citation">Citation</h2><p>For attribution in academic contexts, please cite this work as:</p><pre><code>@misc{ruder2020beyondenglish,
  author = {Ruder, Sebastian},
  title = {{Why You Should Do NLP Beyond English}},
  year = {2020},
  howpublished = {\url{http://ruder.io/nlp-beyond-english}},
}</code></pre><p></p><p><em>Thanks to Aida Nematzadeh, Laura Rimell, and Adhi Kuncoro for valuable feedback on drafts of this post.</em></p>]]></content:encoded></item><item><title><![CDATA[10 Tips for Research and a PhD]]></title><description><![CDATA[This post outlines 10 things that I did during my PhD and found particularly helpful in the long run.]]></description><link>http://ruder.io/10-tips-for-research-and-a-phd/</link><guid isPermaLink="false">5d440433808f272729699855</guid><category><![CDATA[advice]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Fri, 22 May 2020 13:23:32 GMT</pubDate><media:content url="http://ruder.io/content/images/2020/05/lessons_learned.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2020/05/lessons_learned.jpg" alt="10 Tips for Research and a PhD"><p>This advice should be most relevant to people studying machine learning (ML) and natural language processing (NLP) as that is what I did in my PhD. Having said that, this advice is not just limited to PhD students. If you are an independent researcher, want to start a PhD in the future or simply want to learn, then you will find most of this advice applicable.</p><p><strong>Pick and choose.</strong>  Everyone is different. You will have the most success if you adapt the particular advice to your situation and do what works for you. </p><p>TL;DR:</p><ol><li><a href="#1-read-broadly-">Read broadly.</a></li><li><a href="#2-work-on-two-things-">Work on two things.</a></li><li><a href="#3-be-ambitious-">Be ambitious.</a></li><li><a href="#4-collaborate-">Collaborate.</a></li><li><a href="#5-be-proactive-">Be proactive.</a></li><li><a href="#6-write-a-blog-">Write a blog.</a></li><li><a href="#7-keep-a-source-of-positive-energy-">Keep a source of positive energy.</a></li><li><a href="#8-play-to-your-strengths-">Play to your strengths.</a></li><li><a href="#9-intern-or-visit-a-university-">Intern or visit a university.</a></li><li><a href="#10-play-the-long-game-">Play the long game.</a></li></ol><h1 id="1-read-broadly-">1) Read broadly.</h1><p>While a PhD encourages you to delve deep into a specific topic, you can add value by making connections between different topics or entirely different fields. The papers that draw such connections can often be insightful. Many ideas in deep learning take inspiration from other fields such as biology (<a href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf">Hinton et al., 2014</a>), neuroscience (<a href="https://arxiv.org/abs/1611.05763">Wang et al., 2016</a>), physics (<a href="https://arxiv.org/abs/1902.04615">Cohen et al., 2019</a>), and many others.</p><p>In order to have a rich repertoire to draw on for inspiration, try to cultivate diverse interests. Look beyond your immediate horizon. Attend summer schools in other areas. Connect with people from other labs. Talk to people outside your subarea at conferences. Read papers from different disciplines.</p><p>ArXiv is a great source of research papers but staying up-to-date with the <a href="https://arxiv.org/help/subscribe">daily arXiv digest</a> feels like drinking from a firehose. Instead, I use services such as <a href="https://www.arxiv-sanity.com/">arXiv sanity preserver</a>, <a href="https://arxivist.com/">arXivist</a>, my Twitter feed, and recommendations from friends to stay up-to-date and to seek out different topics. I also generally prefer to read 10 papers superficially rather than one paper in-depth (as <a href="http://newsletter.ruder.io/issues/deep-learning-indaba-2018-edition-132825">suggested by Jeff Dean</a>). With a search-able paper management system (I use <a href="https://www.mendeley.com/">Mendeley</a>) you can always go back and reread the most relevant ones.</p><p>It can be helpful to dabble in different areas early in your PhD to get a sense for what interests you. Once you have found something, focus on the problems that you deeply care about. Think about the narrative you'd like to tell as part of your thesis.</p><h1 id="2-work-on-two-things-">2) Work on two things.</h1><p>While it is good to complete a project before starting a new one, working on a single project has downsides. If the project is not going well, your motivation and well-being may suffer. If you hit a roadblock, you can do nothing but grind until you resolve it. Developing such resilience is important but may at times come at a high mental cost.</p><p>Instead, I've found it useful to work on two projects at once to keep my sanity in check. If you hit a wall on one project, you can spend some time working on the other. This allows you to free up your mind and gain a new perspective, which may help you resolve the problem. If one of the projects is going well, this may also give you a boost to make progress on the other.</p><p>To minimise context-switching, I generally try to work on one project each day. It is also helpful if both projects are in similar areas so that you can apply what you learn on one project to the other one. </p><h1 id="3-be-ambitious-">3) Be ambitious.</h1><blockquote>“<em>Shoot for the moon. Even if you miss, you'll land among the stars.</em>”<br>—Norman Vincent Peale</blockquote><p>Another benefit of working on two projects is that it allows you to be more daring. You can work on a relatively safe project and one that is high-risk but also may be more impactful. The safe work ensures that you will graduate. The high-reward work may have a larger impact.</p><p>Ambitious projects demonstrate that you are creative and can come up with new ideas. Both are extremely valuable qualities. And even if such projects fail, they may lead you to discover unexpected insights that can lead to a publication.</p><p>Ambitious, however, does not mean that you should cater to the largest possible audience. A high impact can also be concentrated in a small community. A good indication of whether something you are working on is impactful is whether you'd be excited if it was published by someone else. In the end, you want to be known as someone that challenges the status quo and charts their own course.</p><h1 id="4-collaborate-">4) Collaborate.</h1><p>The PhD is often painted as a solitary affair, a lone journey on the quest towards knowledge. While you need to show substantial work that is your own in order to graduate, that does not mean that you are all on your own.</p><p>On the contrary, being able to collaborate is an important skill that you will need later on. Many projects with a large impact in ML and NLP such as <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">AlphaGo</a> or <a href="https://openai.com/blog/openai-five/">OpenAI Five</a> have been developed by a team. Whether you are part of a larger team or leading a group, you will have to collaborate with others.</p><p>Collaboration dynamics are more fluid compared to the adviser-PhD relationship, which is typically well defined. Collaborations are about <a href="http://colah.github.io/posts/2019-05-Collaboration/">building trust and mutual respect</a>. Successfully navigating collaborations takes practice. In collaborations, particularly if they are remote, it is important to communicate clearly and to set expectations.</p><p>If you are working on two projects, make one of them a collaboration. Collaborating with someone different from your advisor introduces you to a new perspective and will allow you to learn more than working on your own.</p><p>If you are based in a lab, collaborating with one of your lab mates is often the easiest choice. However, connecting and collaborating with people in other institutions may often be beneficial long-term.</p><h1 id="5-be-proactive-">5) Be proactive.</h1><p>This is probably the most important piece of advice. Don't restrict yourself to the people in your immediate circle.</p><p>Reach out to people. The main value of conferences is in bringing people together. Before a conference, look up who is going (by checking authors of accepted papers) and email them. Try to be respectful, briefly introduce yourself, and state why you'd like to meet them (a useful mnemonic is <a href="https://twitter.com/VikiLovesFACS/status/1202212904071811082">Inigo Montoya's Guide to Networking Success</a>). Most senior people make time for such meetings. Try to talk to many people and particularly seek out those who are not already well-known.</p><p>Outside of conferences, it is often useful to ask people who have worked in your area for research advice via email. It's amazing to see how many people in our field are genuinely helpful. <a href="https://tim.blog/2008/05/19/5-tips-for-e-mailing-busy-people/">Proper email etiquette</a> is important, however and makes it more likely that a busy researcher will respond. In particular, you should make it clear that you've done your research and explored alternative solutions before contacting them.</p><p>Beyond advice, such connections may lead to other opportunities further down the line: job offers, collaborations, mentorship, and even friendship. Many of my collaborations started through such connections—meetings at a conference, a cold email, a Twitter message. The important thing is that they are based on mutual interests and respect. So be conscious of other's people time. In addition, early career researchers with shared interests will often be much more open to collaborations than senior researchers who already have many commitments.</p><p>Being proactive also relates to how you view and talk about your research: Make it easy for other people to discover your work by highlighting it on your website, talking about it online, and writing a blog.</p><h1 id="6-write-a-blog-">6) Write a blog.</h1><p>Blogging has many advantages. It allows you to practice writing—and to learn to enjoy it. In order to finish your PhD, you will have to write a thesis, which can be an excruciating process. Blogging provides the training ground that prepares you for the thesis marathon.</p><p>From a research perspective, it allows you to practice <a href="https://www.nature.com/articles/d41586-019-02918-5">communicating and explaining things clearly</a>. Both are qualities that differentiate the best from mediocre research papers. In fact, <a href="https://www.cs.jhu.edu/~jason/advice/write-the-paper-first.html">clear writing</a> is important both to get your paper accepted and for high impact. In contrast to the hyper-compact format of research papers, a blog allows you to experiment and to find your own voice.</p><p>A blog can also be a great medium to present and share your work. A great blog post about a paper does not just reiterate its main findings but complements it. A blog can be <a href="https://worldmodels.github.io/">much</a> <a href="http://jalammar.github.io/illustrated-transformer/">more</a> <a href="https://distill.pub/2019/memorization-in-rnns/">flexible</a> than a paper: You can highlight interesting connections, provide the reader with a broad overview of the background literature and future directions, walk through an illustrative example, highlight code snippets or qualitative examples, show interactive visualisations, or perform an in-depth error analysis.</p><p>Another great way to start blogging is to discuss what you have just become knowledgeable about. Rachel Thomas puts this as "<a href="https://medium.com/@racheltho/why-you-yes-you-should-blog-7d2544ac1045">you are best positioned to help people one step behind you</a>". If you have just delved into a specialised area, why not save others the time and summarise the work and your insights. Most of my blog posts—from <a href="https://ruder.io/optimizing-gradient-descent/">gradient descent</a> to <a href="https://ruder.io/word-embeddings-1/">word embeddings</a>—started this way. If you have just learned how to do something cool, tell others about it. Conversely, if you want to learn about a certain topic but cannot find information about it online, consider creating that resource yourself. Starting your own blog <a href="https://www.fast.ai/2020/01/16/fast_template/">has never been easier</a>.</p><p>Having a blog is the single thing that has led to the most positive interactions throughout my PhD. ML and NLP have become so large that even if you write about a niche area, people will be interested. While I still feel anxious when I publish something, the response has always been worth it. In general, try to ignore unconstructive feedback and remember that the community appreciates genuine and honest voices.</p><h1 id="7-keep-a-source-of-positive-energy-">7) Keep a source of positive energy.</h1><p>External rewards such as paper acceptances are sparse, so leveraging intrinsic rewards is often necessary.</p><p>The most natural way to stay positive and energised in research is to work on something that excites you and to follow your curiosity. Depending on your funding or position, you might not be able to choose what you work on. In those cases, try to find a particular angle that excites you. Even an application of an existing algorithm can shed light on new and unsolved questions.</p><p>A PhD can be draining at the best of times. So it is important to build a support network that you can rely on. Surround yourself with positive people that support your ideas and ambitions. </p><p>At the same time, find an activity that you can fall back on to give you positive energy when things don't go as planned. This can be a collaboration, a side project, a hobby, exercise, meditation, or something else. For me, blogging filled this need. Compared to the long stretches of radio silence during peer review, writing, publishing and receiving feedback on a blog all within a couple of days feels liberating.</p><p>In the end, the most important resource is not the amount of compute you have, but your personal well-being. A crashed GPU can be rebooted; a burnt out GPU can't be fixed.</p><h1 id="8-play-to-your-strengths-">8) Play to your strengths.</h1><blockquote>“<em>The most value comes from doing something no else can do, or no one else has thought of.</em>”<br>—<a href="https://twitter.com/sama/status/1214274049074814976">Sam Altman</a></blockquote><p>With the increasing interest in ML and NLP, finding a fruitful undisturbed research topic can be challenging. A good strategy is to work on something that you are in the best position to tackle. Your ideal research topic sits at the intersection of work that is impactful, work that you are passionate about, and work that you are uniquely suited for.</p><p>What makes you uniquely suited can be one of many things: Your background; your knowledge of a particular technology, method, language, or data; your personal preferences. Do you come from a non-CS background? Use this as inspiration for your work. Are you a visually creative person? Supplement your blog and papers with graphs and analyses that will inspire others. Are you a strong coder? Implement technically challenging models. Are you great at maths? Prove your claims mathematically.</p><p>Another strength can be your network and the diversity of perspectives that you have access to. So locate others that complement your strengths, whether as advisors, mentors, or collaborators.</p><h1 id="9-intern-or-visit-a-university-">9) Intern or visit a university.</h1><p>The best way to make meaningful connections is to collaborate closely with people and to get to know them in-person. Internships and research visits are both excellent opportunities to expand your network as they enable you to work side-by-side with a group of talented people day-to-day.</p><p>They also allow you to get a feeling for how research is done in another environment. If you are considering whether to go into academia or industry, seeing first-hand how research is done in industry is an invaluable data point. A research visit or internship can also help you decide whether you would enjoy joining a lab or company at a later point.</p><p>Lastly, both are amazing learning experiences as you often will need to get familiar with a new tech stack or new research area. Through the guidance of a knowledgeable mentor different from your advisor, you will also be able to focus on different aspects of your personal growth.</p><h1 id="10-play-the-long-game-">10) Play the long game.</h1><p>Most of us are where we are because someone took a bet on us early on. My first research visit only happened because my host took a chance on me. So if you get the chance, pay it forward. Maximise not just the expected reward of yourself but of others around you.</p><p>While being at a big institution gives you access to an initial network, in the long term you want to develop a network of smart people that you can work with. One of the best ways to build a network is by being proactive and helping people as much as you can. This can be through writing blog posts or libraries, publishing tutorials and courses, doing podcasts, reimplementing models, or helping with open-source software. If you do this consistently, you will develop a reputation for being diligent and helpful and people will want to work with you.</p><p>Generally be kind to others. Assume good intentions. Be generous in giving praise and attribution. Don't hold grudges. In fact, being nice is one of the best things you can do to be successful (see Paul Graham's <a href="http://www.paulgraham.com/mean.html">Mean People Fail</a>). Being nice also has a recurring benefit as conferences are effectively—beyond the presentation and exchange of ideas—a yearly reunion of the friends you make along the way. </p><p>Take care of yourself. Work hard but get enough sleep and exercise. Take the time to learn new things. Work on things that you are not an expert at. In the end, always remind yourself that while a PhD is supposed to culminate in a thesis, the more important outcome of a PhD is a better version of yourself. </p><h1 id="references-and-inspiration">References and inspiration</h1><p>Finally, here are a few more pieces that served as inspiration:</p><ul><li>Andrey Karpathy's <a href="http://karpathy.github.io/2016/09/07/phd/">A Survival Guide to a PhD</a></li><li>The <a href="https://docs.google.com/document/d/18NoNdArdzDLJFQGBMVMsQ-iLOowP1XXDaSVRmYN0IyM/edit?usp=sharing">Frontiers in Natural Language Processing Expert Responses</a> at the Deep Learning Indaba 2018</li><li>John Schulman's <a href="http://joschu.net/blog/opinionated-guide-ml-research.html">An Opinionated Guide to ML Research</a></li><li>Andrey Kurenkov's <a href="https://www.andreykurenkov.com/writing/life/lessons-learned-from-failures/" rel="bookmark">Lessons Learned the Hard Way in Grad School (so far)</a></li><li>Volkan Cirik's <a href="https://www.cs.cmu.edu/~vcirik/blog/2019/phd-101/">PhD 101</a></li><li>Tim Dettmer's <a href="https://timdettmers.com/2020/03/10/how-to-pick-your-grad-school/">How to Pick Your Grad School</a></li><li>Isabelle Augenstein's <a href="https://medium.com/@isabelle.augenstein/increasing-well-being-in-academia-97f3ebc1599f">Increasing Well-Being in Academia</a></li><li>Richard Hamming's <a href="http://www.cs.virginia.edu/~robins/YouAndYourResearch.html">You and Your Research</a></li><li>Fei-Fei Li's <a href="https://bigaidream.gitbooks.io/tech-blog/content/2014/de-mystifying-good-research.html">De-Mystifying Good Research and Good Papers</a></li><li>Sam Altman's <a href="https://blog.samaltman.com/how-to-be-successful">How To Be Successful</a> and associated <a href="https://twitter.com/sama/status/1214274038933020672?lang=en">Twitter thread</a></li><li>Stuart K. Card's <a href="http://hci.stanford.edu/~cagatay/StuCard-WinPrizesGloryPhD.pdf">The PhD Thesis Deconstructed</a></li><li>Tweets from <a href="https://twitter.com/habdollahpouri/status/1201143604418187265">Himan Abdollahpouri</a>, <a href="https://twitter.com/chipro/status/1196889061799055360">Chip Huyen</a>, and many others</li></ul>]]></content:encoded></item><item><title><![CDATA[10 ML & NLP Research Highlights of 2019]]></title><description><![CDATA[This post gathers ten ML and NLP research directions that I found exciting and impactful in 2019.]]></description><link>http://ruder.io/research-highlights-2019/</link><guid isPermaLink="false">5dadee53c1f8023ac7c907cc</guid><category><![CDATA[natural language processing]]></category><category><![CDATA[cross-lingual]]></category><category><![CDATA[transfer learning]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Mon, 06 Jan 2020 08:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2020/01/videobert-1.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2020/01/videobert-1.png" alt="10 ML & NLP Research Highlights of 2019"><p>This post gathers ten ML and NLP research directions that I found exciting and impactful in 2019.</p><p>For each highlight, I summarise the main advances that took place this year, briefly state why I think it is important, and provide a short outlook to the future.</p><p>The full list of highlights is here:</p><ol><li><a href="#1-universal-unsupervised-pretraining">Universal unsupervised pretraining</a></li><li><a href="#2-lottery-tickets">Lottery tickets</a></li><li><a href="#3-the-neural-tangent-kernel">The Neural Tangent Kernel</a></li><li><a href="#4-unsupervised-multilingual-learning">Unsupervised multilingual learning</a></li><li><a href="#5-more-robust-benchmarks">More robust benchmarks</a></li><li><a href="#6-ml-and-nlp-for-science">ML and NLP for science</a></li><li><a href="#7-fixing-decoding-errors-in-nlg">Fixing decoding errors in NLG</a></li><li><a href="#8-augmenting-pretrained-models">Augmenting pretrained models</a></li><li><a href="#9-efficient-and-long-range-transformers">Efficient and long-range Transformers</a></li><li><a href="#10-more-reliable-analysis-methods">More reliable analysis methods</a></li></ol><h1 id="1-universal-unsupervised-pretraining">1) Universal unsupervised pretraining</h1><p><strong>What happened?</strong>  Unsupervised pretraining was prevalent in NLP this year, mainly driven by BERT (<a href="https://www.aclweb.org/anthology/N19-1423/">Devlin et al., 2019</a>) and other variants. <a href="https://twitter.com/xwang_lk/status/1166187855456002049">A whole range of BERT variants</a> have been applied to multimodal settings, mostly involving images and videos together with text (for an example see the figure below). Unsupervised pretraining has also made inroads into domains where supervision had previously reigned supreme. In biology, Transformer language models have been pretrained on protein sequences (<a href="https://www.biorxiv.org/content/10.1101/622803v1.full">Rives et al., 2019</a>). In computer vision, approaches leveraged self-supervision including CPC (<a href="https://arxiv.org/abs/1905.09272">Hénaff et al., 2019</a>), MoCo (<a href="https://arxiv.org/abs/1911.05722">He et al., 2019</a>), and PIRL (<a href="http://ruder.io/research-highlights-2019/arxiv.org/abs/1912.01991">Misra &amp; van der Maaten, 2019</a>) as well as strong generators such as BigBiGAN (<a href="https://arxiv.org/abs/1907.02544">Donahue &amp; Simonyan, 2019</a>) to improve sample efficiency on ImageNet and image generation. In speech, representations learned with a multi-layer CNN (<a href="https://arxiv.org/abs/1904.05862">Schneider et al., 2019</a>) or bidirectional CPC (<a href="https://openreview.net/forum?id=HJe-blSYvH">Kawakami et al., 2019</a>) outperform state-of-the-art models with much less training data.</p><p><strong>Why is it important?</strong>  Unsupervised pretraining enables training models with much fewer labelled examples. This opens up new applications in many different domains where data requirements were previously prohibitive.</p><p><strong>What's next?</strong>  Unsupervised pretraining is here to stay. While the biggest advances have been achieved so far in individual domains, it will be interesting to see a focus towards tighter integration of multiple modalities.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/videobert.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>VideoBERT (<a href="https://arxiv.org/abs/1904.01766">Sun et al., 2019</a>), a recent multimodal variant of BERT that generates video "tokens" given a recipe (above) and predicts future tokens at different time scales given a video token (below).</figcaption></figure><h1 id="2-lottery-tickets">2) Lottery tickets</h1><p><strong>What happened?  </strong><a href="https://openreview.net/forum?id=rJl-b3RcF7">Frankle and Carbin</a> (2019) identified <em>winning tickets</em>, subnetworks in dense, randomly-initialised, feed-forward networks that are so well initialised that training them in isolation achieves similar accuracy to training the full network, as can be seen below. While the initial pruning procedure only worked on small vision tasks, later work (<a href="https://arxiv.org/abs/1903.01611">Frankle et al., 2019</a>) applied the pruning early in training instead of at initialisation, which makes it possible to find small subnetworks of deeper models. <a href="https://arxiv.org/abs/1906.02768">Yu et al. (2019)</a> find winning ticket initialisations also for LSTMs and Transformers in NLP and RL models. While winning tickets are still expensive to find, it is promising that they seem to be transferable across datasets and optimisers (<a href="https://papers.nips.cc/paper/8739-one-ticket-to-win-them-all-generalizing-lottery-ticket-initializations-across-datasets-and-optimizers.pdf">Morcos et al., 2019</a>) and domains (<a href="https://www.aclweb.org/anthology/D19-6117/">Desai et al., 2019</a>).</p><p><strong>Why is it important?</strong>  State-of-the-art neural networks are getting larger and more expensive to train and to use for prediction. Being able to consistently identify small subnetworks that achieve comparable performance enables training and inference with much fewer resources. This can speed up model iteration and opens up new applications in on-device and edge computing.</p><p><strong>What's next?</strong>  Identifying winning tickets is currently still too expensive to provide real benefits in low-resource settings. More robust one-shot pruning methods that are less susceptible to noise in the pruning process should mitigate this. Investigating what makes winning tickets special should also help us gain a better understanding of the initialisation and learning dynamics of neural networks. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/winning_tickets.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>Test accuracy of winning tickets (solid lines) vs. randomly sampled subnetworks (dashed lines) at different pruning ratios (<a href="https://openreview.net/forum?id=rJl-b3RcF7">Frankle &amp; Carbin, 2019</a>).</figcaption></figure><h1 id="3-the-neural-tangent-kernel">3) The Neural Tangent Kernel</h1><p><strong>What happened?  </strong>Somewhat counter-intuitively, very wide (more concretely, <em>infinitely</em> wide) neural networks are easier to study theoretically than narrow ones. It has been shown that in the infinite-width limit, neural networks can be approximated as linear models with a kernel, the Neural Tangent Kernel (NTK; <a href="https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">Jacot et al., 2018</a>). Refer to <a href="https://rajatvd.github.io/NTK/">this post </a>for an intuitive explanation of NTK including an illustration of its training dynamics (see the figure below). In practice, such models, have underperformed their finite-depth counterparts (<a href="https://openreview.net/pdf?id=B1g30j0qF7">Novak et al., 2019</a>; <a href="https://papers.nips.cc/paper/8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers">Allen-Zhu et al., 2019</a>; <a href="http://papers.nips.cc/paper/9449-on-the-inductive-bias-of-neural-tangent-kernels">Bietti &amp; Mairal, 2019</a>), which limits applying the findings to standard methods. Recent work (<a href="https://arxiv.org/abs/1911.00809">Li et al., 2019</a>; <a href="https://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf">Arora et al., 2019</a>), however, has significantly reduced the performance gap to standard methods (see <a href="https://huyenchip.com/2019/12/18/key-trends-neurips-2019.html">Chip Huyen's post</a> for other related NeurIPS 2019 papers).</p><p><strong>Why is it important?</strong>  The NTK is perhaps the most powerful tool at our disposal to analyse the theoretical behaviour of neural networks. While it has its limitations, i.e. practical neural networks still perform better than their NTK counterparts, and insights so far have not translated into empirical gains, it may help us open the black box of deep learning.</p><p><strong>What's next?</strong>  The gap to standard methods seems to be mainly due to benefits of the finite width of such methods, which future work may seek to characterise. This will hopefully also help translating insights from the infinite-width limit to practical settings. Ultimately, the NTK may help us shed light on the training dynamics and generalisation behaviour of neural networks. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/ntk_dynamics.gif" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>Learning dynamics of linear models with an NTK with different α factors. NTKs are visualised as ellipses (credit: <a href="https://rajatvd.github.io/NTK/">Rajat's Blog</a>).</figcaption></figure><h1 id="4-unsupervised-multilingual-learning">4) Unsupervised multilingual learning</h1><p><strong>What happened?  </strong>Cross-lingual representations had mostly focused on the word level for many years (<a href="https://www.jair.org/index.php/jair/article/view/11640">see this survey</a>). Building on advances in unsupervised pretraining, this year saw the development of deep cross-lingual models such as <a href="https://github.com/google-research/bert/blob/master/multilingual.md">multilingual BERT</a>, XLM (<a href="https://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining.pdf">Conneau &amp; Lample, 2019</a>), and XLM-R (<a href="https://arxiv.org/abs/1911.02116">Conneau et al., 2019</a>). Even though these models do not use any explicit cross-lingual signal, they generalise surprisingly well across languages—even without a shared vocabulary or joint training (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>; <a href="https://openreview.net/forum?id=HJeT3yrtDr">Karthikeyan et al., 2019</a>; <a href="https://arxiv.org/abs/1911.01464">Wu et al., 2019</a>). For an overview, have a look at <a href="https://ruder.io/unsupervised-cross-lingual-learning/#unsupervised-deep-models">this post</a>. Such deep models also brought improvements in unsupervised MT (<a href="https://arxiv.org/abs/1905.02450">Song et al., 2019</a>; <a href="https://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining.pdf">Conneau &amp; Lample, 2019</a>), which hit its stride last year (see <a href="https://ruder.io/10-exciting-ideas-of-2018-in-nlp/#1-unsupervised-mt">highlights of 2018</a>) and saw improvements from a more principled combination of statistical and neural approaches (<a href="https://www.aclweb.org/anthology/P19-1019/">Artetxe et al., 2019</a>). Another exciting development is the bootstrapping of deep multilingual models from readily available pretrained representations in English (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>; <a href="https://openreview.net/forum?id=Bkle6T4YvB">Tran, 2020</a>), which can be seen below.</p><p><strong>Why is it important?</strong>  Ready-to-use cross-lingual representations enable training of models with fewer examples for languages other than English. Furthermore, if labelled data in English is available, these methods enable essentially free zero-shot transfer. They may finally help us gain a better understanding of the relationships between different languages.</p><p><strong>What's next?</strong>  It is still unclear why these methods work so well without any cross-lingual supervision. Gaining a better understanding of how these methods work will likely enable us to design more powerful methods and may also reveal insights about the structure of different languages. In addition, we should not only focus on zero-shot transfer but also consider learning from few labelled examples in the target language (see <a href="http://nlp.fast.ai/classification/2019/09/10/multifit.html">this post</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/monolingual_transfer.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>The four steps of the monolingual transfer approach of <a href="https://arxiv.org/abs/1910.11856">Artetxe et al. (2019)</a></figcaption></figure><h1 id="5-more-robust-benchmarks">5) More robust benchmarks</h1><blockquote>There is something rotten in the state of the art.<br>—<a href="https://arxiv.org/abs/1910.14599">Nie et al. (2019)</a> paraphrasing <a href="http://www.shakespeare-online.com/quickquotes/quickquotehamletdenmark.html">Shakespeare</a></blockquote><p><strong>What happened?  </strong>Recent NLP datasets such as HellaSWAG (<a href="https://www.aclweb.org/anthology/P19-1472/">Zellers et al., 2019</a>) are created to be difficult for state-of-the-art models to solve. Examples are filtered by humans to explicitly retain only those where state-of-the-art models fail (see below for an example). This process of human-in-the-loop adversarial curation can be repeated multiple times such as in the recent Adversarial NLI (<a href="https://arxiv.org/abs/1910.14599">Nie et al., 2019</a>) benchmark to enable the creation of datasets that are much more challenging for current methods.</p><p><strong>Why is it important?</strong>  Many researchers have observed that current NLP models do not learn what they are supposed to but instead adopt shallow heuristics and exploit superficial cues in the data (described as <a href="https://thegradient.pub/nlps-clever-hans-moment-has-arrived/">the Clever Hans moment</a>). As datasets become more robust, we would hope that models will be forced to eventually learn the true underlying relations in the data.</p><p><strong>What's next?</strong>  As models become better, most datasets will need to be continuously improved or will quickly become outdated. Dedicated infrastructure and tools will be necessary to facilitate this process. In addition, appropriate baselines should be run including simple methods and models using different variants of the data (such as with incomplete input) so that initial versions of datasets are as robust as possible.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/hellaswag.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>A multiple-choice sentence completion example from HellaSWAG that is difficult to answer for state-of-the-art models. Most hard examples lie in a "Goldilocks zone" of complexity, consisting roughly of three sentences of context and two generated sentences (<a href="https://www.aclweb.org/anthology/P19-1472/">Zellers et al., 2019</a>).</figcaption></figure><h1 id="6-ml-and-nlp-for-science">6) ML and NLP for science</h1><p><strong>What happened?  </strong>There have been some major advances of ML being applied to fundamental science problems. My highlights were the application of deep neural networks to <a href="https://www.nature.com/articles/d41586-019-01357-6">protein folding</a> and to the Many-Electron Schrödinger Equation (<a href="https://arxiv.org/abs/1909.02487">Pfau et al., 2019</a>). On the NLP side, it is exciting to see what impact even standard methods can have when combined with domain expertise. One study used word embeddings to analyse latent knowledge in the materials science literature (<a href="https://www.nature.com/articles/s41586-019-1335-8">Tshitoyan et al., 2019</a>), which can be used to predict which materials will have certain properties (see the figure below). In biology, a lot of data such as genes and proteins is sequential in nature. It is thus a natural fit for NLP methods such as LSTMs and Transformers, which have been applied to protein classification (<a href="https://www.biorxiv.org/content/10.1101/704874v2">Strodthoff et al., 2019</a>; <a href="https://www.biorxiv.org/content/10.1101/622803v2">Rives et al., 2019</a>).</p><p><strong>Why is it important?  </strong>Science is arguably one of the most impactful application domains for ML. Solutions can have a large impact on many other domains and can help solve real-world problems.</p><p><strong>What's next?</strong>  From modelling energy in physics problems (<a href="http://papers.nips.cc/paper/9672-hamiltonian-neural-networks.pdf">Greydanus et al., 2019</a>) to solving differential equations (<a href="https://openreview.net/forum?id=S1eZYeHFDS">Lample &amp; Charton, 2020</a>), ML methods are constantly being applied to new applications in science. It will be interesting to see what the most impactful of these will be in 2020.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/historical_validations_functional_material_predictions.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>Using word embeddings trained on abstracts from different time periods to predict which materials will be studied as ferroelectric (a), photovoltaics (b), and topological insulator (c) in future abstracts. Top 50 predictions are much more likely to be studied compared to all candidate materials (<a href="https://www.nature.com/articles/s41586-019-1335-8">Tshitoyan et al., 2019</a>).</figcaption></figure><h1 id="7-fixing-decoding-errors-in-nlg">7) Fixing decoding errors in NLG</h1><p><strong>What happened?</strong>  Despite ever more powerful models, natural language generation (NLG) models still frequently produce repetitions or gibberish as can be seen below. This was shown to be mainly a result of the maximum likelihood training. I was excited to see improvements that aim to ameliorate this and are orthogonal to advances in modelling. Such improvements came in the form of new sampling methods, such as nucleus sampling (<a href="https://openreview.net/forum?id=rygGQyrFvH">Holtzman et al., 2019</a>) and new loss functions (<a href="https://openreview.net/forum?id=SJeYe0NtvH">Welleck et al., 2019</a>). Another surprising finding was that better search does not lead to better generations: Current models rely to some extent on an imperfect search and beam search errors. In contrast, an exact search most often returns an empty translation in the case of machine translation (<a href="https://www.aclweb.org/anthology/D19-1331.pdf">Stahlberg &amp; Byrne, 2019</a>). This shows that advances in search and modelling must often go hand in hand.</p><p><strong>Why is it important? </strong> Natural language generation is one of the most general tasks in NLP. In NLP and ML research, most papers focus on improving the model, while other parts of the pipeline are typically neglected. For NLG, it is important to remind ourselves that our models still have flaws and that it may be possible to improve the output by fixing the search or the training process.</p><p><strong>What's next?</strong>  Despite more powerful models and successful applications of transfer learning to NLG (<a href="https://arxiv.org/abs/1905.02450">Song et al., 2019</a>; <a href="https://arxiv.org/abs/1901.08149">Wolf et al., 2019</a>), model predictions still contain many artefacts. Identifying and understanding the causes of such artefacts is an important research direction.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/text_degeneration.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>Repetitions (blue) and gibberish (red) produced by GPT-2 using beam search and pure (greedy) sampling (<a href="https://openreview.net/forum?id=rygGQyrFvH">Holtzman et al., 2019</a>).</figcaption></figure><h1 id="8-augmenting-pretrained-models">8) Augmenting pretrained models</h1><p><strong>What happened?  </strong>I was excited to see approaches that equip pretrained models with new capabilities. Some methods augment a pretrained model with a knowledge base in order to improve modelling of entity names (<a href="https://www.aclweb.org/anthology/N19-1117/">Liu et al., 2019</a>) and the recall of facts (<a href="https://www.aclweb.org/anthology/P19-1598/">Logan et al., 2019</a>). Others enable it to perform simple arithmetic reasoning (<a href="https://www.aclweb.org/anthology/D19-1609/">Andor et al., 2019</a>) by giving it access to a number of predefined executable programs. As most models have a weak inductive bias and learn most of their knowledge from data, another way to extend a pretrained model is by augmenting the training data itself, e.g. to capture common sense (<a href="https://www.aclweb.org/anthology/P19-1470/">Bosselut et al., 2019</a>) as can be seen below.</p><p><strong>Why is it important?  </strong>Models are becoming more powerful but <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5888218f39_11_238">there are many things</a> that a model cannot learn from text alone. Particularly when dealing with more complex tasks, the available data may be too limited to learn explicit reasoning using facts or common sense and a stronger inductive bias may often be necessary.</p><p><strong>What's next?</strong>  As models are being applied to more challenging problems, it will increasingly become necessary for modifications to be compositional. In the future, we might combine powerful pretrained models with learnable compositional programs (<a href="https://papers.nips.cc/paper/9608-learning-compositional-neural-programs-with-recursive-tree-search-and-planning">Pierrot et al., 2019</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/comet.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>A standard Transformer with multi-head attention. The model is trained to predict the object of a knowledge base triple given its subject and relation (<a href="https://www.aclweb.org/anthology/P19-1470/">Bosselut et al., 2019</a>).</figcaption></figure><h1 id="9-efficient-and-long-range-transformers">9) Efficient and long-range Transformers</h1><p><strong>What happened?  </strong>This year saw several improvements to the Transformer (<a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need">Vaswani et al., 2017</a>) architecture. The Transformer-XL (<a href="https://www.aclweb.org/anthology/P19-1285/">Dai et al., 2019</a>) and the Compressive Transformer (<a href="https://openreview.net/forum?id=SylKikSYDH">Rae et al., 2020</a>), which can be seen below enable it to better capture long-range dependencies. Many approaches sought to make the Transformer more efficient mostly using different—often sparse—attention mechanisms, such as adaptively sparse attention (<a href="https://www.aclweb.org/anthology/D19-1223/">Correia et al., 2019</a>), adaptive attention spans (<a href="https://www.aclweb.org/anthology/P19-1032/">Sukhbaatar et al., 2019</a>), product-key attention (<a href="https://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys">Lample et al., 2019)</a>, and locality-sensitive hashing (<a href="https://openreview.net/forum?id=rkgNKkHtvB">Kitaev et al., 2020</a>). On the Transformer-based pretraining front, there have been more efficient variants such as ALBERT (<a href="https://openreview.net/forum?id=H1eA7AEtvS">Lan et al., 2020</a>), which employs parameter sharing and ELECTRA (<a href="https://openreview.net/forum?id=r1xMH1BtvB">Clark et al., 2020</a>), which uses a more efficient pretraining task. There were also more efficient pretrained models that did not utilise a Transformer, such as the unigram document model VAMPIRE (<a href="https://www.aclweb.org/anthology/P19-1590/">Gururangan et al., 2019</a>) and the QRNN-based MultiFiT (<a href="https://www.aclweb.org/anthology/D19-1572/">Eisenschlos et al., 2019</a>). Another trend was the distillation of large BERT models into smaller ones (<a href="https://arxiv.org/abs/1903.12136">Tang et al., 2019</a>; <a href="https://www.aclweb.org/anthology/D19-1374/">Tsai et al., 2019</a>; <a href="https://arxiv.org/abs/1910.01108">Sanh et al., 2019</a>).</p><p><strong>Why is it important?</strong>  The Transformer architecture has been influential since its inception. It is a part of most state-of-the-art models in NLP and has been successfully applied to many other domains (see Sections <a href="#1-universal-unsupervised-pretraining">1</a> and <a href="#6-ml-and-nlp-for-science">6</a>). Any improvement to the Transformer architecture may thus have strong ripple effects.</p><p><strong>What's next?</strong>  It will take some time for these improvements to trickle down to the practitioner but given the prevalence and ease of use of pretrained models, more efficient alternatives will likely be adopted quickly. Overall, we will hopefully see a continuing focus on model architectures that emphasise efficiency, with sparsity being one of the key trends. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/compressive_transformer.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>The Compressive Transformer compresses (a fine-grained memory of) past activations into a coarser compressed memory (<a href="https://openreview.net/forum?id=SylKikSYDH">Rae et al., 2020</a>).</figcaption></figure><h1 id="10-more-reliable-analysis-methods">10) More reliable analysis methods</h1><p><strong>What happened?</strong>  A key trend for me this year was the increasing number of papers analysing models. In fact, several of my favourite papers this year were such analysis papers. An early highlight was the excellent survey of analysis methods by <a href="https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00254">Belinkov &amp; Glass (2019)</a>. This year was also the first one (in my memory) where many papers were dedicated to analysing a single model, BERT (such papers are known as <a href="http://newsletter.ruder.io/issues/bert-gpt-2-xlnet-naacl-icml-arxiv-eurnlp-180092">BERTology</a>). In this context, probes (see the figure below), which aim to understand whether a model captures morphology, syntax, etc. by predicting certain properties have become a common tool. I particularly appreciated papers that make probes more reliable (<a href="https://www.aclweb.org/anthology/N19-1225/">Liu et al., 2019</a>; <a href="https://www.aclweb.org/anthology/D19-1275/">Hewitt &amp; Liang, 2019</a>). Reliability is also a theme in the ongoing conversation on whether attention provides meaningful explanations (<a href="https://www.aclweb.org/anthology/N19-1357/">Jain &amp; Wallace, 2019</a>; <a href="https://www.aclweb.org/anthology/D19-1002/">Wiegreffe &amp; Pinter, 2019</a>; <a href="https://medium.com/@byron.wallace/thoughts-on-attention-is-not-not-explanation-b7799c4c3b24">Wallace, 2019</a>). The continuing interest in analysis methods is perhaps best exemplified by the new ACL 2020 track on <a href="https://acl2020.org/calls/papers/">Interpretability and Analysis of Models in NLP</a>.</p><p><strong>Why is it important?  </strong>State-of-the-art methods are used as black boxes. In order to develop better models and to use them in the real world, we need to understand why models make certain decisions. However, our current methods to explain models' predictions are still limited. </p><p><strong>What's next?  </strong>We need more work on explaining predictions that goes beyond visualisation, which is often unreliable. An important trend in this direction are human-written explanations that are being provided by more datasets (<a href="https://papers.nips.cc/paper/8163-e-snli-natural-language-inference-with-natural-language-explanations">Camburu et al., 2018</a>; <a href="https://www.aclweb.org/anthology/P19-1487/">Rajani et al., 2019</a>; <a href="https://arxiv.org/abs/1910.14599">Nie et al., 2019</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2020/01/probing_landscape.png" class="kg-image" alt="10 ML & NLP Research Highlights of 2019"><figcaption>The probing setup used to study linguistic knowledge in representations (<a href="https://www.aclweb.org/anthology/N19-1225/">Liu et al., 2019</a>).</figcaption></figure>]]></content:encoded></item><item><title><![CDATA[Unsupervised Cross-lingual Representation Learning]]></title><description><![CDATA[This post expands on the ACL 2019 tutorial on Unsupervised Cross-lingual Representation Learning. It highlights key insights and takeaways and provides updates based on recent work, particularly unsupervised deep multilingual models.]]></description><link>http://ruder.io/unsupervised-cross-lingual-learning/</link><guid isPermaLink="false">5d514420808f2727296998ff</guid><category><![CDATA[cross-lingual]]></category><category><![CDATA[transfer learning]]></category><category><![CDATA[natural language processing]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Sat, 26 Oct 2019 11:35:18 GMT</pubDate><media:content url="http://ruder.io/content/images/2019/11/unsupervised_multilingual_overview.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2019/11/unsupervised_multilingual_overview.png" alt="Unsupervised Cross-lingual Representation Learning"><p>This post expands on the <a href="https://tinyurl.com/xlingual">ACL 2019 tutorial on Unsupervised Cross-lingual Representation Learning</a>.</p><p>The tutorial was organised by <a href="https://sites.google.com/site/ivanvulic/">Ivan Vulić</a>, <a href="https://anderssoegaard.github.io/">Anders Søgaard</a>, and me. In this post, I highlight key insights and takeaways and provide additional context and updates based on recent work. In particular, I cover <a href="#unsupervised-deep-models">unsupervised deep multilingual models</a> such as multilingual BERT. You can see the structure of this post below:</p><figure class="kg-card kg-image-card"><img src="http://ruder.io/content/images/2019/10/unsupervised_multilingual_overview.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"></figure><p>The slides of the tutorial are <a href="https://docs.google.com/presentation/d/1miPu0hkBFSRTAxP-Psg0xXOvAbVSJPhQXehMQdUI88I/edit?usp=sharing">available online</a>.</p><h1 id="introduction">Introduction</h1><p>Cross-lingual representation learning can be seen as an instance of transfer learning, similar to domain adaptation. The domains in this case are different languages. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/10/cross_lingual_learning_taxonomy.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>Cross-lingual learning in the transfer learning taxonomy (<a href="https://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=64">Ruder, 2019</a>)</figcaption></figure><p>Methods from domain adaptation have also been applied to cross-lingual transfer (<a href="https://arxiv.org/abs/1008.0716">Prettenhofer &amp; Stein, 2011</a>, <a href="https://pdfs.semanticscholar.org/f08f/269b715c31bfa02dc4df31f5561fd74222f0.pdf">Wan et al., 2011</a>). For a clearer distinction between domain adaptation and cross-lingual learning, have a look at <a href="https://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=63">this section</a>.</p><p>Viewing cross-lingual learning as a form of transfer learning can help us understand why it might be useful and when it might fail: </p><ol><li>Transfer learning is useful whenever the scenarios between which we transfer share some underlying structure. In a similar vein, languages share commonalities on many levels—from <a href="https://en.wikipedia.org/wiki/Loanword">loanwords</a> and <a href="https://en.wikipedia.org/wiki/Cognate">cognates</a> on the lexical level, to the <a href="https://universaldependencies.org/">structure of sentences</a> on the syntactic level, to the <a href="https://en.wikipedia.org/wiki/Semantic_primes">meaning of words</a> on the semantic level. Learning about the structure of one language may therefore help us do better when learning a second language.</li><li>Transfer learning struggles when the source and target settings are too different. Similarly, it is harder to transfer between languages that are dissimilar (e.g. that have different typological features in the <a href="https://wals.info/languoid">World Atlas of Language Structures</a>).</li></ol><p>Similar to the <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5888218f39_46_122">broader transfer learning landscape</a>, much of the work on cross-lingual learning over the last years has focused on learning representations of words. While there have been approaches that learn sentence-level representations, only recently are we witnessing the emergence of <a href="#unsupervised-deep-models">deep multilingual models</a>. Have a look at <a href="https://www.jair.org/index.php/jair/article/view/11640/26511">this survey</a> for an overview of the history of cross-lingual models. For more on transfer learning, check out <a href="http://ruder.io/state-of-transfer-learning-in-nlp/">this recent post</a>. Cross-lingual learning might be useful—but why should we care about applying NLP to other languages in the first place?</p><h2 id="the-digital-language-divide">The Digital Language Divide</h2><p>The language you speak shapes your experience of the world. This is known as the <a href="https://en.wikipedia.org/wiki/Linguistic_relativity">Sapir-Whorf hypothesis</a> and is contested. On the other hand, it is factual that the language you speak shapes your experience of the world <em>online</em>. What language you speak determines your access to information, education, and even human connections. In other words, even though we think of the Internet as being open to anyone, there is a <a href="http://labs.theguardian.com/digital-language-divide/">digital language divide</a> between dominant languages (mostly from the western world) and others.</p><p>As NLP technologies are becoming more commonplace, this divide extends to the technological level: At best, this means that non-English language speakers do not benefit from the latest features of digital assistants; at worst, algorithms are biased against or discriminate against speakers of non-English languages—we can see <a href="https://www.washingtonpost.com/graphics/2018/business/alexa-does-not-understand-your-accent/">occurrences</a> <a href="https://www.wired.com/2017/03/voice-is-the-next-big-platform-unless-you-have-an-accent/">of this</a> <a href="https://qz.com/1141122/google-translates-gender-bias-pairs-he-with-hardworking-and-she-with-lazy-and-other-examples/">already</a> <a href="https://journals.sagepub.com/doi/10.1068/a44674">today</a>. To ensure that non-English language speakers are not left behind and at the same time to offset the existing imbalance, we need to apply our models to non-English languages.</p><h2 id="the-nlp-resource-hierarchy">The NLP Resource Hierarchy</h2><p>In current machine learning, the amount of available training data is the main factor that influences an algorithm's performance. Data that is useful for current NLP models can range from unlabelled data in the form of documents online or articles on Wikipedia, labelled data in the form of large curated datasets, or parallel corpora used in machine translation.</p><p>We can roughly differentiate between languages with many and languages with few data resources in a hierarchy, which can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/10/nlp_resource_hierarchy.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>A conceptual view of the NLP resource hierarchy</figcaption></figure><p>This approximately corresponds with a language's presence online. Note that many languages cannot be assigned clearly to a single level of the hierarchy. Some languages with few speakers such as <a href="https://en.wikipedia.org/wiki/Aragonese_language">Aragonese</a> or <a href="https://en.wikipedia.org/wiki/Waray_language">Waray-Waray</a> have a <a href="https://meta.wikimedia.org/wiki/List_of_Wikipedias_by_speakers_per_article">disproportionally</a> <a href="https://meta.wikimedia.org/wiki/List_of_Wikipedias">large Wikipedia</a>, while for others such as <a href="https://en.wikipedia.org/wiki/Latvian_language">Latvian</a> <a href="https://www.statmt.org/europarl/">parallel data</a> is available due to their status as an official language. On the other hand, many languages with tens of millions of speakers such as <a href="https://en.wikipedia.org/wiki/Xhosa_language">Xhosa</a> and <a href="https://en.wikipedia.org/wiki/Zulu_language">Zulu</a> have barely more than <a href="https://meta.wikimedia.org/wiki/List_of_Wikipedias">1,000 Wikipedia articles</a>.</p><p>Nevertheless, the existence of such a hierarchy highlights why unsupervised cross-lingual representation learning is promising: for most of the world's languages, no labelled data is available and creating labelled data is expensive as native speakers are under-represented online. In order to train models for such languages, we need to make use of unlabelled data and transfer learning from high-resource languages.</p><h2 id="cross-lingual-representations">Cross-lingual Representations</h2><p>Whereas in the past it was necessary to learn language-specific models for every language and preprocessing and features needed to be applied to every language in isolation, cross-lingual representation learning enables us to apply the <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g56add75db5_0_104">transfer learning formula</a> of pretraining and adaptation <em>across languages</em>. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/10/transfer_learning_formula.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>The transfer learning formula (<a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g56add75db5_0_104">NAACL 2019 Transfer learning tutorial</a>)</figcaption></figure><!--kg-card-begin: markdown--><p>In this context, we will talk about a (high-resource) source language \(L_1\) where unlabelled data and labelled data for a particular task are available, and a (low-resource) target language \(L_2\)  where only unlabelled data is available. For cross-lingual transfer learning, we add one additional step to the transfer learning recipe above:</p>
<ol>
<li><strong>Pretraining</strong>: Learn <em>cross-lingual</em> representations that are shared across languages. This is what the rest of this post is about.</li>
<li><strong>Adaptation</strong>: Adapt the model to the labelled data of the desired task in \(L_1\) by learning task-specific parameters on top of the cross-lingual representations. The cross-lingual parameters are often kept fixed (this is the same as <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5888218f39_59_11">feature extraction</a>). As the task-specific parameters are learned on top of cross-lingual ones, they are expected to transfer to the other language.</li>
<li><strong>Zero-shot transfer</strong>: The model, which captures cross-lingual information, can be applied directly to perform inference on data of the target language.</li>
</ol>
<!--kg-card-end: markdown--><p>If labelled data in the target language is also available, then the model can be fine-tuned jointly on both languages in Step 2.</p><p>Throughout this post, I will focus on approaches that learn cross-lingual representations as the means for transferring across languages. Depending on the task, there are other ways of transferring information across languages such as by domain adaptation (as seen above), annotation projection (<a href="https://arxiv.org/pdf/1401.5694.pdf">Padó &amp; Lapata, 2009</a>; <a href="https://arxiv.org/abs/1707.02483">Ni et al., 2017</a>; <a href="https://www.aclweb.org/anthology/P19-1172/">Nicolai &amp; Yarowsky, 2019</a>), distant supervision (<a href="https://www.aclweb.org/anthology/D18-1061/">Plank &amp; Agić, 2018</a>) or machine translation (MT; <a href="https://www.aclweb.org/anthology/P16-1133/">Zhou et al., 2016</a>; <a href="https://arxiv.org/abs/1807.08998">Eger et al., 2018</a>).</p><h2 id="why-not-machine-translation">Why not Machine Translation?</h2><p>In light of the recent success of <a href="https://ai.googleblog.com/2019/10/exploring-massively-multilingual.html">massively multilingual</a> and unsupervised machine translation (<a href="https://www.aclweb.org/anthology/D18-1549/">Lample et al., 2018</a>; <a href="https://www.aclweb.org/anthology/P19-1019/">Artetxe et al., 2019</a>), an obvious question is why we would not just use MT to translate the training data in our source language to the target language and train a model on the translated data—or translate the test set and apply our source model directly to it, which often works better in practice. If an MT system is available, this can be a strong baseline (<a href="https://www.aclweb.org/anthology/D18-1269/">Conneau et al., 2018</a>) and unsupervised MT in particular can be useful for low-resource languages (<a href="https://www.aclweb.org/anthology/D18-1549/">Lample et al., 2018</a>).</p><p>However, an MT system for the desired language pair may not be easily available and can be expensive to train. In addition, models trained on machine-translated data often underperform <a href="#deep-unsupervised-models">state-of-the-art deep multilingual models</a>. MT struggles with distant language pairs and domain mismatches (<a href="https://www.aclweb.org/anthology/D19-1632/">Guzmán et al., 2019</a>). It is also not a fit for all tasks; the performance of translation-based models for question answering tasks depends heavily on the translation quality of named entities (<a href="https://www.aclweb.org/anthology/P19-1227/">Liu et al., 2019</a>). For sequence labelling tasks, MT requires projecting annotations across languages, itself a difficult problem (<a href="https://www.aclweb.org/anthology/L18-1344/">Akbik &amp; Vollgraf, 2018</a>).</p><p>In general, however, MT and the methods described in this post are not at odds but complementary. Cross-lingual word representations are used to initialise unsupervised MT (<a href="https://arxiv.org/abs/1902.01313">Artetxe et al., 2019</a>) and cross-lingual representations have been shown to improve neural MT performance (<a href="https://arxiv.org/abs/1901.07291">Lample &amp; Conneau, 2019</a>). Techniques from MT such as word alignment have also inspired much work in cross-lingual representation learning (<a href="https://www.jair.org/index.php/jair/article/view/11640/26511">Ruder et al., 2019</a>). Finally, a multilingual model benefits from being fine-tuned on translations of the labelled data in multiple languages (<a href="https://arxiv.org/abs/1911.02116">Conneau et al., 2019</a>).</p><h1 id="main-framework">Main Framework</h1><p>We now discuss how to learn unsupervised representations, starting at the word level. The main framework for learning cross-lingual word embeddings (CLWEs) consists of <em>mapping-based</em> or <em>post-hoc alignment</em> approaches. For an overview of other approaches, have a look at <a href="https://www.jair.org/index.php/jair/article/view/11640/26511">our survey</a> or <a href="https://www.morganclaypool.com/doi/abs/10.2200/S00920ED2V01Y201904HLT042">book</a> on the topic.</p><p>Mapping-based methods are currently the preferred approach due to their ease and efficiency of use and conceptual simplicity. They only require monolingual word embeddings trained in each language and then simply learn a linear projection that maps between the embedding spaces. The general framework can be seen in the Figure below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/10/projection_framework.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>A general framework for projection-based cross-lingual word embeddings (<a href="https://www.aclweb.org/anthology/P19-1070/">Glavaš et al., 2019</a>)</figcaption></figure><!--kg-card-begin: markdown--><p>Given monolingual embedding spaces \(\mathbf{X}_{L_1} = \{\mathbf{x}^i_{L_1} \}^{|V_{L_1}|}_{i=1} \) and \(\mathbf{X}_{L_2} = \{\mathbf{x}^j_{L_2} \}^{|V_{L_2}|}_{j=1} \) in two languages where \(\mathbf{x}\) is a word embedding and \(|V|\) is the size of the vocabulary, we perform the following steps (<a href="https://www.aclweb.org/anthology/P19-1070/">Glavaš et al., 2019</a>):</p>
<ol>
<li><strong>Construct a seed translation dictionary</strong>. The dictionary \(D = \{w^i_{L_1}, w^j_{L_2} \}^K\) contains \(K\) pairs of words and their translations.</li>
<li><strong>Create word-aligned monolingual subspaces</strong>. We create subspaces \(\mathbf{X}_S = \{\mathbf{x}^i_{L_1}\}^K_{i=1} \) and \(\mathbf{X}_T = \{\mathbf{x}^j_{L_2}\}^K_{j=1} \) where \(\mathbf{X}_S \in \mathbb{R}^{K \times d}\) and \(\mathbf{X}_T \in \mathbb{R}^{K \times d}\) where \(d\) is the dimensionality of the word embeddings. We do this simply by looking up the vectors of the words and their translations in the monolingual embedding spaces, \( \{w^i_{L_1} \} \) from \(\mathbf{X}_{L_1}\) and \( \{w^j_{L_2} \} \) from \(\mathbf{X}_{L_2}\) respectively.</li>
<li><strong>Learn a mapping between the subspaces to a common space</strong>. Specifically, we project the matrices \(\mathbf{X}_S\) and \(\mathbf{X}_T\) into a common space \(\mathbf{X}_{CL}\). In the general setting, we learn two projection matrices \(\mathbf{W}_{L_1} \in \mathbb{R}^{d \times d} \) and \(\mathbf{W}_{L_2} \in \mathbb{R}^{d \times d}\) to project \(\mathbf{X}_{L_1}\) and \(\mathbf{X}_{L_2}\) respectively to the shared space.</li>
</ol>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>We often treat one of the monolingual spaces \(\mathbf{X}_{L_1}\) (typically the English space) as the cross-lingual space \(\mathbf{X}_{CL}\) and only learn a mapping \(\mathbf{W}_{L_2}\) from \(\mathbf{X}_{L_2}\) to this space. The standard approach for learning this mapping is to minimise the Euclidean distance between the representations of words \( \mathbf{X}_T \) and their projected translations \( \mathbf{X}_S\mathbf{W} \) in the dictionary \(D\) (<a href="https://arxiv.org/abs/1309.4168">Mikolov et al., 2013</a>):</p>
<p>\(\mathbf{W}_{L_2} = \arg \min_{\mathbf{W}} \| \mathbf{X}_S\mathbf{W} - \mathbf{X}_T \|_2 \)</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>After having learned this mapping, we can now project a word embedding \(\mathbf{x}_{L_2}\) from \(\mathbf{X}_{L_2}\) simply as \(\mathbf{W}_{L_2} \mathbf{x}_{L_2} \) to the space of \(\ \mathbf{X}_{L_1}\). We can see how this mapping looks like geometrically below.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/mapping_visualisation.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>Visualisation of the mapping-based approach between Swahili and English subspaces (inspired by slides from Eneko Agirre &amp; Mikel Artetxe)</figcaption></figure><!--kg-card-begin: markdown--><p>More complex mappings (such as with a deep neural network) have been observed to lead to poorer performance (<a href="https://arxiv.org/abs/1309.4168">Mikolov et al., 2013</a>). Better word translation results can be achieved when \(\mathbf{W}_{L_2}\) is constrained to be orthogonal (<a href="https://www.aclweb.org/anthology/N15-1104/">Xing et al., 2015</a>, <a href="https://www.aclweb.org/anthology/D16-1250/">Artetxe et al., 2016</a>). Intuitively, this ensures that when \(\mathbf{W}_{L_2}\) vectors are projected to the \(\mathbf{X}_{CL}\) space that the structure of the monolingual embedding space is preserved. If \(\mathbf{W}_{L_2}\) is orthogonal, then the optimisation problem is known as the so-called Procrustes problem (<a href="https://link.springer.com/article/10.1007/BF02289451">Schönemann, 1966</a>), which has a closed form solution:</p>
<p>\(<br>
\begin{align}<br>
\begin{split}<br>
\mathbf{W}_{L_2} &amp; = \mathbf{UV}^\top, \text{with} \\<br>
\mathbf{U \Sigma V}^\top &amp; = SVD(\mathbf{X}_T \mathbf{X}_S{}^\top)<br>
\end{split}<br>
\end{align}<br>
\)</p>
<!--kg-card-end: markdown--><p>Almost all projection-based methods, whether they are supervised or unsupervised, solve the Procrustes problem to learn a linear mapping in Step 3. The main part where they differ is in how they obtain the initial seed dictionary in Step 1. Recent supervised approaches (<a href="https://www.aclweb.org/anthology/P17-1042/">Artetxe et al., 2017</a>) have been shown to achieve reasonable performance using dictionaries of only 25-40 translation pairs. The main idea that enables learning from such limited supervision is bootstrapping or self-learning. </p><h1 id="self-learning">Self-learning</h1><p>Starting from a seed dictionary, recent approaches employ a <a href="https://docs.google.com/presentation/d/1miPu0hkBFSRTAxP-Psg0xXOvAbVSJPhQXehMQdUI88I/edit#slide=id.g5d7e0f0964_0_0">self-learning procedure</a> (<a href="https://www.aclweb.org/anthology/P17-1042/">Artetxe et al., 2017</a>), which can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/10/self-learning_loop_wide.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>The self-learning loop (Credit: Eneko Agirre)</figcaption></figure><!--kg-card-begin: markdown--><p>We learn an initial mapping based on the seed dictionary and use this mapping to project \(\mathbf{X}_{L_2}\) vectors to the target space \(\mathbf{X}_{CL}\). In this cross-lingual space, \(L_1\) words should be close to their translations in \(L_2\). We can now use the \(L_1\) words and their nearest neighbours in \(L_2\) to build our new dictionary. In practice, people often only consider frequent words and mutual nearest neighbours, i.e. words that are nearest neighbours from both \(L_1\) and \(L_2\) directions (<a href="https://www.aclweb.org/anthology/P16-1024/">Vulić et al., 2016</a>; <a href="https://openreview.net/forum?id=H196sainb">Lample et al., 2018</a>).</p>
<!--kg-card-end: markdown--><p>The framework that is typically used in practice consists of additional components (<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16935/16781">Artetxe et al., 2018</a>) that can be seen in the Figure below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/cross-lingual_framework_landscape.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>The general unsupervised cross-lingual word embedding framework (<a href="https://arxiv.org/abs/1909.01638">Vulić et al., 2019</a>)</figcaption></figure><p>In particular, beyond the induction of the initial seed dictionary (C1), the learning of the projections, and the self-learning (C3), there are several pre- and post-processing steps that are often used in practice. You can have a look at <a href="https://docs.google.com/presentation/d/1miPu0hkBFSRTAxP-Psg0xXOvAbVSJPhQXehMQdUI88I/edit#slide=id.g5bb054856f_1_193">the slides</a> or refer to <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16935/16781">Artetxe et al. (2018)</a> for an overview.</p><h1 id="unsupervised-seed-induction">Unsupervised Seed Induction</h1><!--kg-card-begin: markdown--><p>Unsupervised seed induction aims to find a good initial dictionary that we can use to bootstrap the self-learning loop. We can differentiate approaches based on whether they learn an initial mapping by minimising the distance between the \(\mathbf{X}_{L_1}\) or \(\mathbf{X}_{L_2}\) spaces—often with an adversarial component—or whether they rely on a non-adversarial approach.</p>
<!--kg-card-end: markdown--><h2 id="adversarial-approaches">Adversarial approaches</h2><!--kg-card-begin: markdown--><p>Adversarial approaches are inspired by <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">generative adversarial networks (GANs)</a>. The generator is parameterised by our projection matrix \(\mathbf{W}_{L_2}\) while the discriminator is a separate neural network that tries to discriminate between true embeddings from \(\mathbf{X}_{L_2}\) and projected embeddings \(\mathbf{W}_{L_1} \mathbf{X}_{L_1}\) as can be seen below.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/10/adversarial_mapping.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>The adversarial approach for learning cross-lingual word embeddings</figcaption></figure><!--kg-card-begin: markdown--><p>In order to fool the discriminator, the generator has to transform \(\mathbf{X}_{L_1}\) in such a way that it matches the distribution of \(\mathbf{X}_{L_2}\). The underlying hypothesis is that the transformation that makes the distributions as similar as possible also puts words close to their translations in the cross-lingual space \(\mathbf{X}_{CL}\). GANs can be notoriously brittle. The first approach that worked more robustly in certain settings was proposed by <a href="https://arxiv.org/abs/1710.04087">Conneau et al. (2018)</a>.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>Other ways have been proposed to minimise the distance between the distributions of \(\mathbf{X}_{L_1}\) and \(\mathbf{X}_{L_2}\) that leverage Wasserstein GANs, Earth Mover's distance, and optimal transport. For more details about these approaches and their connections, have a look at <a href="https://docs.google.com/presentation/d/1miPu0hkBFSRTAxP-Psg0xXOvAbVSJPhQXehMQdUI88I/edit#slide=id.g5edc0b865a_3_4040">the corresponding slides</a>.</p>
<!--kg-card-end: markdown--><h2 id="weak-supervision-and-second-order-similarity">Weak supervision and second-order similarity</h2><p>Another strategy for obtaining a seed lexicon is through the use of heuristics. If the writing systems of two languages share the same alphabet, then there will be many words that are spelled the same in both languages, many of them named entities such as "New York" and "Barack Obama". A list of pairs of such identically spelled words can be used as a weakly supervised seed dictionary (<a href="https://www.aclweb.org/anthology/P18-1072/">Søgaard et al., 2018</a>). Even if languages do not share alphabets, many languages still use Arabic numerals, which can be used as a seed dictionary (<a href="https://www.aclweb.org/anthology/P17-1042/">Artetxe et al., 2017</a>).</p><p>Such a weakly supervised seed dictionary is available for most language pairs and works surprisingly well, often outperforming purely unsupervised approaches. Intuitively, despite being noisy, a dictionary consisting of identically spelled words provides a sufficient number of anchor points for learning an initial mapping.</p><p>Still, it would be useful to have a seed dictionary that is independent of a language's writing system. Rather than requiring that translations should be similar (e.g. spelled the same), we can go one step further and require that translations should be similar to other words across languages <em>in the same way</em> (<a href="https://www.aclweb.org/anthology/P18-1073/">Artetxe et al. 2018</a>). In other words, the similarity distributions of translations within each language should be similar.</p><p>Specifically, if we calculate the cosine similarity of one word with all other words in the same language, sort the values, and normalise, we obtain the following distributions for "two", "due" and "cane":</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/10/intralingual_similarity_distributions.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>Intralingual similarity distributions of three words (<a href="https://www.aclweb.org/anthology/P18-1073/">Artetxe et al., 2018</a>).</figcaption></figure><p>The intralingual similarity distributions of translations such as "two" and "due" should be more similar compared to other words such as "cane". Based on this insight, we can use the nearest neighbours of our similarity distributions across languages as the initial dictionary. This notion of second-order similarity (the similarity of similarity distributions) is a powerful concept and is also leveraged by distance metrics such as the <a href="https://docs.google.com/presentation/d/1miPu0hkBFSRTAxP-Psg0xXOvAbVSJPhQXehMQdUI88I/edit#slide=id.g5d7e0f0964_9_50">Gromov-Wasserstein distance</a>.</p><h1 id="systematic-comparisons">Systematic Comparisons</h1><p>Unsupervised cross-lingual word embeddings are theoretically interesting. They can teach us how to better leverage monolingual data across languages. We can also use them to learn more about how different languages relate to each other.</p><p>However, supervision in the form of translation pairs is available for many languages and weak supervision is readily available. Unsupervised methods are thus only of practical interest if they are able to outperform to their supervised counterparts.</p><p>Initial papers claimed that unsupervised methods are indeed competitive and even outperform the basic supervised Procrustes method. Recent studies, however, control for the <a href="#self-learning">additional components in the framework</a> and compare unsupervised and supervised methods on equal footing. The picture that has emerged is that current unsupervised methods generally underperform supervised methods with 500-1,000 seed translation pairs (<a href="https://www.aclweb.org/anthology/P19-1070/">Glavaš et al., 2019</a>; <a href="https://www.aclweb.org/anthology/D19-1449/">Vulić et al., 2019</a>) as can be seen in the Figure below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/bli_performance_vs_seed_dict_size_landscape.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>Comparison of BLI performances of a fully unsupervised method (UNSUPER), a supervised method (SUPER), and supervised methods with self-learning (+SL +NOD, +SL +SYM; <a href="https://www.aclweb.org/anthology/D19-1449/">Vulić et al., 2019</a>).</figcaption></figure><p>Moreover, even the most robust unsupervised mapping-based approaches fail for many language pairs. This has to do with the topology of the monolingual embedding spaces: Unsupervised methods rely on the assumption that the embedding spaces are approximately isomorphic, i.e. that they have similar structure  (<a href="https://www.aclweb.org/anthology/P18-1072/">Søgaard et al., 2018</a>). If this is not the case, then the unsupervised seed induction step fails.</p><p>The structure of embedding spaces can be different if embeddings belong to dissimilar languages but also if they were trained on different domains or using different algorithms (<a href="https://www.aclweb.org/anthology/P18-1072/">Søgaard et al., 2018</a>; <a href="https://www.aclweb.org/anthology/D18-1056/">Hartmann et al., 2018</a>). Different methods such as a combination of several independent linear maps (<a href="https://www.aclweb.org/anthology/D18-1047">Nakashole, 2018)</a>, iterative normalisation (<a href="https://arxiv.org/pdf/1906.01622.pdf">Zhang et al., 2019)</a>, or incrementally adding new languages to the multilingual space (<a href="https://www.aclweb.org/anthology/N19-1188/">Heymann et al., 2019</a>) have been proposed to align such non-isomorphic embedding spaces. Recent work has also shown that—controlling for additional components—the standard GAN approach is competitive with more advanced GANs such as Wasserstein GAN and with the second-order similarity initialisation (<a href="https://github.com/coastalcph/stepbystep/blob/master/poster.pdf">Hartmann et al., 2019</a>).</p><h1 id="unsupervised-deep-models">Unsupervised Deep Models</h1><p>Supervised deep models that learn from parallel sentences or documents have been proposed before (see Sections 7 and 8 of <a href="https://www.jair.org/index.php/jair/article/view/11640/26511">this survey</a>). In light of the success of pretrained language models, similar techniques have recently been applied to train unsupervised deep cross-lingual representations. </p><h2 id="joint-models">Joint models</h2><p>The most prominent example in this line of work is <a href="https://github.com/google-research/bert/blob/master/multilingual.md">multilingual BERT</a> (mBERT), a BERT-base model that was jointly trained on the corpora of 104 languages with a shared vocabulary of 110k subword tokens.</p><p>mBERT is trained using masked language modelling (MLM)—with no explicit supervision—but has nevertheless been shown to learn cross-lingual representations that generalise surprisingly well to other languages via zero-shot transfer (<a href="https://www.aclweb.org/anthology/P19-1493/">Pires et al., 2019</a>; <a href="https://www.aclweb.org/anthology/D19-1077/">Wu &amp; Dredze, 2019</a>). This generalisation ability has been attributed to three factors: 1) identical subwords in the shared vocabulary acting as anchor points for learning an alignment (similar to the weak supervision of CLWEs); 2) joint training across multiple languages that spreads this effect; and 3) deep cross-lingual representations that go beyond vocabulary memorisation and generalise across languages.</p><p>Recent work (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>; <a href="https://openreview.net/forum?id=HJeT3yrtDr">Anonymous et al., 2019</a>; <a href="https://arxiv.org/abs/1911.01464">Wu et al., 2019</a>), however, shows that a shared vocabulary is not required for learning unsupervised deep cross-lingual representations. <a href="https://arxiv.org/abs/1910.11856">Artetxe et al. (2019)</a> additionally demonstrate that joint training is unnecessary and identify the vocabulary size per language as an important factor: Multilingual models with larger vocabulary sizes consistently perform better.</p><p>Extensions to mBERT augment it with a supervised objective (<a href="https://arxiv.org/abs/1901.07291">Lample and Conneau, 2019</a>), which is inspired by <a href="https://www.jair.org/index.php/jair/article/view/11640/26511">bilingual skip-gram approaches</a> and can be seen in the Figure below. Others add auxiliary pre-training tasks such as cross-lingual word recovery and paraphrase detection (<a href="https://arxiv.org/abs/1909.00964">Huang et al., 2019</a>), encourage representations of translations to be similar (<a href="https://openreview.net/forum?id=r1xCMyBtPS">Anonymous, 2019</a>), and scale it up (<a href="https://arxiv.org/abs/1911.02116">Conneau et al., 2019</a>). Joint training has been applied not only to Transformers but also to LSTM-based methods where initialisation with CLWEs has been found useful (<a href="https://www.aclweb.org/anthology/N19-1392/">Mulcaire et al., 2019</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/translation_language_modelling.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>The supervised cross-lingual language modelling objective of <a href="https://arxiv.org/abs/1901.07291">Lample and Conneau (2019</a>)</figcaption></figure><h2 id="mapping-based-approaches">Mapping-based approaches</h2><p>The mapping-based approaches that we discussed previously have also been applied to the contextual representations of deep bilingual models. The main assumption behind these methods is that contextual representations—similar to their non-contextual counterparts—can also be aligned via a linear transformation. As can be seen in the Figure below, contextual representations cluster and have similar distributions across languages.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/contextual_aligned_landscape.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>PCA visualisation of contextual embeddings of the English word "bear" and its two possible Spanish translations, "tener" and "oso" in an aligned multilingual space (<a href="https://www.aclweb.org/anthology/N19-1162/">Schuster et al., 2019</a>)</figcaption></figure><!--kg-card-begin: markdown--><p>For contextual representations, the mapping can be done either on the token level or on the type level. On the token level, contextual representations of tokens in both languages can be aligned using word alignment information in parallel data (<a href="https://www.aclweb.org/anthology/N19-1391/">Aldarmaki &amp; Diab, 2019</a>). On the type level, an aggregate representation of a word's contextual representations such as its mean can be used as its type embeddings (<a href="https://www.aclweb.org/anthology/N19-1162/">Schuster et al., 2019</a>). Type embeddings across languages can then be aligned using the same mapping-based approaches as before. <a href="https://www.aclweb.org/anthology/K19-1004/">Liu et al. (2019)</a> perform further analyses and find that type-level alignment performs best. Mapping-based methods have also recently been applied to BERT-based representations (<a href="https://openreview.net/forum?id=S1l-C0NtwS">Anonymous et al., 2019</a>; <a href="https://arxiv.org/abs/1911.01464">Wu et al., 2019</a>).</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>Rather than learning a mapping between two existing pretrained models in \(L_1\) and \(L_2\), we can apply a model that was pretrained in a high-resource language \(L_1\) to a low-resource language \(L_2\) much more easily by simply learning token-level embeddings in \(L_2\) while keeping the model body fixed (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>, <a href="https://openreview.net/forum?id=Bkle6T4YvB">Anonymous et al., 2019</a>). The full process can be seen below. This alignment to a monolingual model may avoid the non-isomorphism of fixed monolingual vector spaces and the resulting model is competitive with jointly trained models.</p>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/monolingual_transfer.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>The four steps of the monolingual transfer approach of <a href="https://arxiv.org/abs/1910.11856">Artetxe et al. (2019</a>)</figcaption></figure><h1 id="future-directions">Future Directions</h1><h2 id="benchmarks">Benchmarks</h2><p>A standard task for evaluating CLWEs is bilingual lexicon induction (BLI), which evaluates the quality of the cross-lingual embedding space by determining whether words are nearest neighbours of their translations in a test dictionary. Recently, several researchers have identified issues with the standard MUSE dictionaries (<a href="https://arxiv.org/abs/1710.04087">Conneau et al., 2018</a>) for this task, which only represent frequent words and are not morphologically diverse (<a href="https://www.aclweb.org/anthology/D19-1090/">Czarnowska et al., 2019</a>; <a href="https://www.aclweb.org/anthology/D19-1328/">Kementchedjhieva et al., 2019</a>). BLI performance on words with lower frequency ranks drops off drastically as can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/dictionary_frequency_ranks_landscape.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>BLI performance vs frequency rank of words in MUSE and new dictionaries (<a href="https://arxiv.org/abs/1909.02855">Czarnowska et al., 2019</a>)</figcaption></figure><p>We are ultimately interested in applying our methods to downstream tasks in low-resource languages. While BLI correlates well with certain downstream tasks for mapping-based approaches (<a href="https://www.aclweb.org/anthology/P19-1070/">Glavaš et al., 2019</a>), the correlation is weaker for methods that are not based on an orthogonal transformation. Downstream evaluation is thus important to accurately gauge the performance of novel methods in the future.</p><p>However, even existing cross-lingual downstream tasks have their problems. Cross-lingual document classification, a classic task for the evaluation of cross-lingual representations (<a href="https://www.aclweb.org/anthology/C12-1089/">Klementiev et al., 2012</a>), mostly requires superficial keyword-matching and CLWEs outperform deep representations (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>). A more recent multilingual benchmark, XNLI (<a href="https://www.aclweb.org/anthology/D18-1269/">Conneau et al., 2018</a>) may be plagued by similar artefacts as the MultiNLI dataset from which it has been derived (<a href="https://www.aclweb.org/anthology/N18-2017/">Gururangan et al., 2018</a>).</p><p>On the whole, we need more challenging benchmarks for evaluating multilingual models. Question answering might be a good candidate as it is a common probe for language understanding and has been shown to be harder to exploit. Recently, several cross-lingual question answering datasets have been proposed, including XQuAD (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>), which extends SQuAD 1.1. to ten other languages, the open-domain XQA (<a href="https://www.aclweb.org/anthology/P19-1227/">Liu et al., 2019</a>), and MLQA (<a href="https://arxiv.org/abs/1910.07475">Lewis et al., 2019</a>).</p><h2 id="understanding-representations">Understanding representations</h2><p>Benchmarks are also important for better understanding the representations that our models learn. Along these lines, it is still unclear what makes monolingual embedding spaces non-isomorphic. Even embedding spaces between similar languages are not strictly isomorphic as can be seen below. Similar issues might affect the spaces of deep contextual representations. Identifying the cause of this phenomenon may help us gain a better understanding of our learning algorithms and the biases they encode in their representations. It may also shed light on the semantic similarities and differences between languages.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/nn_graphs_freq_words_landscape.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>Nearest neighbour graphs of 10 most frequent nouns in English and their German translations (<a href="https://www.aclweb.org/anthology/P18-1072">Søgaard et al., 2018</a>)</figcaption></figure><p>There have been several studies analysing what CLWEs learn but it is still mostly unclear what representations learned by unsupervised deep multilingual models capture. As token-level alignment to a deep monolingual model achieves competitive results (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>), deep multilingual models may still mostly learn lexical alignment. If this is the case, we need better methods that incentivise learning deep multilingual representations. Parallel data improves performance in practice (<a href="https://arxiv.org/abs/1901.07291">Lample &amp; Conneau, 2019</a>), but for low-resource languages we would like to use as little supervision as possible.</p><p>Deep representations from an English BERT model can be transferred to another language without any modification (besides learning new token embeddings; <a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>). Consequently, it should be interesting to analyse what cross-lingual information monolingual models capture. In order to gain new insights, however, we need better benchmarks and evaluation protocols for the analysis of cross-lingual knowledge.</p><h2 id="practical-considerations">Practical considerations</h2><p>In the end, we'd like to learn cross-lingual representations in order to apply them to downstream tasks in low-resource languages. In light of this, we should not forget practical concerns of such low-resource scenarios. While zero-shot transfer is a focus of current methods, <a href="http://nlp.fast.ai/classification/2019/09/10/multifit.html">zero-shot transfer is not free</a>. It requires access to training data from the same task and distribution in a high-resource language. We should thus continue investing in approaches that can be trained with few samples in a target language and that combine the best of both the monolingual and cross-lingual worlds, for instance via cross-lingual bootstrapping that can be seen below (<a href="https://www.aclweb.org/anthology/D19-1572/">Eisenschlos et al., 2019</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/11/multifit_bootstrapping_landscape.png" class="kg-image" alt="Unsupervised Cross-lingual Representation Learning"><figcaption>Steps of a cross-lingual bootstrapping method for zero-shot cross-lingual transfer (<a href="https://www.aclweb.org/anthology/D19-1572/">Eisenschlos et al., 2019</a>)</figcaption></figure><p>In addition, recent deep multilingual models are large and expensive to train. Training smaller multilingual models, for instance via distillation (<a href="https://www.aclweb.org/anthology/D19-1374/">Tsai et al., 2019</a>), will be key for deployment in resource-constrained scenarios.</p><p>Recent wide-coverage parallel corpora such as JW-300 (<a href="https://www.aclweb.org/anthology/P19-1310/">Agić &amp; Vulić, 2019</a>) and WikiMatrix (<a href="https://arxiv.org/abs/1907.05791">Schwenk et al., 2019</a>) enable the training of massively multilingual or supervised systems for many new low-resource languages where parallel data was previously not available. However, these resources still do not cover languages where only few unlabelled data is available. In order to apply our methods to tasks in such languages, we need to develop approaches that can learn representations from a limited number of samples. Resources that provide limited data (mostly dictionaries) for a large number of languages are the <a href="https://asjp.clld.org">ASJP database</a>, <a href="https://panlex.org/">PanLex</a>, and <a href="https://babelnet.org/">BabelNet</a>.</p><p>Finally, while CLWEs underperform deep multilingual models on more challenging benchmarks (<a href="https://arxiv.org/abs/1910.11856">Artetxe et al., 2019</a>), they are still preferred for easier tasks such as document classification due to their efficiency and ease of use. In addition, they serve as a useful initialisation to kick-start the training of deep models (<a href="https://www.aclweb.org/anthology/N19-1392/">Mulcaire et al., 2019</a>; <a href="https://openreview.net/forum?id=Bkle6T4YvB">Anonymous et al., 2019</a>) and of unsupervised NMT models (<a href="https://arxiv.org/abs/1902.01313">Artetxe et al., 2019</a>). Consequently, they may be a useful foundation for the development of more resource and parameter-efficient deep multilingual models.</p><h2 id="citation">Citation</h2><p>If you found this post helpful, consider citing <a href="https://www.aclweb.org/anthology/P19-4007">the tutorial</a> as:</p><pre><code>@inproceedings{ruder2019unsupervised,
  title={Unsupervised Cross-Lingual Representation Learning},
  author={Ruder, Sebastian and S{\o}gaard, Anders and Vuli{\'c}, Ivan}, 
  booktitle={Proceedings of ACL 2019, Tutorial Abstracts},
  pages={31--38},
  year={2019}
}</code></pre>]]></content:encoded></item><item><title><![CDATA[The State of Transfer Learning in NLP]]></title><description><![CDATA[This post expands on the NAACL 2019 tutorial on Transfer Learning in NLP. It highlights key insights and takeaways and provides updates based on recent work.]]></description><link>http://ruder.io/state-of-transfer-learning-in-nlp/</link><guid isPermaLink="false">5c76ff7c1b9b0d18555b9eb3</guid><category><![CDATA[transfer learning]]></category><category><![CDATA[natural language processing]]></category><category><![CDATA[events]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Sun, 18 Aug 2019 15:22:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2019/08/transfer_learning_methods_small.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2019/08/transfer_learning_methods_small.png" alt="The State of Transfer Learning in NLP"><p>Update 16.10.2020: Added <a href="https://www.infoq.cn/article/zD5QkcIzF9253friWPVd">Chinese</a> and <a href="https://www.ibidemgroup.com/edu/traduccion-aprendizaje-pnl/">Spanish</a> translations.</p><p>This post expands on the <a href="http://tiny.cc/NAACLTransfer">NAACL 2019 tutorial on Transfer Learning in NLP</a>. The tutorial was organized by Matthew Peters, Swabha Swayamdipta, Thomas Wolf, and me. In this post, I highlight key insights and takeaways and provide updates based on recent work. You can see the structure of this post below:</p><figure class="kg-card kg-image-card"><img src="http://ruder.io/content/images/2019/08/agenda.png" class="kg-image" alt="The State of Transfer Learning in NLP"></figure><p>The <a href="http://tiny.cc/NAACLTransfer">slides</a>, a <a href="http://tiny.cc/NAACLTransferColab">Colaboratory notebook</a>, and <a href="http://tiny.cc/NAACLTransferCode">code</a> of the tutorial are available online.</p><h1 id="introduction">Introduction</h1><p>For an overview of what transfer learning is, have a look at <a href="http://ruder.io/transfer-learning/">this blog post</a>. Our go-to definition throughout this post will be the following, which is illustrated in the diagram below:</p><blockquote>Transfer learning is a means to extract knowledge from a source setting and apply it to a different target setting.</blockquote><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/transfer_learning_scenario.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>An illustration of the process of transfer learning.</figcaption></figure><p>In the span of little more than a year, transfer learning in the form of pretrained language models has <a href="https://thegradient.pub/nlp-imagenet/">become ubiquitous in NLP</a> and has contributed to the state of the art on a wide range of tasks. However, transfer learning is not a recent phenomenon in NLP. One illustrative example is progress on the task of Named Entity Recognition (NER), which can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/ner_results.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>Performance on Named Entity Recognition (NER) on CoNLL-2003 (English) over time.</figcaption></figure><p>Throughout its history, most of the major improvements on this task have been driven by different forms of transfer learning: from early self-supervised learning with auxiliary tasks (<a href="http://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf">Ando and Zhang, 2005</a>) and phrase &amp; word clusters (<a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35520.pdf">Lin and Wu, 2009</a>) to the language model embeddings (<a href="https://arxiv.org/pdf/1705.00108.pdf">Peters et al., 2017</a>) and pretrained language models (<a href="https://aclweb.org/anthology/N18-1202">Peters et al., 2018</a>; <a href="https://alanakbik.github.io/papers/coling2018.pdf">Akbik et al., 2018</a>; <a href="https://arxiv.org/abs/1903.07785">Baevski et al., 2019</a>) of recent years.</p><p>There are different types of transfer learning common in current NLP. These can be roughly classified along three dimensions based on a) whether the source and target settings deal with the same task; and b) the nature of the source and target domains; and c) the order in which the tasks are learned. A taxonomy that highlights the variations can be seen below:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/transfer_learning_taxonomy.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>A taxonomy for transfer learning in NLP (<a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=64">Ruder, 2019</a>).</figcaption></figure><p>Sequential transfer learning is the form that has led to the biggest improvements so far. The general practice is to pretrain representations on a large unlabelled text corpus using your method of choice and then to adapt these representations to a supervised target task using labelled data as can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/pretraining_adaptation.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>The general procedure of sequential transfer learning.</figcaption></figure><h3 id="major-themes">Major themes</h3><p>Several major themes can be observed in how this paradigm has been applied:</p><p><strong>From words to words-in-context</strong>  Over time, representations incorporate more context. Early approaches such as word2vec (<a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et al., 2013</a>) learned a single representation for every word independent of its context. Later approaches then scaled these representations to sentences and documents (<a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf">Le and Mikolov, 2014</a>; <a href="https://arxiv.org/abs/1705.02364">Conneau et al., 2017</a>). Current approaches learn word representations that change based on the word's context (<a href="http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf">McCann et al., 2017</a>; <a href="https://aclweb.org/anthology/N18-1202">Peters et al., 2018</a>). </p><p><strong>LM pretraining</strong>   Many successful pretraining approaches are based on variants of language modelling (LM). Advantages of LM are that it does not require any human annotation and that many languages have enough text available to learn reasonable models. In addition, LM is versatile and enables learning both sentence and word representations with a variety of objective functions.</p><p><strong>From shallow to deep</strong>  Over the last years, state-of-the-art models in NLP have become progressively deeper. Up to two years ago, the state of the art on most tasks was a 2-3 layer deep BiLSTM, with machine translation being an outlier with 16 layers (<a href="https://arxiv.org/abs/1609.08144">Wu et al., 2016</a>). In contrast, current models like BERT-Large and GPT-2 consist of 24 Transformer blocks and recent models are even deeper.</p><p><strong>Pretraining vs target task</strong>  The choice of pretraining and target tasks is closely intertwined. For instance, sentence representations are not useful for word-level predictions, while span-based pretraining is important for span-level predictions. On the whole, for the best target performance, it is beneficial to choose a similar pretraining task.</p><h1 id="pretraining">Pretraining</h1><h3 id="why-does-language-modelling-work-so-well">Why does language modelling work so well?</h3><p>The remarkable success of pretrained language models is surprising. One reason for the success of language modelling may be that it is a very difficult task, even for humans. To have any chance at solving this task, a model is required to learn about syntax, semantics, as well as certain facts about the world. Given enough data, a large number of parameters, and enough compute, a model can do a reasonable job. Empirically, language modelling works better than other pretraining tasks such as translation or autoencoding (<a href="https://arxiv.org/abs/1809.10040">Zhang et al. 2018</a>; <a href="https://www.aclweb.org/anthology/P19-1439">Wang et al., 2019</a>).</p><p>A recent predictive-rate distortion (PRD) analysis of human language (<a href="http://socsci.uci.edu/~rfutrell/papers/hahn2019estimating.pdf">Hahn and Futrell, 2019</a>) suggests that human language—and language modelling—has infinite statistical complexity but that it can be approximated well at lower levels. This observation has two implications: 1) We can obtain good results with comparatively small models; and 2) there is a lot of potential for scaling up our models. For both implications we have empirical evidence, as we can see in the next sections.</p><h3 id="sample-efficiency">Sample efficiency</h3><p>One of the main benefits of pretraining is that it reduces the need for annotated data. In practice, transfer learning has often been shown to achieve similar performance compared to a non-pretrained model with 10x fewer examples or more as can be seen below for ULMFiT (<a href="https://aclweb.org/anthology/P18-1031">Howard and Ruder, 2018</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/sample_efficiency.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>Performance of a model trained from scratch (blue) vs. two pretrained models fine-tuned on labelled target data (orange) as well as unlabelled target data (green) respectively (<a href="https://aclweb.org/anthology/P18-1031">Howard and Ruder, 2018</a>).</figcaption></figure><h3 id="scaling-up-pretraining">Scaling up pretraining</h3><p>Pretrained representations can generally be improved by jointly increasing the number of model parameters and the amount of pretraining data. Returns start to diminish as the amount of pretraining data grows huge. Current performance curves such as the one below, however, do not indicate that we have reached a plateau. We can thus expect to see even bigger models trained on more data. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/scaling_up_pretraining.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>Average GLUE score with different amounts of Common Crawl data for pretraining (<a href="https://arxiv.org/abs/1903.07785">Baevski et al., 2019</a>).&nbsp;</figcaption></figure><p>Recent examples of this trend are <a href="https://arxiv.org/abs/1907.12412">ERNIE 2.0</a>, <a href="https://arxiv.org/abs/1906.08237">XLNet</a>, <a href="https://devblogs.nvidia.com/training-bert-with-gpus/">GPT-2 8B</a>, and <a href="https://arxiv.org/abs/1907.11692">RoBERTa</a>. The latter in particular finds that simply training BERT for longer and on more data improves results, while GPT-2 8B reduces perplexity on a language modelling dataset (though only by a comparatively small factor).</p><h3 id="cross-lingual-pretraining">Cross-lingual pretraining</h3><p>A major promise of pretraining is that it can help us bridge the digital language divide and can enable us learn NLP models for more of the world's 6,000 languages. Much work on cross-lingual learning has focused on training separate word embeddings in different languages and learning to align them (<a href="https://www.jair.org/index.php/jair/article/view/11640">Ruder et al., 2019</a>). In the same vein, we can learn to align contextual representations (<a href="https://www.aclweb.org/anthology/N19-1162">Schuster et al., 2019</a>). Another common method is to share a subword vocabulary and train one model on many languages (<a href="https://www.aclweb.org/anthology/N19-1423">Devlin et al., 2019</a>; <a href="https://arxiv.org/abs/1812.10464">Artetxe and Schwenk, 2019</a>; <a href="https://www.aclweb.org/anthology/N19-1392">Mulcaire et al., 2019</a>; <a href="https://arxiv.org/abs/1901.07291">Lample and Conneau, 2019</a>). While this is easy to implement and is a strong cross-lingual baseline, it leads to under-representation of low-resource languages (<a href="https://www.aclweb.org/anthology/P19-1027">Heinzerling and Strube, 2019</a>). Multilingual BERT in particular has been the subject of much recent attention (<a href="https://www.aclweb.org/anthology/P19-1493">Pires et al., 2019</a>; <a href="https://arxiv.org/abs/1904.09077">Wu and Dredze, 2019</a>). Despite its strong zero-shot performance, dedicated monolingual language models often are competitive, while being more efficient (<a href="http://ruder.io/publications/">Eisenschlos et al., 2019</a>).</p><h3 id="practical-considerations">Practical considerations</h3><p>Pretraining is cost-intensive. Pretraining the Transformer-XL style model we used in the tutorial takes 5h–20h on 8 V100 GPUs (a few days with 1 V100) to reach a good perplexity. Sharing pretrained models is thus very important. Pretraining is relatively robust to the choice of hyper-parameters—apart from needing a learning rate warm-up for transformers. As a general rule, your model should not have enough capacity to overfit if your dataset is large enough. Masked language modeling (as in BERT) is typically 2-4 times slower to train than standard LM as masking only a fraction of words yields a smaller signal.</p><h1 id="what-is-in-a-representation">What is in a representation?</h1><p>Representations have been shown to be predictive of certain linguistic phenomena such as alignments in translation or syntactic hierarchies. Better performance has been achieved when pretraining with syntax; even when syntax is not explicitly encoded, representations still learn some notion of syntax (<a href="https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00019">Williams et al. 2018</a>). Recent work has furthermore shown that knowledge of syntax can be distilled efficiently into state-of-the-art models (<a href="https://www.aclweb.org/anthology/P19-1337">Kuncoro et al., 2019</a>). Network architectures generally determine what is in a representation. For instance, BERT has been observed to capture syntax (<a href="https://arxiv.org/abs/1905.05950">Tenney et al., 2019</a>; <a href="https://arxiv.org/abs/1901.05287">Goldberg, 2019</a>). Different architectures show different layer-wise trends in terms of what information they capture (<a href="https://www.aclweb.org/anthology/N19-1112">Liu et al., 2019</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/probing_setup.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>The general setup in probing tasks used to study linguistic knowledge within contextual word representations (<a href="https://www.aclweb.org/anthology/N19-1112">Liu et al., 2019</a>).</figcaption></figure><p>The information that a model captures also depends how you look at it: Visualizing activations or attention weights provides a bird's eye view of the model's knowledge, but focuses on a few samples; probes that train a classifier on top of learned representations in order to predict certain properties (as can be seen above) discover corpus-wide specific characteristics, but may introduce their own biases; finally, network ablations are great for improving the model, but may be task-specific.</p><h1 id="adaptation">Adaptation</h1><p>For adapting a pretrained model to a target task, there are several orthogonal directions we can make decisions on: architectural modifications, optimization schemes, and whether to obtain more signal.</p><h2 id="architectural-modifications">Architectural modifications</h2><p>For architectural modifications, the two general options we have are:</p><p><strong>a) Keep the pretrained model internals unchanged</strong>  This can be as simple as adding one or more linear layers on top of a pretrained model, which is commonly done with BERT. Instead, we can also use the model output as input to a separate model, which is often beneficial when a target task requires interactions that are not available in the pretrained embedding, such as span representations or modelling cross-sentence relations.</p><p><strong>b) Modify the pretrained model internal architecture</strong>  One reason why we might want to do this is in order to adapt to a structurally different target task such as one with several input sequences. In this case, we can use the pretrained model to initialize as much as possible of a structurally different target task model. We might also want to apply task-specific modifications such as adding skip or residual connections or attention. Finally, modifying the target task parameters may reduce the number of parameters that need to be fine-tuned by adding bottleneck modules (“adapters”) between the layers of the pretrained model (<a href="https://arxiv.org/abs/1902.00751">Houlsby et al., 2019</a>; <a href="https://arxiv.org/abs/1902.02671">Stickland and Murray, 2019</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/adapter_layer_small.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>An adapter layer (right) as used in a Transformer block (left) (<a href="https://arxiv.org/abs/1902.00751">Houlsby et al., 2019</a>).</figcaption></figure><h2 id="optimization-schemes">Optimization schemes</h2><p>In terms of optimizing the model, we can choose which weights we should update and how and when to update those weights.</p><h3 id="which-weights-to-update">Which weights to update</h3><p>For updating the weights, we can either tune or not tune (the pretrained weights):</p><p><strong>a) Do not change the pretrained weights (feature extraction)</strong>  In practice, a linear classifier is trained on top of the pretrained representations. The best performance is typically achieved by using the representation not just of the top layer, but learning a linear combination of layer representations (<a href="https://arxiv.org/abs/1802.05365">Peters et al., 2018</a>, <a href="https://arxiv.org/abs/1705.08142">Ruder et al., 2019</a>). Alternatively, pretrained representations can be used as features in a downstream model. When adding adapters, only the adapter layers are trained.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/feature_extraction.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>Use of a pretrained model as features in a separate downstream model.</figcaption></figure><p><strong>b) Change the pretrained weights (fine-tuning)</strong>  The pretrained weights are used as initialization for parameters of the downstream model. The whole pretrained architecture is then trained during the adaptation phase.</p><h3 id="how-and-when-to-update-the-weights">How and when to update the weights</h3><p>The main motivation for choosing the order and how to update the weights is that we want to avoid overwriting useful pretrained information and maximize positive transfer. Related to this is the concept of catastrophic forgetting (<a href="https://www.sciencedirect.com/science/article/pii/S0079742108605368">McCloskey &amp; Cohen, 1989</a>; <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.480.7627&amp;rep=rep1&amp;type=pdf">French, 1999</a>), which occurs if a model forgets the task it was originally trained on. In most settings, we only care about the performance on the target task, but this may differ depending on the application.</p><p>A guiding principle for updating the parameters of our model is to update them progressively from top-to-bottom in time, in intensity, or compared to a pretrained model:</p><p><strong>a) Progressively in time (freezing)</strong>  The main intuition is that training all layers at the same time on data of a different distribution and task may lead to instability and poor solutions. Instead, we train layers individually to give them time to adapt to the new task and data. This goes back to layer-wise training of early deep neural networks (<a href="https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">Hinton et al., 2006</a>; <a href="https://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf">Bengio et al., 2007</a>). Recent approaches (<a href="https://www.aclweb.org/anthology/D17-1169">Felbo et al., 2017</a>; <a href="https://arxiv.org/abs/1801.06146">Howard and Ruder, 2018</a>; <a href="https://arxiv.org/abs/1902.10547">Chronopoulou et al., 2019</a>) mostly vary in the combinations of layers that are trained together; all train all parameters jointly in the end. Unfreezing has not been investigated in detail for Transformer models.</p><p><strong>b) Progressively in intensity (lower learning rates)</strong>  We want to use lower learning rates to avoid overwriting useful information. Lower learning rates are particularly important in lower layers (as they capture more general information), early in training (as the model still needs to adapt to the target distribution), and late in training (when the model is close to convergence). To this end, we can use discriminative fine-tuning (<a href="https://aclweb.org/anthology/P18-1031">Howard and Ruder, 2018</a>), which decays the learning rate for each layer as can be seen below. In order to maintain lower learning rates early in training, a triangular learning rate schedule can be used, which is also known as learning rate warm-up in Transformers. <a href="https://arxiv.org/abs/1908.03265">Liu et al. (2019)</a> recently suggest that warm-up reduces variance in the early stage of training.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/discriminative_fine_tuning-1.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>Discriminative fine-tuning (<a href="https://aclweb.org/anthology/P18-1031">Howard and Ruder, 2018</a>).</figcaption></figure><p><strong>c) Progressively vs. a pretrained model (regularization)</strong>  One way to minimize catastrophic forgetting is to encourage target model parameters to stay close to the parameters of the pretrained model using a regularization term (<a href="https://www.aclweb.org/anthology/K17-1029">Wiese et al., CoNLL 2017</a>, <a href="https://www.pnas.org/content/114/13/3521">Kirkpatrick et al., PNAS 2017</a>).</p><h2 id="trade-offs-and-practical-considerations">Trade-offs and practical considerations</h2><p>In general, the more parameters you need to train from scratch the slower your training will be. Feature extraction requires adding more parameters than fine-tuning (<a href="https://arxiv.org/abs/1903.05987">Peters et al., 2019</a>), so is typically slower to train. Feature extraction, however, is more space-efficient when a model needs to be adapted to many tasks as it only requires storing one copy of the pretrained model in memory. Adapters strike a balance by adding a small number of additional parameters per task.</p><p>In terms of performance, no adaptation method is clearly superior in every setting. If source and target tasks are dissimilar, feature extraction seems to be preferable (<a href="https://arxiv.org/abs/1903.05987">Peters et al., 2019</a>). Otherwise, feature extraction and fine-tuning often perform similar, though this depends on the budget available for hyper-parameter tuning (fine-tuning may often require a more extensive hyper-parameter search). Anecdotally, Transformers are easier to fine-tune (less sensitive to hyper-parameters) than LSTMs and may achieve better performance with fine-tuning.</p><p>However, large pretrained models (e.g. BERT-Large) are prone to degenerate performance when fine-tuned on tasks with small training sets. In practice, the observed behavior is often “on-off”: the model either works very well or does not work at all as can be seen in the figure below. Understanding the conditions and causes of this behavior is an open research question.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/08/sentence_encoders_on_stilts_5k_examples.png" class="kg-image" alt="The State of Transfer Learning in NLP"><figcaption>Distribution of task scores across 20 random restarts for BERT (red) and BERT that was fine-tuned on MNLI (green) when fine-tuning on no more than 5k examples for each task (<a href="https://arxiv.org/abs/1811.01088">Phang et al., 2018</a>).</figcaption></figure><h2 id="getting-more-signal">Getting more signal</h2><p>The target task is often a low-resource task. We can often improve the performance of transfer learning by combining a diverse set of signals:</p><p><strong>Sequential adaptation</strong>  If related tasks are available, we can fine-tune our model first on a related task with more data before fine-tuning it on the target task. This<br>helps particularly for tasks with limited data and similar tasks (<a href="https://arxiv.org/abs/1811.01088v2">Phang et al., 2018</a>) and improves sample efficiency on the target task (<a href="https://arxiv.org/abs/1901.11373">Yogatama et al., 2019</a>).</p><p><strong>Multi-task fine-tuning</strong>  Alternatively, we can also fine-tune the model jointly on related tasks together with the target task. The related task can also be an unsupervised auxiliary task. Language modelling is a good choice for this and has been shown to help even without pretraining (<a href="https://arxiv.org/abs/1704.07156">Rei et al., 2017</a>). The task ratio can optionally be annealed to de-emphasize the auxiliary task towards the end of training (<a href="https://arxiv.org/abs/1902.10547">Chronopoulou et al., NAACL 2019</a>). Language model fine-tuning is used as a separate step in ULMFiT (<a href="https://aclweb.org/anthology/P18-1031">Howard and Ruder, 2018</a>). Recently, multi-task fine-tuning has led to improvements even with many target tasks (<a href="https://arxiv.org/abs/1901.11504">Liu et al., 2019</a>, <a href="https://www.aclweb.org/anthology/P19-1439">Wang et al., 2019</a>).</p><p><strong>Dataset slicing</strong>  Rather than fine-tuning with auxiliary tasks, we can use <a href="https://dawn.cs.stanford.edu/2019/03/22/glue/">auxiliary heads that are trained only on particular subsets of the data</a>. To this end, we would first analyze the errors of the model, use heuristics to automatically identify challenging subsets of the training data, and then train auxiliary heads jointly with main head.</p><p><strong>Semi-supervised learning</strong>  We can also use semi-supervised learning methods to make our model's predictions more consistent by perturbing unlabelled examples. The perturbation can be noise, masking (<a href="https://arxiv.org/abs/1809.08370">Clark et al., 2018</a>), or data augmentation, e.g. back-translation (<a href="https://arxiv.org/abs/1904.12848">Xie et al., 2019</a>).</p><p><strong>Ensembling</strong>  To improve performance the predictions of models fine-tuned with different hyper-parameters, fine-tuned with different pretrained models, or trained on different target tasks or dataset splits may be combined.</p><p><strong>Distilling</strong>  Finally, large models or ensembles of models may be distilled into a single, smaller model. The model can also be a lot simpler (Tang et al., 2019) or have a different inductive bias (<a href="https://www.aclweb.org/anthology/P19-1337">Kuncoro et al., 2019</a>). Multi-task fine-tuning can also be combined with distillation (<a href="https://arxiv.org/abs/1907.04829">Clark et al., 2019</a>).</p><h1 id="down-stream-applications">Down-stream applications</h1><p>Pretraining large-scale models is costly, not only in terms of computation but also in terms of the environmental impact (<a href="https://www.aclweb.org/anthology/P19-1355">Strubell et al., 2019</a>). Whenever possible, it's best to use open-source models. If you need to train your own models, please share your pretrained models with the community.</p><h3 id="frameworks-and-libraries">Frameworks and libraries</h3><p>For sharing and accessing pretrained models, different options are available:</p><p><strong>Hubs</strong>  Hubs are central repositories that provide a common API for accessing pretrained models. The two most common hubs are <a href="https://www.tensorflow.org/hub">TensorFlow Hub</a> and <a href="https://pytorch.org/hub">PyTorch Hub</a>. Hubs are generally simple to use; however, they act more like a black-box as the source code of the model cannot be easily accessed. In addition, modifying the internals of a pretrained model architecture can be difficult.</p><p><strong>Author released checkpoints</strong>  Checkpoint files generally contain all the weights of a pretrained model. In contrast to hub modules, the model graph still needs to be created and model weights need to be loaded separately. As such, checkpoint files are more difficult to use than hub modules, but provide you with full control over the model internals.</p><p><strong>Third-party libraries</strong>  Some third-party libraries like <a href="https://allennlp.org/">AllenNLP</a>, <a href="https://github.com/fastai/fastai">fast.ai</a>, and <a href="https://github.com/huggingface/pytorch-transformers">pytorch-transformers</a> provide easy access to pretrained models. Such libraries typically enable fast experimentation and cover many standard use cases for transfer learning.</p><p>For examples of how such models and libraries can be used for downstream tasks, have a look at the <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g569f436ced_0_30">code snippets</a> in the slides, the <a href="http://tiny.cc/NAACLTransferColab">Colaboratory notebook</a>, and the <a href="http://tiny.cc/NAACLTransferCode">code</a>.</p><h1 id="open-problems-and-future-directions">Open problems and future directions</h1><p>There are many open problems and interesting future research directions. Below is just an updated selection. For more pointers, have a look at <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g569f436ced_0_34">the slides</a>.</p><h3 id="shortcomings-of-pretrained-language-models">Shortcomings of pretrained language models</h3><p>Pretrained language models are still bad at fine-grained linguistic tasks (<a href="https://arxiv.org/abs/1903.08855">Liu et al., 2019</a>), hierarchical syntactic reasoning (<a href="https://www.aclweb.org/anthology/P19-1337">Kuncoro et al., 2019</a>), and common sense (when you actually make it difficult; <a href="https://arxiv.org/abs/1905.07830">Zellers et al., 2019</a>). They still fail at natural language generation, in particular maintaining long-term dependencies, relations, and coherence. They also tend to overfit to surface form information when fine-tuned and can still mostly be seen as ‘rapid surface learners’.</p><p>As we have noted above, particularly large models that are fine-tuned on small amounts of data are difficult to optimize and suffer from high variance. Current pretrained language models are also very large. Distillation and pruning are two ways to deal with this.</p><h3 id="pretraining-tasks">Pretraining tasks</h3><p>While the language modelling objective has shown to be effective empirically, it has its weaknesses. Lately, we have seen that bidirectional context and modelling contiguous word sequences is particularly important. Maybe most importantly, language modelling encourages a focus on syntax and word co-occurrences and only provides a weak signal for capturing semantics and long-term context. We can take inspiration from other forms of self-supervision. In addition, we can design specialized pretraining tasks that explicitly learn certain relationships (<a href="https://arxiv.org/abs/1907.10529">Joshi et al., 2019</a>, <a href="https://arxiv.org/abs/1907.12412">Sun et al., 2019</a>).</p><p>On the whole, it is difficult to learn certain types of information from raw text. Recent approaches incorporate structured knowledge (<a href="http://arxiv.org/abs/1905.07129">Zhang et al., 2019</a>; <a href="https://www.aclweb.org/anthology/P19-1598">Logan IV et al., 2019</a>) or leverage multiple modalities (<a href="https://arxiv.org/abs/1904.01766">Sun et al., 2019</a>; <a href="https://arxiv.org/abs/1908.02265">Lu et al., 2019</a>) as two potential ways to mitigate this problem.</p><h2 id="citation">Citation</h2><p> If you found this post helpful, consider citing <a href="https://www.aclweb.org/anthology/N19-5004">the tutorial</a> as:</p><pre><code>@inproceedings{ruder2019transfer,
  title={Transfer Learning in Natural Language Processing},
  author={Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials},
  pages={15--18},
  year={2019}
}</code></pre><h3 id="translations">Translations</h3><p>This article has been translated into the following languages:</p><ul><li><a href="https://www.infoq.cn/article/zD5QkcIzF9253friWPVd">Chinese</a> (by Sambodhi)</li><li><a href="https://www.ibidemgroup.com/edu/traduccion-aprendizaje-pnl/">Spanish</a> (by Ibidem Group)</li></ul>]]></content:encoded></item><item><title><![CDATA[EurNLP]]></title><description><![CDATA[The first European NLP Summit (EurNLP) will take place in London on October 11, 2019. It is an opportunity to foster discussion and collaboration between researchers in and around Europe.]]></description><link>http://ruder.io/eurnlp/</link><guid isPermaLink="false">5ce51295d55ba83bebb51674</guid><category><![CDATA[events]]></category><category><![CDATA[natural language processing]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Thu, 04 Jul 2019 20:56:39 GMT</pubDate><media:content url="http://ruder.io/content/images/2019/07/EurNLP-ban_2.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2019/07/EurNLP-ban_2.jpg" alt="EurNLP"><p>The <a href="https://www.eurnlp.org/">first European NLP Summit (EurNLP)</a> will take place in London on October 11. Registration is <a href="https://eurnlp.splashthat.com/">open now</a>. Travel grants are available.</p><p>The Natural Language Processing community has seen unprecedented growth in recent years (see for instance the <a href="http://acl2019pcblog.fileli.unipi.it/?p=156">ACL 2019 Chairs blog</a>). As more people are entering the field and NLP research sprouts in more places, making meaningful connections and communicating effectively becomes more difficult.</p><p>To successfully scale our conferences, we require structures that enable us to integrate and to provide mentorship and advice to the next generation of researchers and engineers. In addition, we are in need of mechanisms that facilitate collaboration and the exchange of ideas in our community. </p><p>Conferences such as <a href="https://naacl2019.org/">NAACL 2019</a> have been fostering initiatives that encourage diversity and inclusion, such as mentoring, childcare, and live captions. In addition, meetups with demographic affinity groups (WiNLP, Queer in AI, Black in AI, etc.) and venues such as “birds-of-a-feather” tables facilitate meeting like-minded people.</p><p>Beyond the global exchange of information and ideas in the field, we believe that an effective way to foster such connections is on the regional level, to connect newcomers with experts in their vicinity and with each other. In order to keep the global community vital and healthy, we need to ensure that local talent is able to flourish and that role models and mentors are available to provide inspiration and feedback.</p><p>Europe has a vibrant and world-leading NLP community, with research hubs across the continent and positioned both in academia and industry. The aim of <a href="https://www.eurnlp.org/">EurNLP</a> (pronounced “your NLP”) is to bring our community closer together. We want to provide an informal focused get-together event to foster discussion and collaboration between researchers in and around Europe. EurNLP is supported by EACL and aimed to be complementary to scientific publication-oriented European events. It is also intended to be complementary to summer schools that are focused on educating the next generation and can now be found <a href="http://mlss.cc/">all around the globe</a>, including <a href="http://www.deeplearningindaba.com/">Africa</a>, <a href="https://khipu.ai/">South America</a>, <a href="https://www.sea-mls.com/">Southeast Asia</a>, and <a href="http://lxmls.it.pt">Europe</a>.</p><p>EurNLP is inspired by the highly successful West Coast NLP Summit (<a href="https://www.wecnlp.ai/">WeCNLP</a>). In contrast to a purely scientific symposium, EurNLP is an industry-academia partnership and hosted in cooperation with companies. In order to grow the European NLP community and solve real-world problems our society is facing, we must bridge academia and industry and connect fundamental and applied research.</p><p>The <a href="https://www.eurnlp.org/">first European NLP summit</a> will take place in London on October 11, 2019. The registration for EurNLP is <a href="https://eurnlp.splashthat.com/">open now</a>. Thanks to our industrial host Facebook and sponsorship from DeepMind, registration is not only free, but there are also travel grants available to students with an accepted abstract.</p><p>—The EurNLP Organisation Committee (Barbara Plank, Sebastian Ruder, Sebastian Riedel, Armand Joulin, Fabrizio Silvestri)</p>]]></content:encoded></item><item><title><![CDATA[NAACL 2019 Highlights]]></title><description><![CDATA[This post discusses highlights of NAACL 2019. It covers transfer learning, common sense reasoning, natural language generation, bias, non-English languages, and diversity and inclusion.]]></description><link>http://ruder.io/naacl2019/</link><guid isPermaLink="false">5cfd6e4dd55ba83bebb516be</guid><category><![CDATA[events]]></category><category><![CDATA[transfer learning]]></category><category><![CDATA[cross-lingual]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Sun, 09 Jun 2019 20:56:15 GMT</pubDate><media:content url="http://ruder.io/content/images/2019/06/transfer_learning_tutorial_room-4.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2019/06/transfer_learning_tutorial_room-4.jpg" alt="NAACL 2019 Highlights"><p>Update 19.04.20: Added a <a href="#translations">translation of this post in Spanish</a>.</p><p>This post discusses highlights of the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (<a href="https://naacl2019.org/">NAACL 2019</a>).</p><p>You can find past highlights of conferences <a href="http://ruder.io/tag/events/">here</a>. The conference accepted 424 papers (which you can find <a href="https://www.aclweb.org/anthology/events/naacl-2019/#n19-1">here</a>) and had 1575 participants (see the <a href="https://naacl2019.org/downloads/naacl2019-intro-slides.pdf">opening session slides</a> for more details). These are the topics that stuck out for me most:</p><ul><li><a href="#transfer-learning">Transfer learning</a></li><li><a href="#common-sense-reasoning">Common sense reasoning</a></li><li><a href="#natural-language-generation">Natural language generation</a></li><li><a href="#bias">Bias</a></li><li><a href="#non-english-languages">Non-English languages</a></li><li><a href="#diversity-and-inclusion">Diversity and inclusion</a></li></ul><h2 id="transfer-learning">Transfer learning</h2><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/06/transfer_learning_tutorial_room_2.jpg" class="kg-image" alt="NAACL 2019 Highlights"><figcaption>The room at the Transfer Learning in NLP tutorial (Image credit: <a href="https://twitter.com/soldni/status/1135269017512546304">Luca Soldaini</a>)</figcaption></figure><p>Interest in transfer learning remains high. The <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit?usp=sharing">Transfer Learning in NLP</a> tutorial (pictured above and organized by <a href="https://scholar.google.com/citations?user=K5nCPZwAAAAJ&amp;hl=en">Matthew Peters</a>, <a href="https://www.cs.cmu.edu/~sswayamd/">Swabha Swayamdipta</a>, <a href="http://thomwolf.io/">Thomas Wolf</a>, and me) was packed. NAACL 2019 awarded the best long paper award to <a href="https://www.aclweb.org/anthology/N19-1423">BERT</a>, arguably the most impactful recent transfer learning method. Despite its recency, conference papers already leveraged BERT for <a href="https://www.aclweb.org/anthology/N19-1035">aspect-based sentiment analysis</a>, <a href="https://www.aclweb.org/anthology/N19-1242">review reading comprehension</a>, <a href="https://www.aclweb.org/anthology/N19-1421">common sense reasoning</a>, and <a href="https://www.aclweb.org/anthology/N19-4013">open-domain question answering</a>. </p><p>At the <a href="https://repeval2019.github.io/">RepEval</a> workshop, Kristina Toutanova discussed how to <a href="https://arxiv.org/abs/1906.00300">use transfer learning for open-domain question answering</a>. With appropriate pretraining using an Inverse Cloze Task, the retriever and reader can be fine-tuned directly on QA pairs without an intermediate IR system. This demonstrates that <strong>a careful initialization + fine-tuning</strong> are two key ingredients for transfer learning and work even on challenging tasks. This has also been shown in the past for <a href="https://www.aclweb.org/anthology/P18-1073">learning cross-lingual word embeddings</a> and <a href="https://arxiv.org/abs/1804.07755">unsupervised MT</a>. She also made the point that <strong>single-vector sentence/paragraph representations are very useful for retrieval</strong>—and that we should continue to work on them. Overall, there are many exciting research directions in transfer learning in NLP, some of which we outlined at <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5882add69e_5_673">the end of our tutorial</a>. My other highlights include: </p><ul><li><strong>Single-step Auxiliary loss Transfer Learning</strong> (SiATL; <a href="https://www.aclweb.org/anthology/N19-1213">Chronopoulou et al.</a>), an "embarrassingly simple" approach that reduces some of the complexity of <a href="https://arxiv.org/abs/1801.06146">ULMFiT</a> via multi-task learning and exponentially decaying the auxiliary loss.</li><li><strong>AutoSeM</strong> (<a href="https://www.aclweb.org/anthology/N19-1355">Guo et al</a>.), a two-stage pipeline for multi-task learning that utilizes multi-armed bandits and Bayesian optimization to learn the best auxiliary task and the best task mixing ratio respectively.</li><li>An <strong>evaluation of contextual representation across 16 tasks</strong> (<a href="https://www.aclweb.org/anthology/N19-1112">Liu et al.</a>) that shows that they are bad at capturing fine-grained linguistic knowledge and higher layers in RNNs are more task-specific than in Transformers.</li></ul><h2 id="common-sense-reasoning">Common sense reasoning</h2><p>Language modelling is a pretraining task that has been shown to learn generally useful representations at scale. However, there are some things that are simply never written, even in billions of tokens.<strong> Overcoming this reporting bias is a key challenge</strong> in adapting language models to more complex tasks. To test reasoning with knowledge that is often left unsaid, the <a href="https://www.aclweb.org/anthology/N19-1421">best resource paper</a> used the common sense knowledge base ConceptNet as “seed”. They created <a href="https://www.tau-nlp.org/commonsenseqa">CommonsenseQA</a>, a dataset of multiple-choice questions where most answers have the same relation to the target concept (see below).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/06/commonsenseqa_examples.png" class="kg-image" alt="NAACL 2019 Highlights"><figcaption>Example question-answer pairs in CommonsenseQA (Source: <a href="https://www.aclweb.org/anthology/N19-1421">Talmor et al.</a>)</figcaption></figure><p>This requires the model to use common sense rather than just relational or co-occurrence information to answer the question. BERT achieves 55.9% accuracy on this dataset—and is estimated to achieve around 75% with 100k examples—still well below human performance 88.9%. What does it take to get to those 88.9%? Most likely <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5888218f39_11_238"><strong>structured knowledge, interactive and multimodal learning</strong></a>. In his talk at the <a href="https://sites.google.com/view/sivl2019/home">Workshop on Shortcomings in Vision and Language</a> (SiLV), Yoav Artzi discussed <a href="https://yoavartzi.com/slides/2019_06_06_sivl_naacl.pdf">language diversity in grounded NLU</a>, noting that we need to move from synthetic to more realistic images for learning grounded representations.</p><p>Another prerequisite for natural language understanding is compositional reasoning. The <a href="https://nlitutorial.github.io/">Deep Learning for Natural Language Inference tutorial</a> discussed natural language inference, a common benchmark for evaluating such forms of reasoning in-depth. I particularly liked the following papers:</p><ul><li><strong>A label consistency framework for procedural text comprehension</strong> (<a href="https://www.aclweb.org/anthology/N19-1244">Du et al.</a>) that encourages consistency between predictions from descriptions of the same process. This is a clever way to use intuition and additional data to incorporate an inductive bias into the model.</li><li><strong>Discrete Reasoning Over the content of Paragraphs</strong> (DROP; <a href="https://www.aclweb.org/anthology/N19-1246">Dua et al.</a>), which requires models to resolve references in a question and perform discrete operations (e.g. addition, counting, sorting) over multiple referents in the text.</li></ul><h2 id="natural-language-generation">Natural language generation</h2><p>At the <a href="https://neuralgen.io/">NeuralGen</a> workshop, Graham Neubig <a href="http://www.phontron.com/slides/neubig19neuralgen.pdf">discussed methods to optimize a non-differentiable objective function</a> such as BLEU directly, including minimum risk training and REINFORCE and tricks to deal with their instability and get them to work. While we had touched on <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g512941aa9e_0_36">transfer learning for natural language generation</a> (NLG) in our tutorial, Sasha Rush provided many more details and <a href="http://nlp.seas.harvard.edu/slides/Pre-training%20for%20Generation.pdf">discussed different methods of using language models</a> to improve NLG quality. Another way to improve sample quality is to focus on decoding. Yejin Choi discussed a <a href="https://arxiv.org/abs/1904.09751">new sampling method</a> that samples from the head of the distribution and leads to better text quality. She also discussed the generation of fake news and how large pretrained language models such as <a href="https://arxiv.org/abs/1905.12616">Grover</a> can be used to defend against them. </p><p>Generative adversarial networks (GANs) are a popular way to generate images, but so far have underperformed for language. The <a href="https://drive.google.com/drive/folders/1E4uHe4_TD4yDJws3t1kXJQanUFJiqpBB">Deep Adversarial Learning for NLP tutorial</a> argued that we should not give up on them as the <strong>unsupervised or self-supervised learning done by GANs has many applications in NLP</strong>.</p><p>Another compelling aspect of generation is to enable multiple agents to communicate effectively. Besides providing a window into how language emerges, it may be necessary for interactive learning and to transfer knowledge among agents. Angeliki Lazaridou discussed in her SiLV workshop talk that deep reinforcement learning tools seem to work well for this setting but argued that <strong>better biases are needed</strong>. In addition, it is still <strong>difficult to <a href="https://arxiv.org/abs/1612.07182">interface emergent language to natural language</a></strong>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/06/huse_trade-offs.png" class="kg-image" alt="NAACL 2019 Highlights"><figcaption>Trade-offs between quality and diversity of different models (circles) on NLG tasks (Image credit: <a href="https://www.aclweb.org/anthology/N19-1169">Hashimoto et al.</a>)</figcaption></figure><p>I also enjoyed the following papers:</p><ul><li><strong>Human Unified with Statistical Evaluation</strong> (HUSE; <a href="https://www.aclweb.org/anthology/N19-1169">Hashimoto et al.</a>), a new metric for natural language generation that can consider both diversity and quality and yields a Pareto frontier by trading off one of the two (see above). Methods such as temperature annealing result in higher quality, but reduce diversity.</li><li><strong>Separating planning from realization</strong> (<a href="https://arxiv.org/abs/1904.03396">Moryossef et al.</a>) can improve the quality of generated text from structured data such as RDF triplets as there are often multiple ways structured information can be realized in text.</li><li><strong>Decoupling syntax and surface form generation</strong> (<a href="https://arxiv.org/abs/1804.07707">Cao &amp; Clark</a>) is another way to deal with the underspecified problem of text generation from structured data (in this case, abstract meaning representations). </li><li><strong>A systematic analysis that probes how useful the visual modality actually is for multimodal translation</strong> (<a href="https://www.aclweb.org/anthology/N19-1422">Caglayan et al.</a>) and was awarded the best short paper award. It observes that models with less textual information more strongly rely on the visual context, contrary to current beliefs.</li></ul><h2 id="bias">Bias</h2><p>The <a href="https://naacl2019.org/calls/papers/#theme-topics">theme of the conference</a> was model bias. The diverse sets of keynotes fit very well into this theme. The first keynote by <a href="http://randomwalker.info/">Arvind Narayanan</a> in particular highlighted one under-appreciated aspect of bias, i.e. that we can <strong>leverage the bias in our models to improve our understanding of human culture</strong>.</p><p>On the whole, there is a <strong>fine line between desirable and undesirable bias</strong>. We often try to encode inductive bias about how the world works, such as <a href="http://ruder.io/emnlp-2018-highlights/index.html#inductive-bias">objects being invariant to translation</a>. On the other hand, we do not want our models to learn superficial cues or relations that are not part of our possibly idealized perception of the world, such as <a href="https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf">gender bias</a>. Ultimately, super-human performance should not just entail that models outperform humans quantitively but also that they are less biased and fallible.</p><p>Lastly, we should be conscious that <strong>technology has lasting impact in the real world</strong>. As one vivid example of this, <a href="https://textio.com/team/">Kieran Snyder</a> recounted in her keynote the time when she had to design a sorting algorithm for <a href="https://en.wikipedia.org/wiki/Sinhala_language">Sinhala</a> (see below). Sorting Sinhalese names was necessary for the Sri Lankan government to be able to search for survivors in the aftermath of the 2004 tsunami. Her decision on how to alphabetize the language later became part of an official government policy.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/06/sinhala_vowels.jpg" class="kg-image" alt="NAACL 2019 Highlights"><figcaption>Vowels in Sinhala (Image credit: <a href="https://www.omniglot.com/writing/sinhala.htm">Omniglot</a>)</figcaption></figure><p>Some of my favourite papers on bias include:</p><ul><li><strong>Debiasing methods only superficially remove bias in word embeddings </strong>(<a href="https://www.aclweb.org/anthology/N19-1061">Gonen &amp; Goldberg</a>); bias is still reflected in—and can be recovered from—the distances in the debiased embeddings.</li><li><strong>An evaluation of bias in contextualized word embeddings</strong> (<a href="https://www.aclweb.org/anthology/N19-1064">Zhao et al.</a>) finds that ELMo syntactically and unequally encodes gender information and—more importantly—that this bias is inherited by downstream models, such as a coreference system. </li></ul><h2 id="non-english-languages">Non-English languages</h2><p>On the topic of different languages, during the conference, the <a href="https://twitter.com/emilymbender/status/1135691925674217473">“Bender Rule”</a>—named after <a href="https://faculty.washington.edu/ebender/">Emily Bender</a> who is known for her advocacy for multilingual language processing, among other things—was <a href="https://twitter.com/EvpokPadding/status/1136649868800352262">frequently</a> <a href="https://twitter.com/amitmoryossef/status/1136359765758697478">invoked</a> <a href="https://twitter.com/adinamwilliams/status/1136313903988822016">after</a> <a href="https://twitter.com/EmmaSManning/status/1136395448313401346">presentations</a>. In short, the rule states: "<strong>Always name the language(s) you are working on.</strong>" Not explicitly identifying the language under consideration leads to English being perceived as the default and as proxy for other languages, which is problematic in many ways (see <a href="http://faculty.washington.edu/ebender/papers/Bender-SDSS-2019.pdf">Emily's slides</a> for a thorough rationale).</p><p>In this vein, some of my favourite papers from the conference investigate how the performance of our models changes as we apply them other languages:</p><ul><li><strong>Polyglot contextual representations </strong>(<a href="https://www.aclweb.org/anthology/N19-1392">Mulcaire et al.</a>) that are trained on English and an additional language by initializing word embeddings with cross-lingual representations. For some settings (Chinese SRL, Arabic NER), cross-lingual training yields large improvements.</li><li><strong>A study on transfer of dependency parsers trained on English to 30 other languages </strong>(<a href="https://www.aclweb.org/anthology/N19-1253">Ahmad et al.</a>) finds that RNNs trained on English transfer well to languages close to English, but self-attention models transfer better to distant languages.</li><li><strong>An unsupervised POS tagger for low-resource languages</strong> (<a href="https://www.aclweb.org/anthology/N19-1252">Cardenas et al.</a>) that "deciphers" Brown cluster ids in order to generate the POS sequence and achieves state-of-the-art performance on Sinhalese (see above).</li></ul><h2 id="diversity-and-inclusion">Diversity and inclusion</h2><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/06/badge_stickers.jpg" class="kg-image" alt="NAACL 2019 Highlights"><figcaption>Badge stickers at NAACL 2019 (Image credit: <a href="https://twitter.com/natschluter/status/1135963599216750593">Natalie Schluter</a>)</figcaption></figure><p>As the community is growing it is important that new members feel included and that their voices are heard. NAACL 2019 put into effect a wide range of initiatives in this regard, from thoughtful touches such as badge stickers (see above) to matching newcomers with mentors and “big siblings”, to fundamental ones such as childcare (see below) and <a href="https://naacl2019.org/captions/">live captions</a>. I particularly appreciated the <a href="https://twitter.com/NAACLHLT/status/1134181082151227393">live tweeting</a>, which made the conference accessible to people who could not attend.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/06/childcare_room-1.jpg" class="kg-image" alt="NAACL 2019 Highlights"><figcaption>Childcare room at NAACL 2019 (Image credit: <a href="https://twitter.com/KieranSnyder/status/1135924564771450880">Kieran Snyder</a>)</figcaption></figure><h2 id="translations">Translations</h2><p>This post has been translated into the following languages: </p><ul><li><a href="https://www.traductor-jurado.org/blog/traduccion-lenguaje-natural/">Spanish</a></li></ul><p><em>Cover image: The room at the <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit?usp=sharing">Transfer Learning in NLP</a> tutorial (Image credit: <a href="https://twitter.com/thedansimonson/status/1135275735730597889">Dan Simonson</a>)</em></p>]]></content:encoded></item><item><title><![CDATA[Neural Transfer Learning for Natural Language Processing (PhD thesis)]]></title><description><![CDATA[This post discusses my PhD thesis Neural Transfer Learning for Natural Language Processing and some new material presented in it.]]></description><link>http://ruder.io/thesis/</link><guid isPermaLink="false">5c76ff7c1b9b0d18555b9eb6</guid><category><![CDATA[transfer learning]]></category><category><![CDATA[natural language processing]]></category><category><![CDATA[multi-task learning]]></category><category><![CDATA[domain adaptation]]></category><category><![CDATA[cross-lingual]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Sat, 23 Mar 2019 18:00:43 GMT</pubDate><media:content url="http://ruder.io/content/images/2019/03/transfer_learning_taxonomy-1.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2019/03/transfer_learning_taxonomy-1.png" alt="Neural Transfer Learning for Natural Language Processing (PhD thesis)"><p></p><p>I finally got around to submitting <a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf">my thesis</a>. The thesis touches on the four areas of transfer learning that are most prominent in current Natural Language Processing (NLP): <strong>domain adaptation</strong>, <strong>multi-task learning</strong>, <strong>cross-lingual learning</strong>, and <strong>sequential transfer learning</strong>.</p><p>Most of the work in the thesis has been previously presented (see <a href="http://ruder.io/publications/">Publications</a>). Nevertheless, there are some new parts as well. The most notable are:</p><ol><li><strong>a background chapter</strong> (<a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=28">§2</a>) that lays out key concepts in terms of probability and information theory, machine learning, neural networks, and NLP and connects these to their usage in subsequent chapters;</li><li><strong>a taxonomy for transfer learning for NLP</strong> (<a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=64">§3.1.3</a>, see below) that adapts the taxonomy of <a href="http://www.cs.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf">Pan and Yang (2010)</a> to contemporary settings in NLP;</li><li><strong>an updated review of multi-task learning</strong> (<a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=65">§3.2</a>) that discusses more recent advances and choices in multi-task learning;</li><li><strong>reviews of sequential transfer learning and domain adaptation</strong> (<a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=81">§3.3</a> and <a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=104">§3.4</a>) that identify common themes in each of the research areas;</li><li>and <strong>future directions</strong> (<a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=273">§8.3</a>) in each area that particularly excite me.</li></ol><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/03/transfer_learning_taxonomy.png" class="kg-image" alt="Neural Transfer Learning for Natural Language Processing (PhD thesis)"><figcaption>A taxonomy for transfer learning for NLP.</figcaption></figure><p>Whenever possible, I've tried to draw connections between methods used in different areas of transfer learning. It's a longer read but I hope it may still be helpful to some of you. You can download the complete thesis <a href="http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf">here</a>.</p><p>If you found some material in the thesis helpful, I'd appreciate if you could cite it using the below BibTex:</p><!--kg-card-begin: markdown--><pre><code>@PhdThesis{Ruder2019Neural,
  title={Neural Transfer Learning for Natural Language Processing},
  author={Ruder, Sebastian},
  year={2019},
  school={National University of Ireland, Galway}
}
</code></pre>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[AAAI 2019 Highlights: Dialogue, reproducibility, and more]]></title><description><![CDATA[This post discusses highlights of AAAI 2019. It covers dialogue, reproducibility, question answering, the Oxford style debate, invited talks, and a diverse set of research papers.]]></description><link>http://ruder.io/aaai-2019-highlights/</link><guid isPermaLink="false">5c76ff7c1b9b0d18555b9ebb</guid><category><![CDATA[events]]></category><category><![CDATA[natural language processing]]></category><category><![CDATA[transfer learning]]></category><category><![CDATA[word embeddings]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Thu, 07 Feb 2019 17:00:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2019/02/aaai_reception-1.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2019/02/aaai_reception-1.jpg" alt="AAAI 2019 Highlights: Dialogue, reproducibility, and more"><p>This post discusses highlights of the <a href="https://aaai.org/Conferences/AAAI-19/">Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)</a>.</p><p>I attended <a href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a> in Honolulu, Hawaii last week. Overall, I was particularly surprised by the interest in natural language processing at the conference. There were 15 sessions on NLP (most standing-room only) with ≈10 papers each (oral and spotlight presentations), so around 150 NLP papers (out of 1,150 accepted papers overall). I also really enjoyed the diversity of invited speakers who discussed topics from AI for social good, to adversarial learning and imperfect-information games (videos of all invited talks are available <a href="https://aaai.org/Conferences/AAAI-19/invited-speakers/">here</a>). Another cool thing was the <a href="#debate">Oxford style debate</a>, which required debaters to take controversial positions. This was a nice change of pace from panel discussions, which tend to converge to a uniform opinion.</p><p>Table of contents:</p><ul><li><a href="#dialogue">Dialogue</a></li><li><a href="#reproducibility">Reproducibility</a></li><li><a href="#question-answering">Question answering</a></li><li><a href="#ai-for-social-good">AI for social good</a></li><li><a href="#debate">Debate</a></li><li><a href="#adversarial-learning">Adversarial learning</a></li><li><a href="#imperfect-information-games">Imperfect-information games</a></li><li><a href="#inductive-biases">Inductive biases</a></li><li><a href="#transfer-learning">Transfer learning</a></li><li><a href="#word-embeddings">Word embeddings</a></li><li><a href="#miscellaneous">Miscellaneous</a></li></ul><h1 id="dialogue">Dialogue</h1><p>In his talk at the <a href="https://sites.google.com/view/deep-dial-2019/home?authuser=0">Reasoning and Learning for Human-Machine Dialogues workshop</a>, <a href="https://scholar.google.com/citations?user=V72PR9wAAAAJ&amp;hl=en">Phil Cohen</a> argued that <strong>chatbots are an attempt to avoid solving the hard problems of dialogue</strong>. They provide the <em>illusion</em> of having a dialogue but in fact do not have a clue what we are saying or meaning. What we should rather do is <strong>recognize intents via semantic parsing</strong>. We should then reason about the speech acts, infer a user's <em>plan</em>, and help them to succeed. You can find more information about his views in <a href="https://arxiv.org/abs/1812.01144">this position paper</a>.</p><p>During the panel discussion, <a href="https://www.microsoft.com/en-us/research/people/izitouni/">Imed Zitouni</a> highlighted that the limitations of current dialogue models affect user behaviour. <strong>75-80% of the time users only employ 4 skills</strong>: "play music", "set a timer", "set a reminder", and "what is the weather". Phil argued that we should not have to learn how to talk, how to make an offer, etc. all over again for each domain. We can often build simple dialogue agents for new domains <a href="http://www.aclweb.org/anthology/P18-2008">"overnight"</a>.</p><h1 id="reproducibility">Reproducibility</h1><p>At the <a href="https://www.idi.ntnu.no/~odderik/RAI-2019/">Workshop on Reproducible AI</a>, <a href="http://joelgrus.com/">Joel Grus</a> argued that <a href="https://docs.google.com/presentation/d/1ivK8AKgz8Hx-ZYzPC9gJyQK6tzuhR3UuhCEajFGJDlA/"><strong>Jupyter notebooks are bad for reproducibility</strong></a>. As an alternative, he recommended to adopt higher-level abstractions and declarative configurations. Another good resource for reproducibility is the <a href="https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf">ML reproducibility checklist</a> by <a href="https://www.cs.mcgill.ca/~jpineau/">Joelle Pineau</a>, which provides a list of items for algorithms, theory, and empirical results to enforce reproducibility.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/02/unit_tests_for_ai_experiments.png" class="kg-image" alt="AAAI 2019 Highlights: Dialogue, reproducibility, and more"><figcaption>Unit tests for AI experiments recommended by Joel Grus</figcaption></figure><p>A team from Facebook <a href="https://arxiv.org/abs/1902.04522">reported on their experiments reproducing AlphaZero</a> in their <a href="https://github.com/pytorch/ELF">ELF framework</a>, training a model using 2,000 GPUs in 2 weeks. <strong>Reproducing an on-policy, distributed RL system such as AlphaZero is particularly challenging</strong> as it does not have a fixed dataset and optimization is dependent on the distributed environment. Training smaller versions and scaling up is key. For reproducibility, the random seed, the git commit number, and the logs should be stored.</p><p>During the panel discussion, <a href="https://www.ntnu.edu/employees/odderik">Odd Eric Gunderson</a> argued that reproducibility should be defined as the <em>ability of an independent research team to produce the same results using the same AI method based on the documentation by the original authors</em>. Degrees of reproducibility can be measured based on the availability of different types of documentation, such as the method description, data, and code.</p><p><a href="https://www.isye.gatech.edu/users/pascal-van-hentenryck">Pascal van Hentenryck</a> argued that reproducibility could be made part of the peer review process, such as in the <a href="http://mpc.zib.de/">Mathematical Programming Computation journal</a> where each submission requires an executable file (which does not need to be public). He also pointed out that—empirically—papers with supplementary materials are more likely to be accepted.</p><h1 id="question-answering">Question answering</h1><p>At the <a href="https://researcher.watson.ibm.com/researcher/view_group_subpage.php?id=9904">Reasoning and Complex QA Workshop</a>, <a href="https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/forbus-ken.html">Ken Forbus</a> discussed an <a href="http://www.qrg.northwestern.edu/papers/Files/QRG_Dist_Files/QRG_2018/Crouse-McFate-Forbus-2018.pdf">analogical training method for QA</a> that adapts a general-purpose semantic parser to a new domain with few examples. At the end of his talk, Ken argued that the <strong>train/test method in ML is holding us back</strong>. Our learning systems should use rich relational representations, gather their own data, and evaluate progress.</p><p><a href="https://allenai.org/team/ashishs/">Ashish Sabharwal</a> discussed the <a href="https://github.com/allenai/OpenBookQA">OpenBookQA dataset</a> presented at EMNLP 2018 during his talk. The open book setting is situated between reading comprehension and open-ended QA on the textual QA spectrum (see below).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/02/textual_qa_spectrum.png" class="kg-image" alt="AAAI 2019 Highlights: Dialogue, reproducibility, and more"><figcaption>The textual QA spectrum</figcaption></figure><p>It is designed to probe a deeper understanding rather than memorization skills and requires applying core principles to new situations. He also argued that while entailment is recognized as a core NLP task with many applications, it is still lacking a convincing application to an end-task. This is mainly due to multi-sentence entailment being a lot harder, as irrelevant sentences often have significant textual overlap.</p><p>Furthermore, he discussed the design of leaderboards, which have to make tradeoffs along multiple competing axes with respect to the host, the submitters, and the community. <strong>A particular deficit of current leaderboards is that they make it difficult to share and build upon successful techniques. </strong>For an extensive discussion of the pros and cons of leaderboards, check out <a href="https://soundcloud.com/nlp-highlights/80-leaderboards-and-science-with-siva-reddy">this recent NLP Highlights podcast</a>.</p><p>The first part of the final panel discussion focused on important outstanding technical challenges for question answering. <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-witbrock">Michael Witbrock</a> emphasized <strong>techniques to create datasets that cannot easily be exploited by neural networks</strong>, such as the adversarial filtering in <a href="http://aclweb.org/anthology/D18-1009">SWAG</a>. Ken argued that models should come up with answers and explanations rather than performing multiple choice question answering, while Ashish noted that such explanations need to be automatically validated.</p><p><a href="https://www.cs.cmu.edu/~hovy/">Eduard Hovy</a> suggested that one way towards a system that can perform more complex QA could consist of the following steps:</p><ol><li>Build a symbolic numerical reasoner that leverages relations from an existing KB, such as <a href="http://www.cs.utexas.edu/users/ml/nldata/geoquery.html">Geobase</a>, which contains geography facts.</li><li>Look at the subset of questions in existing natural language datasets, which require reasoning that is possible with the reasoner.</li><li>Annotate these questions with semantic parses and train a semantic parsing model to convert the questions to logical forms. These can then be provided to the reasoner to produce an answer.</li><li>Augment the reasoner with another reasoning component and repeat steps 2-3.</li></ol><p>The panel members noted that such reasoners exist, but lack a common API.</p><p>Finally, here are a few papers on question answering that I enjoyed:</p><ul><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-RuckleA.3648.pdf"><strong>COALA: A Neural Coverage-Based Approach for Long Answer Selection with Small Data</strong></a>: An approach that ranks answers based on how many of the question aspects they cover. They incorporate syntactic information via dependency parses and find that this improves performance.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-DengYang.4661.pdf"><strong>Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge Base Question Answering</strong></a>: Answer selection and knowledge base QA are learned jointly via multi-task learning. Attention is performed on different views of the data.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-TafjordO.6869.pdf"><strong>QUAREL: A Dataset and Models for Answering Questions about Qualitative Relationships</strong></a>: A challenging new QA dataset of 2,771 story questions that require knowledge about qualitative relationships pertaining to 19 quantities such as smoothness, friction, speed, heat, and distance.</li></ul><h1 id="ai-for-social-good">AI for social good</h1><p>During his invited talk, <a href="http://teamcore.usc.edu/tambe/">Milind Tambe</a> looked back on 10 years of research in AI and multiagent systems for social good (video available <a href="https://vimeo.com/313940453">here</a>; slides available <a href="http://teamcore.usc.edu/lectures/AAAI_2019.pdf">here</a>). Milind discussed his research on using game theory to optimize security resources such as <a href="https://pdfs.semanticscholar.org/1a10/5181f785502be8d71e6f6f0569e6eedd60e6.pdf">patrols at airports</a>, <a href="https://research.create.usc.edu/cgi/viewcontent.cgi?referer=https://scholar.google.com/&amp;httpsredir=1&amp;article=1134&amp;context=nonpublished_reports">air marshal assignments on flights</a>, <a href="http://teamcore.usc.edu/projects/coastguard/default.htm">coast guard patrols</a>, and <a href="http://teamcore.usc.edu/people/Paws/index.html">ranger patrols in African national parks to protect against poachers</a>. Overall, his talk was a striking reminder of the positive effects AI can have if it is employed for social good.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/02/ml_for_predicting_poacher_behaviour.png" class="kg-image" alt="AAAI 2019 Highlights: Dialogue, reproducibility, and more"><figcaption>An overview of an ML approach for predicting poacher behaviour in an African national park</figcaption></figure><h1 id="debate">Debate</h1><p>The <a href="https://en.wikipedia.org/wiki/Debate#Oxford-style_debating">Oxford style</a> debate focused on the proposition “The AI community today should continue to focus mostly on ML methods” (video available <a href="https://vimeo.com/314378703">here</a>). It pitted <a href="https://www.cs.purdue.edu/homes/neville/">Jennifer Neville</a> and <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a> on the 'pro' side against <a href="http://cs.brown.edu/~mlittman/">Michael Littman</a> and <a href="https://allenai.org/team/orene/">Oren Etzioni</a> on the 'against' side, with <a href="https://www.cs.ubc.ca/~kevinlb/">Kevin Leyton-Brown</a> as moderator. Overall, the debate was entertaining and engaging to watch.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/02/oxford_style_debate.png" class="kg-image" alt="AAAI 2019 Highlights: Dialogue, reproducibility, and more"><figcaption>The debater panel (from left to right): Peter Stone, Jennifer Neville, Kevin Leyton-Brown (moderator), Michael Littman, Oren Etzioni</figcaption></figure><p>Here are some representative remarks from each of the debaters that stuck with me:</p><blockquote><em>"The unique strength of the AI community is that we focus on the problems that need to be solved." – Jennifer Neville<br>"We are in the middle of one of the most amazing paradigm shifts in all of science, certainly computer science." – Oren Etzioni<br>"If you want to have an impact, don’t follow the bandwagon. Keep alive other areas." – Peter Stone<br>"Scientists in the natural sciences are actually very excited about ML as much of their research relies on expensive computations, which can be approximated with neural networks." – Michael Littman</em></blockquote><p>There were some important observations and ultimately a general consensus that ML alone is not enough and we need to integrate other methods with ML. Yonatan Belinkov also <a href="https://twitter.com/boknilev/status/1090451665486925825">live tweeted</a>, while I <a href="https://twitter.com/seb_ruder/status/1090454767438946304">tweeted some remarks that elicited laughs</a>.</p><h1 id="adversarial-learning">Adversarial learning</h1><p>During his invited talk (video available <a href="https://vimeo.com/313941176">here</a>), <a href="https://ai.google/research/people/105214">Ian Goodfellow</a> discussed a multiplicity of areas to which adversarial learning has been applied. Among many advances, Ian mentioned that he was impressed by the performance and flexibility of <a href="https://arxiv.org/abs/1805.08318">attention masks for GANs</a>, particularly that they are not restricted to circular masks. </p><p>He discussed adversarial examples, which are a consequence of moving away from i.i.d. data: attackers are able to confuse the model by showing unusual data from a different distribution such as <a href="https://arxiv.org/abs/1707.08945">graffiti on stop signs</a>. He also argued—contrary to the prevalent opinion—that deep models that are more robust are more interpretable than linear models. The main reason is that the latent space of a linear model is totally unintuitive, while a more robust model is more inspectable (as can be seen below).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/02/latent_space_vulnerable_model_vs_robust_model.png" class="kg-image" alt="AAAI 2019 Highlights: Dialogue, reproducibility, and more"><figcaption>Traversing the latent space of a linear model (left) vs. a deep, more robust model (right) between different MNIST labels starting from "9"</figcaption></figure><p>Semi-supervised learning with GANs can allow models to be more sample-efficient. What is interesting about such applications is that they focus on the discriminator (which is normally discarded) rather than the generator where the <a href="https://arxiv.org/abs/1606.03498">discriminator is extended</a> to <a href="https://arxiv.org/abs/1606.03498">classify </a><a href="https://arxiv.org/abs/1606.01583"><em>n+1</em> classes</a>. Regarding leveraging GANs for NLP, Ian conceded that we currently have not found a good way to deal with the large action space required to generate sentences with RL.</p><h1 id="imperfect-information-games">Imperfect-information games</h1><p>In his invited talk (video available <a href="https://vimeo.com/313942390">here</a>), <a href="http://www.cs.cmu.edu/~sandholm/">Tuomas Sandholm—</a>whose <a href="https://www.engadget.com/2017/02/10/libratus-ai-poker-winner/">AI Libratus was the first AI to beat top Heads-Up No-Limit Texas Hold'em professionals in January 2017—</a>discussed new results for solving imperfect-information games. He stressed that <strong>only game-theoretically sound techniques yield strategies that are robust against all opponents in imperfect-information games</strong>. Other advantages of a game-theoretic approach are a) that even if humans have access to the entire history of plays of the AI, they still can't find holes in its strategy; and b) it requires no data, just the rules of the game.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2019/02/real-world_applications_imperfect-information_games.png" class="kg-image" alt="AAAI 2019 Highlights: Dialogue, reproducibility, and more"><figcaption>Most real-world applications are imperfect-information games</figcaption></figure><p>For solving such games, the quality of the solution depends on the quality of the abstraction. Developing better abstractions is thus important, which also applies to modelling such games. In imperfect-information games, planning is important. In real-time planning, we must consider how the opponent can adapt to changes in the policy. In contrast to perfect-information games, states do not have well-defined values.</p><h1 id="inductive-biases">Inductive biases</h1><p>There were several papers that incorporated different inductive biases into existing models:</p><ul><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-GuptaPankaj1.4838.pdf"><strong>Document Informed Neural Autoregressive Topic Models with Distributional Prior</strong></a>: An extension of the <a href="https://papers.nips.cc/paper/4613-a-neural-autoregressive-topic-model.pdf">DocNADE</a> topic model using word embedding vectors as prior. The model is evaluated on 15 datasets.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-XiaQ.7468.pdf"><strong>Syntax-aware Neural Semantic Role Labeling</strong></a>: The authors incorporate various syntax features into a semantic role labelling model. In contrast to common practice, which often tries to incorporate syntax via a TreeLSTM, they find that shortest dependency path and tree position features perform best. </li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-LuY.5171.pdf"><strong>Relation Structure-Aware Heterogeneous Information Network Embedding</strong></a>: A network embedding model that treats different relations differently: For affiliation relations (<em>"papers are published in conferences")</em> Euclidean distance is used, while for interaction relations (<em>"authors write papers")</em> a translation-based distance is used.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-GuoM.1484.pdf"><strong>Gaussian Transformer: a Lightweight Approach for Natural Language Inference</strong></a>: A Transformer with a Gaussian prior for the self-attention that encourages focusing on neighbouring tokens.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-LeeJ.6432.pdf"><strong>Gradient-based Inference for Networks with Output Constraints</strong></a>: A method to incorporate output constraints, e.g. matching number of brackets for syntactic parsing, agreement with parse spans for SRL, etc. into the model via gradient-based inference at test-time. The method is extensively evaluated and also performs well on out-of-domain data.</li><li><strong><a href="https://arxiv.org/abs/1811.00146">ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning</a>: </strong>A collection of 300k textual descriptions focusing on if-then relations with variables. Multi-task models that exploit the hierarchical structure of the data perform better.</li></ul><h1 id="transfer-learning">Transfer learning</h1><p>Papers on transfer learning ranged from multi-task learning and semi-supervised learning to sequential and zero-shot transfer:</p><ul><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-ChenLingzhen.6418.pdf"><strong>Transfer Learning for Sequence Labeling using Source Model and Target Data</strong></a>: Extension of fine-tuning techniques for NER for the case where the target task includes labels from the source domain (as well as new labels). 1) Output layer is extended with embeddings for new labels. 2) A BiLSTM takes the features of the source model as input and feeds its output to the target model. </li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-SanhV.7138.pdf"><strong>A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks</strong></a>: A hierarchical model that jointly learns coreference resolution, relation extraction, entity mention detection, and NER. It achieves state of the art on 3/4 tasks. (<em>Disclaimer: I'm a co-author of this paper.</em>)</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-SachanD.7236.pdf"><strong>Revisiting LSTM Networks for Semi-Supervised Text Classification via Mixed Objective Function</strong></a>: A combination of entropy minimization, adversarial and virtual adversarial training with a simple 1-layer BiLSTM achieves state-of-the-art results on multiple text classification datasets. </li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-RijhwaniS.6382.pdf"><strong>Zero-shot Neural Transfer for Cross-lingual Entity Linking</strong></a>: A cross-lingual entity linking model that trains a character-based entity similarity encoder on a bilingual lexicon of entities. Conceptually similar to <a href="https://arxiv.org/abs/1706.04902">cross-lingual word embedding models</a>. For languages that do not share the same script, words are transcribed to phonemes.</li><li><strong><a href="https://arxiv.org/abs/1808.10059">Zero-Shot Adaptive Transfer for Conversational Language Understanding</a>: </strong>A model that performs zero-shot slot tagging by embedding the slot description and fine-tuning a pretrained model on the target domain.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-SiddhantA.2024.pdf"><strong>Unsupervised Transfer learning for Spoken Language Understanding in Intelligent Agents</strong></a>: A more light-weight ELMo model that pretrains a shared BiLSTM layer for intent classification and entity tagging and fine-tunes it with ULMFiT techniques.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-RuderS.6318.pdf"><strong>Latent Multi-task Architecture Learning</strong></a>: A multi-task learning architecture that enables more flexible parameter sharing between tasks and generalizes existing transfer and multi-task learning architectures. (<em>Disclaimer: I'm a co-author of this paper.</em>)</li><li><strong><a href="https://arxiv.org/abs/1811.11456">GIRNet: Interleaved Multi-Task Recurrent State Sequence Models</a></strong>: A multi-task learning model that leverages the output from auxiliary models based on position-dependent gates. The model is applied to sentiment analysis and POS tagging of code-switched data and target-dependent sentiment analysis.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-ZhangLipeng.3275.pdf"><strong>A Generalized Language Model in Tensor Space</strong></a>: A higher-order language model that builds a representation based on the tensor product of word vectors. The model achieves strong results on PTB and WikiText.</li></ul><h1 id="word-embeddings">Word embeddings</h1><p>Naturally there were also a number of papers that provided new methods for learning word embeddings:</p><ul><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-LiuTianlin.5754.pdf"><strong>Unsupervised Post-processing of Word Vectors via Conceptor Negation</strong></a>: A post-processing method that uses conceptors (a linear transformation) to dampen directions where a word vector has high variances. Post-processed embeddings not only improve on word similarity, but also on dialogue state tracking.</li><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-AbdallaM.6635.pdf"><strong>Enriching Word Embeddings with a Regressor Instead of Labeled Corpora</strong></a>: A method that enriches word embeddings during training with sentiment information based on a regressor trained on valence information from a sentiment lexicon. The enriched embeddings improve performance on sentiment and non-sentiment tasks.</li><li><strong><a href="https://arxiv.org/abs/1811.03866">Learning Semantic Representations for Novel Words: Leveraging Both Form and Context</a>: </strong>A model that learns representations for novel words both from the surface form and the context—in contrast to previous models that only leverage one of the sources.</li></ul><h1 id="miscellaneous">Miscellaneous</h1><p>Finally, here are some papers that I enjoyed that do not fit into any of the above categories:</p><ul><li><a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-DalviF.5894.pdf"><strong>What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models</strong></a>: A supervised method to extract relevant neurons with regard to a task (by correlating neurons with the target property) and an unsupervised method to extract salient neurons with regard to the model (by correlating neurons across models). Techniques are evaluated on NMT and language modelling.</li><li><strong><a href="https://arxiv.org/abs/1811.12181">What Should I Learn First: Introducing LectureBank for NLP Education and Prerequisite Chain Learning</a></strong>: A dataset containing 1,352 NLP lecture files classified according to a taxonomy with 208 prerequisite relation topics. A model is trained to learn prerequisite relations to answer "what should one learn first".</li></ul><p><em>Cover image: AAAI-19 Opening Reception</em></p>]]></content:encoded></item><item><title><![CDATA[The 4 Biggest Open Problems in NLP]]></title><description><![CDATA[This is the second post based on the Frontiers of NLP session at the Deep Learning Indaba 2018. It discusses 4 major open problems in NLP.]]></description><link>http://ruder.io/4-biggest-open-problems-in-nlp/</link><guid isPermaLink="false">5c76ff7c1b9b0d18555b9eb0</guid><category><![CDATA[natural language processing]]></category><category><![CDATA[cross-lingual]]></category><category><![CDATA[events]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Tue, 15 Jan 2019 15:28:00 GMT</pubDate><media:content url="http://ruder.io/content/images/2019/01/narrativeqa_ghostbuster.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2019/01/narrativeqa_ghostbuster.png" alt="The 4 Biggest Open Problems in NLP"><p>This post discusses 4 major open problems in NLP based on an expert survey and a panel discussion at the Deep Learning Indaba.</p><p>This is the second blog post in a two-part series. The series expands on the Frontiers of Natural Language Processing session organized by <a href="https://www.kamperh.com/">Herman Kamper</a>, <a href="http://www.stephangouws.com/">Stephan Gouws</a>, and me at the <a href="http://www.deeplearningindaba.com/">Deep Learning Indaba 2018</a>. Slides of the entire session can be found <a href="https://drive.google.com/file/d/15ehMIJ7wY9A7RSmyJPNmrBMuC7se0PMP/view">here</a>. The <a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/">first post</a> discussed major recent advances in NLP focusing on neural network-based methods. This post discusses major open problems in NLP. You can find a recording of the panel discussion this post is based on <a href="https://youtu.be/sGVi4gb90zk?list=PLICxY_yQeGYlcjO6ANCXworXHGC6hHXjA&amp;t=3649">here</a>.</p><p>In the weeks leading up to the Indaba, we asked NLP experts a number of simple but big questions. Based on the responses, we identified the four problems that were mentioned most often:</p><ol><li><a>Natural language understanding</a></li><li><a>NLP for low-resource scenarios</a></li><li><a>Reasoning about large or multiple documents</a></li><li><a>Datasets, problems, and evaluation</a></li></ol><p>We discussed these problems during a panel discussion. This article is mostly based on the <a href="https://docs.google.com/document/d/18NoNdArdzDLJFQGBMVMsQ-iLOowP1XXDaSVRmYN0IyM/edit">responses from our experts</a> (which are well worth reading) and thoughts of my fellow panel members <a href="https://twitter.com/alienelf?lang=en">Jade Abbott</a>, <a href="http://www.stephangouws.com/">Stephan Gouws</a>, <a href="http://omojumiller.com/">Omoju Miller</a>, and <a href="https://twitter.com/bernardt_d">Bernardt Duvenhage</a>. I will aim to provide context around some of the arguments, for anyone interested in learning more.</p><figure class="kg-card kg-image-card"><img src="http://ruder.io/content/images/2018/12/fellow_panel_members.png" class="kg-image" alt="The 4 Biggest Open Problems in NLP"></figure><h1 id="natural-language-understanding">Natural language understanding</h1><blockquote>I think the biggest open problems are all related to natural language<br>understanding. [...] <strong>we should develop systems that read and<br>understand text the way a person does</strong>, by forming a representation of<br>the world of the text, with the agents, objects, settings, and the<br>relationships, goals, desires, and beliefs of the agents, and everything else<br>that humans create to understand a piece of text. Until we can do that, all of our progress is in improving our systems’ ability to do pattern matching.<br>– Kevin Gimpel</blockquote><p>Many experts in our survey argued that the problem of natural language understanding (NLU) is central as it is a prerequisite for many tasks such as natural language generation (NLG). The consensus was that none of our current models exhibit 'real' understanding of natural language.</p><p><strong>Innate biases vs. learning from scratch</strong>   A key question is what biases and structure should we build explicitly into our models to get closer to NLU. Similar ideas were discussed at the Generalization workshop at NAACL 2018, which Ana Marasovic <a href="https://thegradient.pub/frontiers-of-generalization-in-natural-language-processing/">reviewed for The Gradient</a> and I <a href="http://ruder.io/highlights-naacl-2018/#generalization">reviewed here</a>. Many responses in our survey mentioned that models should incorporate common sense. In addition, dialogue systems (and chat bots) were mentioned several times.</p><p>On the other hand, for reinforcement learning, David Silver argued that you would ultimately want the model to <a href="https://twitter.com/seb_ruder/status/1040241906066829313">learn everything by itself, including the algorithm, features, and predictions</a>. Many of our experts took the opposite view, arguing that you should actually build in some understanding in your model. What should be learned and what should be hard-wired into the model was also explored in the <a href="https://www.abigailsee.com/2018/02/21/deep-learning-structure-and-innate-priors.html">debate between Yann LeCun and Christopher Manning</a> in February 2018.</p><p><strong>Program synthesis</strong>   Omoju argued that incorporating understanding is difficult as long as we do not understand the mechanisms that actually underly NLU and how to evaluate them. She argued that we might want to take ideas from <a href="https://en.wikipedia.org/wiki/Program_synthesis">program synthesis</a> and automatically learn programs based on high-level specifications instead. Ideas like this are related to <a href="https://arxiv.org/abs/1511.02799">neural module networks</a> and <a href="https://arxiv.org/abs/1511.06279">neural programmer-interpreters</a>.<br><br>She also suggested we should look back to approaches and frameworks that were originally developed in the 80s and 90s, such as <a href="https://framenet.icsi.berkeley.edu/">FrameNet</a> and merge these with statistical approaches. This should help us infer common sense-properties of objects, such as whether a car is a vehicle, has handles, etc. Inferring such common sense knowledge has also been a focus of <a href="http://ruder.io/10-exciting-ideas-of-2018-in-nlp/#3-common-sense-inference-datasets">recent datasets in NLP</a>. </p><p><strong>Embodied learning</strong>   Stephan argued that we should use the information in available structured sources and knowledge bases such as <a href="https://en.wikipedia.org/wiki/Wikidata">Wikidata</a>. He noted that humans learn language through experience and interaction, by being embodied in an environment. One could argue that there exists a single learning algorithm that if used with an agent embedded in a sufficiently rich environment, with an appropriate reward structure, could learn NLU from the ground up. However, the compute for such an environment would be tremendous. For comparison, AlphaGo required a huge infrastructure to solve a well-defined board game. The creation of a general-purpose algorithm that can continue to learn is related to <a href="http://axon.cs.byu.edu/~martinez/classes/678/Presentations/Martin.pdf">lifelong learning</a> and to<a href="https://arxiv.org/abs/1802.08864"> general problem solvers</a>.</p><p>While many people think that we are headed in the direction of embodied learning, we should thus not underestimate the infrastructure and compute that would be required for a full embodied agent. In light of this, waiting for a full-fledged embodied agent to learn language seems ill-advised. However, we can take steps that will bring us closer to this extreme, such as <a href="https://arxiv.org/abs/1706.06551">grounded language learning in simulated environments</a>, <a href="https://openreview.net/forum?id=rJeXCo0cYX">incorporating interaction</a>, or <a href="http://aclweb.org/anthology/D18-1166">leveraging multimodal data</a>.</p><p><strong>Emotion</strong>   Towards the end of the session, Omoju argued that it will be very difficult to incorporate a human element relating to emotion into embodied agents. Emotion, however, is very relevant to a deeper understanding of language. On the other hand, we might not need agents that actually possess human emotions. Stephan stated that the Turing test, after all, is defined as mimicry and sociopaths—while having no emotions—can fool people into thinking they do. We should thus be able to find solutions that do not need to be embodied and do not have emotions, but understand the emotions of people and help us solve our problems. Indeed, sensor-based emotion recognition systems <a href="http://eqradio.csail.mit.edu/">have continuously improved—</a>and we have also seen improvements in <a href="https://www.aclweb.org/anthology/D17-1169">textual emotion detection systems</a>.</p><p><strong>Cognitive and neuroscience</strong>   An audience member asked how much knowledge of neuroscience and cognitive science are we leveraging and building into our models. Knowledge of neuroscience and cognitive science can be great for inspiration and used as a guideline to shape your thinking. As an example, <a href="https://papers.nips.cc/paper/6057-using-fast-weights-to-attend-to-the-recent-past.pdf">several</a> <a href="https://papers.nips.cc/paper/7120-thinking-fast-and-slow-with-deep-learning-and-tree-search.pdf">models</a> have sought to imitate humans' ability to <em>think fast and slow</em>. AI and neuroscience are complementary in many directions, as Surya Ganguli illustrates in <a href="https://neuroscience.stanford.edu/news/intertwined-quest-understanding-biological-intelligence-and-creating-artificial-intelligence">this post</a>.</p><p>Omoju recommended to take inspiration from theories of cognitive science, such as the cognitive development theories by <a href="https://www2.education.uiowa.edu/html/eportfolio/tep/07p075folder/Piaget_Vygotsky.htm">Piaget and Vygotsky</a>. She also urged everyone to pursue interdisciplinary work. This sentiment was echoed by other experts. For instance, Felix Hill recommended to go to cognitive science conferences.</p><h1 id="nlp-for-low-resource-scenarios">NLP for low-resource scenarios</h1><blockquote>Dealing with low-data settings (low-resource languages, dialects (including social media text "dialects"), domains, etc.).  This is not a completely "open" problem in that there are already a lot of promising ideas out there; <strong>but we still don't have a universal solution to this universal problem</strong>.<br>– Karen Livescu</blockquote><p>The second topic we explored was generalisation beyond the training data in low-resource scenarios. Given the setting of the Indaba, a natural focus was low-resource languages. The first question focused on whether it is necessary to develop specialised NLP tools for specific languages, or it is enough to work on general NLP.</p><p><strong>Universal language model</strong>   Bernardt argued that there are universal commonalities between languages that could be exploited by a universal language model. The challenge then is to obtain enough data and compute to train such a language model. This is closely related to recent efforts to train a <a href="https://github.com/google-research/bert/blob/master/multilingual.md">cross-lingual Transformer language model</a> and <a href="https://arxiv.org/abs/1812.10464">cross-lingual sentence embeddings</a>. </p><p><strong>Cross-lingual representations</strong>   Stephan remarked that not enough people are working on low-resource languages. There are <a href="https://en.wikipedia.org/wiki/Languages_of_Africa">1,250-2,100 languages in Africa</a> alone, most of which have received scarce attention from the NLP community. The question of specialized tools also depends on the NLP task that is being tackled. The main issue with current models is sample efficiency. <a href="https://arxiv.org/abs/1706.04902">Cross-lingual word embeddings</a> are sample-efficient as they only require word translation pairs or even only monolingual data. They align word embedding spaces sufficiently well to do coarse-grained tasks like topic classification, but don't allow for more fine-grained tasks such as machine translation. Recent efforts nevertheless show that these embeddings form an important building lock for <a href="http://ruder.io/10-exciting-ideas-of-2018-in-nlp/#1-unsupervised-mt">unsupervised machine translation</a>. </p><p>More complex models for higher-level tasks such as question answering on the other hand require thousands of training examples for learning. Transferring tasks that require actual natural language understanding from high-resource to low-resource languages is still very challenging. With the development of cross-lingual datasets for such tasks, such as <a href="https://arxiv.org/abs/1809.05053">XNLI</a>, the development of strong cross-lingual models for more reasoning tasks should hopefully become easier. </p><p><strong>Benefits and impact</strong>   Another question enquired—given that there is inherently only small amounts of text available for under-resourced languages—whether the benefits of NLP in such settings will also be limited. Stephan vehemently disagreed, reminding us that as ML and NLP practitioners, we typically tend to view problems in an information theoretic way, e.g. as maximizing the likelihood of our data or improving a benchmark. <strong>Taking a step back, the actual reason we work on NLP problems is to build systems that break down barriers. We want to build models that enable people to read news that was not written in their language, ask questions about their health when they don't have access to a doctor, etc.</strong></p><p>Given the potential impact, building systems for low-resource languages is in fact one of the most important areas to work on. While one low-resource language may not have a lot of data, there is a long tail of low-resource languages; most people on this planet in fact speak a language that is in the low-resource regime. We thus really need to find a way to get our systems to work in this setting.<br><br>Jade opined that it is almost ironic that as a community we have been focusing on languages with a lot of data as these are the languages that are well taught around the world. <strong>The languages we should really focus on are the low-resource languages where not much data is available.</strong> The great thing about the Indaba is that people are working and making progress on such low-resource languages. Given the scarcity of data, even simple systems such as bag-of-words will have a large real-world impact. Etienne Barnard, one of the audience members, noted that he observed a different effect in real-world speech processing: Users were often more motivated to use a system in English if it works for their dialect compared to using a system in their own language.</p><p><strong>Incentives and skills</strong>   Another audience member remarked that people are incentivized to work on highly visible benchmarks, such as English-to-German machine translation, but incentives are missing for working on low-resource languages. Stephan suggested that incentives exist in the form of unsolved problems. However, skills are not available in the right demographics to address these problems. What we should focus on is to teach skills like machine translation in order to empower people to solve these problems. Academic progress unfortunately doesn't necessarily relate to low-resource languages. However, if cross-lingual benchmarks become more pervasive, then this should also lead to more progress on low-resource languages.</p><p><strong>Data availability</strong>   Jade finally argued that a big issue is that there are no datasets available for low-resource languages, such as languages spoken in Africa. If we create datasets and make them easily available, such as hosting them on <a href="https://africaopendata.org/">openAFRICA</a>, that would incentivize people and lower the barrier to entry. It is often sufficient to make available test data in multiple languages, as this will allow us to evaluate cross-lingual models and track progress. Another data source is the <a href="https://www.sadilar.org/">South African Centre for Digital Language Resources (SADiLaR)</a>, which provides resources for many of the languages spoken in South Africa.</p><h1 id="reasoning-about-large-or-multiple-documents">Reasoning about large or multiple documents</h1><blockquote>Representing large contexts efficiently. <strong>Our current models are mostly based on recurrent neural networks, which cannot represent longer contexts well. </strong>[...] The stream of work on graph-inspired RNNs is potentially promising, though has only seen modest improvements and has not been widely adopted due to them being much less straight-forward to train than a vanilla RNN.<br>– Isabelle Augenstein</blockquote><p>Another big open problem is reasoning about large or multiple documents. The recent <a href="http://ruder.io/10-exciting-ideas-of-2018-in-nlp/#9-qa-and-reasoning-with-large-documents">NarrativeQA dataset</a> is a good example of a benchmark for this setting. Reasoning with large contexts is closely related to NLU and requires scaling up our current systems dramatically, until they can read entire books and movie scripts. A key question here—that we did not have time to discuss during the session—is whether we need better models or just train on more data.</p><p>Endeavours such as <a href="https://openai.com/five/">OpenAI Five</a> show that current models can do a lot if they are scaled up to work with a lot more data and a lot more compute. With sufficient amounts of data, our current models might similarly do better with larger contexts. The problem is that supervision with large documents is scarce and expensive to obtain. Similar to language modelling and <a href="https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf">skip-thoughts</a>, we could imagine a document-level unsupervised task that requires predicting the next paragraph or chapter of a book or deciding which chapter comes next. However, this objective is likely too sample-inefficient to enable learning of useful representations.</p><p>A more useful direction thus seems to be to develop methods that can represent context more effectively and are better able to keep track of relevant information while reading a document. Multi-document summarization and <a href="https://arxiv.org/abs/1901.00603">multi-document question answering</a> are steps in this direction. Similarly, we can build on language models with improved <a href="https://arxiv.org/abs/1612.03969">memory</a> and <a href="https://arxiv.org/abs/1703.03129">lifelong learning</a> capabilities.</p><h1 id="datasets-problems-and-evaluation">Datasets, problems, and evaluation</h1><blockquote>Perhaps the biggest problem is to <strong>properly define the problems themselves.</strong> And by properly defining a problem, I mean building <strong>datasets and evaluation</strong> procedures that are appropriate to measure our progress towards concrete goals. Things would be easier if we could reduce everything to Kaggle style competitions!<br>– Mikel Artetxe </blockquote><p>We did not have much time to discuss problems with our current benchmarks and evaluation settings but you will find many relevant responses in <a href="https://docs.google.com/document/d/18NoNdArdzDLJFQGBMVMsQ-iLOowP1XXDaSVRmYN0IyM/edit">our survey</a>. The final question asked what the most important NLP problems are that should be tackled for societies in Africa. Jade replied that the most important issue is to solve the low-resource problem. Particularly being able to use translation in education to enable people to access whatever they want to know in their own language is tremendously important.</p><p>The session concluded with general advice from our experts on other questions that we had asked them, such as "<em>What, if anything, has led the field in the wrong direction?</em>" and "<em>What advice would you give a postgraduate student in NLP starting their project now?</em>" You can find responses to all questions in <a href="https://docs.google.com/document/d/18NoNdArdzDLJFQGBMVMsQ-iLOowP1XXDaSVRmYN0IyM/edit#">the survey</a>.</p><h2 id="deep-learning-indaba-2019">Deep Learning Indaba 2019</h2><p>If you are interested in working on low-resource languages, consider attending the <a href="http://www.deeplearningindaba.com/indaba-2019.html">Deep Learning Indaba 2019</a>, which takes place in Nairobi, Kenya from 25-31 August 2019.</p><p><em>Credit: Title image text is from the <a href="https://arxiv.org/abs/1712.07040">NarrativeQA dataset</a>. The image is from the <a href="https://drive.google.com/file/d/15ehMIJ7wY9A7RSmyJPNmrBMuC7se0PMP/view">slides of the NLP session</a>.</em></p>]]></content:encoded></item><item><title><![CDATA[10 Exciting Ideas of 2018 in NLP]]></title><description><![CDATA[This post gathers 10 ideas that I found exciting and impactful this year—and that we'll likely see more of in the future. For each idea, it highlights 1-2 papers that execute them well.]]></description><link>http://ruder.io/10-exciting-ideas-of-2018-in-nlp/</link><guid isPermaLink="false">5c76ff7c1b9b0d18555b9eb7</guid><category><![CDATA[transfer learning]]></category><category><![CDATA[multi-task learning]]></category><category><![CDATA[meta-learning]]></category><category><![CDATA[natural language processing]]></category><category><![CDATA[semi-supervised learning]]></category><category><![CDATA[language models]]></category><category><![CDATA[cross-lingual]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Wed, 19 Dec 2018 19:28:46 GMT</pubDate><media:content url="http://ruder.io/content/images/2018/12/syntactic_scaffold-1.png" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2018/12/syntactic_scaffold-1.png" alt="10 Exciting Ideas of 2018 in NLP"><p>This post gathers 10 ideas that I found exciting and impactful this year—and that we'll likely see more of in the future.</p><p>For each idea, I will highlight 1-2 papers that execute them well. I tried to keep the list succinct, so apologies if I did not cover all relevant work. The list is necessarily subjective and covers ideas mainly related to transfer learning and generalization. Most of these (with some exceptions) are not trends (but I suspect that some might become more 'trendy' in 2019). Finally, I would love to read about your highlights in the comments or see highlights posts about other areas.</p><h2 id="1-unsupervised-mt">1) Unsupervised MT</h2><p>There were <a href="https://arxiv.org/abs/1711.00043">two</a> <a href="https://arxiv.org/abs/1710.11041">unsupervised</a> MT papers at ICLR 2018. They were <em>surprising</em> in that they worked at all, but results were still low compared to supervised systems. At EMNLP 2018, unsupervised MT hit its stride with <a href="https://arxiv.org/abs/1804.07755">two</a> <a href="https://arxiv.org/abs/1809.01272">papers</a> from the same two groups that significantly improve upon their previous methods. My highlight:</p><ul><li><a href="https://arxiv.org/abs/1804.07755"><strong>Phrase-Based &amp; Neural Unsupervised Machine Translation</strong> (EMNLP 2018)</a>:  The paper does a nice job in distilling the three key requirements for unsupervised MT: a good initialization, language modelling, and modelling the inverse task (via back-translation). All three are also beneficial in other unsupervised scenarios, as we will see below. Modelling the inverse task enforces cyclical consistency, which has been employed in different approaches—most prominently in <a href="https://arxiv.org/abs/1703.10593">CycleGAN</a>. The paper performs extensive experiments and evaluates even on two low-resource language pairs, English-Urdu and English-Romanian. We will hopefully see more work on low-resource languages in the future.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2018/12/phrase_based_and_neural_unsupervised_mt.png" class="kg-image" alt="10 Exciting Ideas of 2018 in NLP"><figcaption>Toy illustration of the three principles of unsupervised MT. A) Two monolingual datasets. B) Initialization. C) Language modelling. D) Back-translation <a href="https://arxiv.org/abs/1804.07755">(Lample et al., 2018)</a>.</figcaption></figure><h2 id="2-pretrained-language-models">2) Pretrained language models</h2><p>Using pretrained language models is probably the <a href="http://ruder.io/nlp-imagenet/">most significant NLP trend</a> this year, so I won't spend much time on it here. There have been a slew of memorable approaches: <a href="https://arxiv.org/abs/1802.05365">ELMo</a>, <a href="https://arxiv.org/abs/1801.06146">ULMFiT</a>, <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">OpenAI Transformer</a>, and <a href="https://arxiv.org/abs/1810.04805">BERT</a>. My highlight:</p><ul><li><a href="https://arxiv.org/abs/1802.05365"><strong>Deep contextualized word representations</strong> (NAACL-HLT 2018)</a>: The paper that introduced ELMo has been much lauded. Besides the impressive empirical results, where it shines is the careful analysis section that teases out the impact of various factors and analyses the information captured in the representations. The word sense disambiguation (WSD) analysis by itself (below on the left) is well executed. Both demonstrate that a LM on its own provides WSD and POS tagging performance close to the state-of-the-art.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2018/12/wsd_and_pos_tagging_results.png" class="kg-image" alt="10 Exciting Ideas of 2018 in NLP"><figcaption>Word sense disambiguation (left) and POS tagging (right) results of first and second layer bidirectional language model compared to baselines <a href="https://arxiv.org/abs/1802.05365">(Peters et al., 2018)</a>.</figcaption></figure><h2 id="3-common-sense-inference-datasets">3) Common sense inference datasets</h2><p>Incorporating common sense into our models is one of the most important directions moving forward. However, creating good datasets is not easy and even popular ones <a href="http://aclweb.org/anthology/N18-2017">show</a> <a href="http://www.aclweb.org/anthology/S18-2023">large</a> biases. This year, there have been some well-executed datasets that seek to teach models some common sense such as <a href="https://arxiv.org/abs/1805.06939">Event2Mind</a> and <a href="https://arxiv.org/abs/1808.05326">SWAG</a>, both from the University of Washington. SWAG was solved <a href="https://twitter.com/seb_ruder/status/1050727451138150400">unexpectedly quickly</a>. My highlight:</p><ul><li><strong><a href="http://visualcommonsense.com/">Visual Commonsense Reasoning</a></strong><a href="http://visualcommonsense.com/"> (arXiv 2018)</a>: This is the first visual QA dataset that includes a rationale (an explantation) with each answer. In addition, questions require complex reasoning. The creators go to great lengths to address possible bias by ensuring that every answer's prior probability of being correct is 25% (every answer appears 4 times in the entire dataset, 3 times as an incorrect answer and 1 time as the correct answer); this requires solving a constrained optimization problem using models that compute relevance and similarity. Hopefully preventing possible bias will become a common component when creating datasets. Finally, just look at the gorgeous presentation of the data 👇.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2018/12/visual_commonsense_reasoning.png" class="kg-image" alt="10 Exciting Ideas of 2018 in NLP"><figcaption>VCR: Given an image, a list of regions, and a question, a model must answer the question and provide a rationale explaining why its answer is right <a href="https://arxiv.org/abs/1811.10830">(Zellers et al., 2018)</a>.</figcaption></figure><h2 id="4-meta-learning">4) Meta-learning</h2><p>Meta-learning has seen much use in few-shot learning, reinforcement learning, and robotics—the most prominent example: <a href="https://arxiv.org/abs/1703.03400">model-agnostic meta-learning (MAML)</a>—but successful applications in NLP have been rare. Meta-learning is most useful for problems with a limited number of training examples. My highlight:</p><ul><li><a href="http://aclweb.org/anthology/D18-1398"><strong>Meta-Learning for Low-Resource Neural Machine Translation</strong> (EMNLP 2018)</a>: The authors use MAML to learn a good initialization for translation, treating each language pair as a separate meta-task. Adapting to low-resource languages is probably the most useful setting for meta-learning in NLP. In particular, combining multilingual transfer learning (such as <a href="https://github.com/google-research/bert/blob/master/multilingual.md">multilingual BERT</a>), unsupervised learning, and meta-learning is a promising direction.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2018/12/meta-learning_vs_transfer_learning.png" class="kg-image" alt="10 Exciting Ideas of 2018 in NLP"><figcaption>The difference between transfer learning multilingual transfer learning, and meta-learning. Solid lines: learning of the initialization. Dashed lines: Path of fine-tuning <a href="http://aclweb.org/anthology/D18-1398">(Gu et al., 2018)</a>.</figcaption></figure><h2 id="5-robust-unsupervised-methods">5) Robust unsupervised methods</h2><p>This year, <a href="http://aclweb.org/anthology/P18-1072">we</a> <a href="http://aclweb.org/anthology/D18-1056">and</a> others have observed that unsupervised cross-lingual word embedding methods break down when languages are dissimilar. This is a common phenomenon in transfer learning where a discrepancy between source and target settings (e.g. domains in <a href="https://www.cs.jhu.edu/~mdredze/publications/sentiment_acl07.pdf">domain adaptation</a>, tasks in <a href="https://arxiv.org/abs/1706.08840">continual learning</a> and <a href="http://www.aclweb.org/anthology/E17-1005">multi-task learning</a>) leads to deterioration or failure of the model. Making models more robust to such changes is thus important. My highlight:</p><ul><li><a href="http://www.aclweb.org/anthology/P18-1073"><strong>A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings</strong> (ACL 2018)</a>: Instead of meta-learning an initialization, this paper uses their understanding of the problem to craft a better initialization. In particular, they pair words in both languages that have a similar distribution of words they are similar to. This is a great example of using domain expertise and insights from an analysis to make a model more robust.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2018/12/similarity_distribution.png" class="kg-image" alt="10 Exciting Ideas of 2018 in NLP"><figcaption>The similarity distributions of three words. Equivalent translations ('two' and 'due') have more similar distributions than non-related words ('two' and 'cane'—meaning 'dog'; <a href="http://www.aclweb.org/anthology/P18-1073">Artexte et al., 2018</a>).</figcaption></figure><h2 id="6-understanding-representations">6) Understanding representations</h2><p>There have been a lot of efforts in better understanding representations. In particular, <a href="https://arxiv.org/abs/1608.04207">'diagnostic classifiers'</a> (tasks that aim to measure if learned representations can predict certain attributes) have become <a href="http://arxiv.org/abs/1805.01070">quite common</a>. My highlight:</p><ul><li><a href="http://aclweb.org/anthology/D18-1179"><strong>Dissecting Contextual Word Embeddings: Architecture and Representation</strong> (EMNLP 2018)</a>: This paper does a great job of better understanding pretrained language model representations. They extensively study learned word and span representations on carefully designed unsupervised and supervised tasks. The resulting finding: Pretrained representations learn tasks related to low-level morphological and syntactic tasks at lower layers and longer range semantics at higher layers. To me this really shows that pretrained language models indeed capture similar properties as <a href="https://thegradient.pub/nlp-imagenet/">computer vision models pretrained on ImageNet</a>.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2018/12/bilm_transformer_information.png" class="kg-image" alt="10 Exciting Ideas of 2018 in NLP"><figcaption>Per-layer performance of BiLSTM and Transformer pretrained representations on (from left to right) POS tagging, constituency parsing, and unsupervised coreference resolution <a href="http://aclweb.org/anthology/D18-1179">(Peters et al., 2018)</a>.</figcaption></figure><h2 id="7-clever-auxiliary-tasks">7) Clever auxiliary tasks</h2><p>In many settings, we have seen an increasing usage of multi-task learning with carefully chosen auxiliary tasks. For a good auxiliary task, data must be easily accessible. One of the most prominent examples is <a href="https://arxiv.org/abs/1810.04805">BERT</a>, which uses next-sentence prediction (that has been used in <a href="https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf">Skip-thoughts</a> and more recently in <a href="https://arxiv.org/pdf/1803.02893.pdf">Quick-thoughts</a>) to great effect. My highlights: </p><ul><li><a href="http://aclweb.org/anthology/D18-1412"><strong>Syntactic Scaffolds for Semantic Structures</strong> (EMNLP 2018)</a>: This paper proposes an auxiliary task that pretrains span representations by predicting for each span the corresponding syntactic constituent type. Despite being conceptually simple, the auxiliary task leads to large improvements on span-level prediction tasks such as semantic role labelling and coreference resolution. This papers shows that specialised representations learned at the level required by the target task (here: spans) are immensely beneficial.</li><li><strong><a href="https://arxiv.org/abs/1810.08854">pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence Inference</a></strong><a href="https://arxiv.org/abs/1810.08854"> (arXiv 2018)</a>: In a similar vein, this paper pretrains <em>word pair representations</em> by maximizing the pointwise mutual information of pairs of words with their context. This encourages the model to learn more meaningful representations of word pairs than with more general objectives, such as language modelling. The pretrained representations are effective in tasks such as SQuAD and MultiNLI that require cross-sentence inference. We can expect to see more pretraining tasks that capture properties particularly suited to certain downstream tasks and are complementary to more general-purpose tasks like language modelling.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2018/12/syntactic_scaffold.png" class="kg-image" alt="10 Exciting Ideas of 2018 in NLP"><figcaption>Syntactic, PropBank and coreference annotations from OntoNotes. PropBank SRL arguments and coreference mentions are annotated on top of syntactic constituents. Almost every argument is related to a syntactic constituent <a href="http://aclweb.org/anthology/D18-1412">(Swayamdipta et al., 2018)</a>.</figcaption></figure><h2 id="8-combining-semi-supervised-learning-with-transfer-learning">8) Combining semi-supervised learning with transfer learning</h2><p>With the recent advances in transfer learning, we should not forget more explicit ways of using target task-specific data. In fact, pretrained representations are complementary with many forms of semi-supervised learning. We have explored <a href="http://aclweb.org/anthology/P18-1096">self-labelling approaches</a>, a particular category of semi-supervised learning. My highlight: </p><ul><li><a href="http://aclweb.org/anthology/D18-1217"><strong>Semi-Supervised Sequence Modeling with Cross-View Training</strong> (EMNLP 2018)</a>: This paper shows that a conceptually very simple idea, making sure that the predictions on different views of the input agree with the prediction of the main model, can lead to gains on a diverse set of tasks. The idea is similar to word dropout but allows leveraging unlabelled data to make the model more robust. Compared to other self-ensembling models such as <a href="https://papers.nips.cc/paper/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results.pdf">mean teacher</a>, it is specifically designed for particular NLP tasks. With much work on <em>implicit</em> semi-supervised learning, we will hopefully see more work that explicitly tries to model the target predictions going forward.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2018/12/cross-view_training-1.png" class="kg-image" alt="10 Exciting Ideas of 2018 in NLP"><figcaption>Inputs seen by auxiliary prediction modules: Auxiliary 1: <em>They traveled to</em> __________________. Auxiliary 2: <em>They traveled to</em> <strong>Washington</strong> _______. Auxiliary 3: _____________ <strong>Washington</strong> <em>by plane</em>. Auxiliary 4: ________________________ <em>by plane</em> <a href="http://aclweb.org/anthology/D18-1217">(Clark et al., 2018)</a>.</figcaption></figure><h2 id="9-qa-and-reasoning-with-large-documents">9) QA and reasoning with large documents</h2><p>There have been a lot of developments in question answering (QA), with an <a href="https://arxiv.org/abs/1809.09600">array</a> <a href="https://stanfordnlp.github.io/coqa/">of</a> <a href="http://quac.ai/">new</a> <a href="https://arxiv.org/abs/1806.03822">QA</a> <a href="http://qangaroo.cs.ucl.ac.uk/">datasets</a>. Besides conversational QA and performing multi-step reasoning, the most challenging aspect of QA is to synthesize narratives and large bodies of information. My highlight: </p><ul><li><a href="http://aclweb.org/anthology/Q18-1023"><strong>The NarrativeQA Reading Comprehension Challenge</strong> (TACL 2018)</a>: This paper proposes a challenging new QA dataset based on answering questions about entire movie scripts and books. While this task is still out of reach for current methods, models are provided the option of using a summary (rather than the entire book) as context, of selecting the answer (rather than generate it), and of using the output from an IR model. These variants make the task more feasible and enable models to gradually scale up to the full setting. We need more datasets like this that present ambitious problems, but still manage to make them accessible.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2018/12/narrative_qa.png" class="kg-image" alt="10 Exciting Ideas of 2018 in NLP"><figcaption>Comparison of QA datasets <a href="http://aclweb.org/anthology/Q18-1023">(Kočiský et al., 2018)</a>.&nbsp;</figcaption></figure><h2 id="10-inductive-bias">10) Inductive bias</h2><p>Inductive biases such as convolutions in a CNN, regularization, dropout, and other mechanisms are core parts of neural network models that act as a regularizer and make models more sample-efficient. However, coming up with a broadly useful inductive bias and incorporating it into a model is challenging. My highlights:</p><ul><li><a href="http://aclweb.org/anthology/K18-1030"><strong>Sequence classification with human attention</strong> (CoNLL 2018)</a>: This paper proposes to use human attention from eye-tracking corpora to regularize attention in RNNs. Given that many current models such as Transformers use attention, finding ways to train it more efficiently is an important direction. It is also great to see another example that human language learning can help improve our computational models. </li><li><a href="http://aclweb.org/anthology/D18-1548"><strong>Linguistically-Informed Self-Attention for Semantic Role Labeling</strong> (EMNLP 2018)</a>: This paper has a lot to like: a Transformer trained jointly on both syntactic and semantic tasks; the ability to inject high-quality parses at test time; and out-of-domain evaluation. It also regularizes the Transformer's multi-head attention to be more sensitive to syntax by training one attention head to attend to the syntactic parents of each token. We will likely see more examples of Transformer attention heads used as auxiliary predictors focusing on particular aspects of the input.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2018/12/out-of-domain_srl_performance.png" class="kg-image" alt="10 Exciting Ideas of 2018 in NLP"><figcaption>10 years of PropBank semantic role labeling. Comparison of Linguistically-Informed Self-Attention (LISA) with other methods on out-of-domain data <a href="https://people.cs.umass.edu/~strubell/">(Strubell et al., 2018)</a>.</figcaption></figure>]]></content:encoded></item><item><title><![CDATA[EMNLP 2018 Highlights: Inductive bias, cross-lingual learning, and more]]></title><description><![CDATA[This post discusses highlights of EMNLP 2018. It focuses on talks and papers dealing with inductive bias, cross-lingual learning, word embeddings, latent variable models, language models, and datasets.]]></description><link>http://ruder.io/emnlp-2018-highlights/</link><guid isPermaLink="false">5c76ff7c1b9b0d18555b9eb4</guid><category><![CDATA[events]]></category><category><![CDATA[transfer learning]]></category><category><![CDATA[cross-lingual]]></category><category><![CDATA[word embeddings]]></category><category><![CDATA[language models]]></category><category><![CDATA[natural language processing]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Tue, 06 Nov 2018 10:12:40 GMT</pubDate><media:content url="http://ruder.io/content/images/2018/11/emnlp_conference_garden_view.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://ruder.io/content/images/2018/11/emnlp_conference_garden_view.jpg" alt="EMNLP 2018 Highlights: Inductive bias, cross-lingual learning, and more"><p>The post discusses highlights of the 2018 Conference on Empirical Methods in Natural Language Processing (<a href="http://emnlp2018.org/">EMNLP 2018</a>).</p><p><em><em>This post originally appeared at the <a href="http://blog.aylien.com/emnlp-2018-highlights-inductive-bias-cross-lingual-learning-and-more/">AYLIEN blog</a>.</em></em></p><p>You can find past highlights of conferences <a href="http://ruder.io/tag/events/">here</a>. You can find all 549 accepted papers in <a href="https://aclanthology.coli.uni-saarland.de/volumes/proceedings-of-the-2018-conference-on-empirical-methods-in-natural-language-processing">the EMNLP proceedings</a>. In this review, I will focus on papers that relate to the following topics: </p><ul><li><a href="#inductive-bias">Inductive bias</a></li><li><a href="#cross-lingual-learning">Cross-lingual learning</a></li><li><a href="#word-embeddings">Word embeddings</a></li><li><a href="#latent-variable-models">Latent variable models</a></li><li><a href="#language-models">Language models</a></li><li><a href="#datasets">Datasets</a></li><li><a href="#miscellaneous">Miscellaneous</a></li></ul><h2 id="inductive-bias">Inductive bias</h2><p>The <em><a href="https://en.wikipedia.org/wiki/Inductive_bias">inductive bias</a></em> of a machine learning algorithm is the set of assumptions that the model makes in order to generalize to new inputs. For instance, the inductive bias obtained through <a href="http://ruder.io/multi-task/">multi-task learning</a> encourages the model to prefer hypotheses (sets of parameters) that explain more than one task.</p><ul><li>Inductive bias was the main theme during <a href="http://www.conll.org/keynotes-2018">Max Welling</a>'s keynote at <a href="http://www.conll.org/keynotes-2018">CoNLL 2018</a>. The two key takeaways from his talk are:</li></ul><blockquote>Lesson 1: If there is symmetry in the input space, exploit it.</blockquote><p>The most canonical example for exploiting such symmetry are convolutional neural networks, which are <em>translation invariant</em>. Invariance in general means that an object is recognized as an object even if its appearance <em>varies</em> in some way. <a href="https://arxiv.org/abs/1602.07576">Group equivariant convolutional networks</a> and <a href="https://arxiv.org/abs/1612.08498">Steerable CNNs</a> similarly are <em>rotation invariant</em> (see below). Given the success of CNNs in computer vision, it is a compelling research direction to think of <em>what types of invariance are possible in language and how these can be implemented in neural networks.</em></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2018/11/invariances.png" class="kg-image" alt="EMNLP 2018 Highlights: Inductive bias, cross-lingual learning, and more"><figcaption>Translation and rotation invariance in computer vision (Source: <a href="https://stats.stackexchange.com/questions/208936/what-is-translation-invariance-in-computer-vision-and-convolutional-neural-netwo/208949#208949">Matt Krause</a>)</figcaption></figure><blockquote>Lesson 2: When you know the generative process, you should exploit it.</blockquote><p>For many problems the generative process is known but the inverse process of reconstructing the original input is not. Examples of such <em>inverse problems</em> are MRI, image denoising and super-resolution, but also audio-to-speech decoding and machine translation. The <a href="https://arxiv.org/abs/1706.04008">Recurrent Inference Machine (RIM)</a> uses an RNN to iteratively generate an incremental update to the input until a sufficiently good estimate of the true signal has been reached, which can be seen for MRI below. This can be seen as similar to producing text via editing, rewriting, and iterative refining.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2018/11/rim.gif" class="kg-image" alt="EMNLP 2018 Highlights: Inductive bias, cross-lingual learning, and more"><figcaption>Inference process of an RIM for MRI (left: generated image; middle: reference; right: error; Source: <a href="http://sbt.science.uva.nl/mri/2017/11/24/the-recurrent-inference-machine/">CIFAR</a>)</figcaption></figure><ul><li>A popular way to obtain certain types of invariance in current NLP approaches is via adversarial examples. To this end, <a href="http://aclweb.org/anthology/D18-1316">Alzantot et al.</a> use a black-box population-based optimization algorithm to generate semantic and syntactic adversarial examples.</li><li><a href="http://aclweb.org/anthology/K18-1007">Minervini and Riedel</a> propose to incorporate logic to generate adversarial examples. In particular, they use a combination of combinatorial optimization and a language model to generate examples that maximally violate such logic constraints for natural language inference.</li><li>Another form of inductive bias can be induced via regularization. In particular, <a href="http://aclweb.org/anthology/K18-1030">Barrett et al.</a> received the special paper award at CoNLL for showing that <em>human attention</em> provides a good inductive bias for attention in neural networks. The human attention is derived from eye-tracking corpora, which—importantly—can be disjoint from the training data.</li><li>For another beneficial inductive bias for attention, in one of the best papers of the conference, <a href="http://aclweb.org/anthology/D18-1548">Strubell et al.</a> encourage one attention head to attend to the syntactic parents for each token in multi-head attention. They additionally use multi-task learning and allow the injection of a syntactic parse at test time.</li><li>Many NLP tasks such as entailment and semantic similarity compute some sort of alignment between two sequences, but this alignment is either at the word or sentence level. <a href="http://aclweb.org/anthology/D18-1184">Liu et al.</a> propose to incorporate a structural bias by using <em>structured alignments</em>, which match spans in both sequences to each other.</li><li>Tree-based have been popular in NLP and encode the bias that knowledge of syntax is beneficial. <a href="http://aclweb.org/anthology/D18-1492">Shi et al.</a> analyze a phenomenon that runs counter to this, which is that trivial trees with no syntactic information often achieve better results than syntactic trees. Their key insight is that in well-performing trees, crucial words are closer to the final representation, which helps in mitigating RNNs' <a href="http://ruder.io/acl-2018-highlights/#analyzingtheinductivebias">sequential recency bias</a>.</li><li>For aspect-based sentiment analysis, sentence representations are typically computed separately from aspect representations. <a href="http://aclweb.org/anthology/D18-1136">Huang and Carley</a> propose a nice way to condition the sentence representation on the aspect by using the aspect representation as the <em>parameters of the filters or gates</em> in a CNN. Allowing encoded representations to directly parameterize other parts of a neural network might be useful for other applications, too.</li></ul><h2 id="cross-lingual-learning">Cross-lingual learning</h2><p>There are roughly 6,500 languages spoken around the world. Despite this, the predominant focus of research is on English. This seems to change perceptibly as more papers are investigating cross-lingual settings.</p><ul><li>In her <a href="http://www.conll.org/keynotes-2018">CoNLL keynote</a>, <a href="https://www.mpi.nl/people/majid-asifa">Asifa Majid</a> gives an insightful overview of how culture and language can shape our internal representation of concepts. A common example of this is Scottish having <a href="https://www.bbc.com/news/uk-scotland-34323967">421 terms for snow</a>. This phenomenon not only applies to our environment, but also to how we talk about ourselves and our bodies.</li></ul><blockquote>Languages vary surprisingly in the parts of the body they single out for naming. Variations in part of the lexicon can have knock-on effects for other parts.</blockquote><p>If you ask speakers of different languages to color in different body parts in a picture, the body parts that are associated with each term depend on the language. In Dutch, the hand is often considered to be part of the term 'arm', whereas in Japanese, the arm is more clearly delimited. Indonesian, lacking an everyday term that corresponds to 'hand', associates 'arm' with both the hand and the arm as can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://ruder.io/content/images/2018/11/hand_arm-1.jpg" class="kg-image" alt="EMNLP 2018 Highlights: Inductive bias, cross-lingual learning, and more"><figcaption>Composite images for 'arm' in Dutch, Japanese, and Indonesian <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/tops.12159">(Majid &amp; van Staden, 2015)</a></figcaption></figure><p>The representations we obtain from language influence every form of perception. Hans Henning claimed that "olfactory (i.e. related to smell) abstraction is impossible". Most languages lack terms describing specific scents and odours. In contrast, the <a href="https://en.wikipedia.org/wiki/Jahai_people">Jahai</a>, a people of hunter-gatherers in Malaysia, have half a dozen terms for different qualities of smell, which allow them to identify smells much more precisely <a href="http://rstb.royalsocietypublishing.org/content/373/1752/20170139">(Majid et al., 2018)</a>.</p><p>There was a surprising amount of work on <a href="http://ruder.io/cross-lingual-embeddings/">cross-lingual word embeddings</a> at the conference. Taking insights from Asifa's talk, it will be interesting to <em>incorporate insights from psycholinguistics</em> in how we model words across languages and different cultures, as cross-lingual embeddings have mostly focused on word-to-word alignment and so far did not even consider polysemy.</p><ul><li>For cross-lingual word embeddings, <a href="http://aclweb.org/anthology/K18-1021">Kementchedjhieva et al.</a> show that mapping the languages onto a third, latent space (the mean of the monolingual embedding spaces) rather than directly onto each other, makes it easier to learn an alignment. This approach also naturally enables the integration of supporting languages in low resource scenarios. (Note: I'm a co-author on this paper.)</li><li>With a similar goal in mind, <a href="http://aclweb.org/anthology/D18-1027">Doval et al.</a> propose to move each word vector towards the mean between its current representation and the representation of the translation in a separate refinement step.</li><li>Similar to using multilingual support, <a href="http://aclweb.org/anthology/D18-1024">Chen and Cardie</a> propose to jointly learn cross-lingual embeddings between multiple languages by modeling the relations between all language pairs.</li><li><a href="http://aclweb.org/anthology/D18-1056">Hartmann et al.</a> analyze an observation of our <a href="http://aclweb.org/anthology/P18-1072">ACL 2018 paper</a>: Aligning embedding spaces induced with different algorithms <em>does not work</em>. They show, however, that a linear transformation still exists and hypothesize that the optimization problem of learning this transformation might be complicated by the algorithms' different inductive biases.</li><li>Not only word embedding spaces induced by different algorithms, but also word embedding spaces in different languages have different structures, especially for distant languages. <a href="http://aclweb.org/anthology/D18-1047">Nakashole</a> proposes to learn a transformation that is sensitive to the local neighborhood, which is particularly beneficial for distant languages.</li><li>For the same problem, <a href="http://aclweb.org/anthology/D18-1043">Hoshen and Wolf</a> propose to first align the second moment of the word distributions and then iteratively refine the alignment.</li><li><a href="http://aclweb.org/anthology/D18-1214">Alvarez-Melis and Jaakkola</a> offer a different perspective on word-to-word translation by viewing it as an optimal transport problem. They use the Gromov-Wasserstein distance to measure similarities between pairs of words across languages.</li><li><a href="http://aclweb.org/anthology/D18-1268">Xu et al.</a> instead propose to minimize the Sinkhorn distance between the source and target distributions.</li><li><a href="http://aclweb.org/anthology/D18-1023">Huang et al.</a> go beyond word alignment with their approach. They introduce multiple cluster-level alignments and additionally enforce the clusters to be consistently distributed across multiple languages.</li><li>In one of the best papers of the conference, <a href="http://aclweb.org/anthology/D18-1549">Lample et al.</a> proposes an unsupervised phrase-based machine translation model, which works particularly well for low-resource languages. On Urdu-English, it outperforms a supervised phrase-based model trained on 800,000 noisy and out-of-domain parallel sentences.</li><li><a href="http://aclweb.org/anthology/D18-1399">Artetxe et al.</a> propose a similar phrase-based approach to unsupervised machine translation.</li></ul><h2 id="word-embeddings">Word embeddings</h2><p>Besides cross-lingual word embeddings, there was naturally also work investigating and improving word embeddings, but this seemed to be a lot less pervasive than in past years.</p><ul><li><a href="http://aclweb.org/anthology/D18-1057">Zhuang et al.</a> propose to use second-order co-occurrence relations to train word embeddings via a newly designed metric.</li><li><a href="http://aclweb.org/anthology/D18-1059">Zhao et al.</a> propose to learn word embeddings for out-of-vocabulary words by viewing words as bags of character n-grams.</li><li><a href="http://aclweb.org/anthology/D18-1181">Bosc and Vincent</a> learn word embeddings by reconstructing dictionary definitions.</li><li><a href="http://aclweb.org/anthology/D18-1521">Zhao et al.</a> learn gender-neutral word embeddings rather than removing the bias from trained embeddings. Their approach allocates certain dimensions of the embedding to gender information, while it keeps the remaining dimensions gender-neutral.</li></ul><h2 id="latent-variable-models">Latent variable models</h2><p>Latent variable models are slowly emerging as a useful tool to express a structural inductive bias and to model the linguistic structure of words and sentences.</p><ul><li>Kim et al. provided an excellent tutorial of deep latent variable models. The slides can be found <a href="http://nlp.seas.harvard.edu/latent-nlp-tutorial.html">here</a>.</li><li>In his talk at the BlackBox NLP workshop, Graham Neubig highlighted latent variables as a way to model the <em>latent linguistic structure</em> of text with neural network. In particular, he discussed <a href="http://aclweb.org/anthology/P17-1029">multi-space variational encoder-decoders</a> and <a href="http://aclweb.org/anthology/P18-1070">tree-structured variational auto-encoders</a>, two semi-supervised learning models that leverage latent variables to take advantage of unlabeled data.</li><li>In <a href="http://aclweb.org/anthology/D18-1042">our paper</a>, we showed how cross-lingual embedding methods can be seen as latent variable models. We can use this insight to derive an EM algorithm and learn a better alignment between words.</li><li><a href="http://aclweb.org/anthology/D18-1062">Dou et al.</a> similarly propose a latent variable model based on a variational auto-encoder for unsupervised bilingual lexicon induction.</li><li>In the model by <a href="http://aclweb.org/anthology/D18-1088">Zhang et al.</a>, sentences are viewed as latent variables for summarization. Sentences with activated variables are extracted and directly used to infer gold summaries.</li><li>There were also papers that proposed methods for more general applications. <a href="http://aclweb.org/anthology/D18-1480">Xu and Durrett</a> propose to use a different distribution in variational auto-encoders that mitigates the common failure mode of a collapsing KL divergence.</li><li><a href="http://aclweb.org/anthology/D18-1108">Niculae et al.</a> propose a new approach to build dynamic computation graphs with latent structure through sparsity.</li></ul><h2 id="language-models">Language models</h2><p>Language models are becoming more commonplace in NLP and many papers investigated different architectures and properties of such models.</p><ul><li>In an insightful paper, <a href="http://aclweb.org/anthology/D18-1179">Peters et al.</a> show that LSTMs, CNNs, and self-attention language models all learn high-quality representations. They additionally show that the representations vary with network depth: morphological information is encoded at the word embedding layer; local syntax is captured at lower layers and longer-range semantics are encoded at the upper layers. </li><li><a href="http://aclweb.org/anthology/D18-1503">Tran et al.</a> show that LSTMs generalize hierarchical structure better than self-attention. This hints at possible limitations of the Transformer architecture and suggests that we might need different encoding architectures for different tasks.</li><li><a href="http://aclweb.org/anthology/D18-1458">Tang et al.</a> find that the Transformer and CNNs are not better than RNNs at modeling long-distance agreement. However, models relying on self-attention excel at word sense disambiguation. </li><li>Other papers looks at different properties of language models. <a href="http://aclweb.org/anthology/D18-1523">Amrami and Goldberg</a> show that language models can achieve state-of-the-art for unsupervised word sense induction. Importantly, rather than just providing the left and right context to the word, they find that appending "and" provides more natural and better results. It will be interesting to see what other clever uses we will find for LMs.</li><li><a href="http://aclweb.org/anthology/D18-1505">Krishna et al.</a> show that ELMo performs better than logic rules on sentiment analysis tasks. They also demonstrate that language models can implicitly learn logic rules.</li><li>In the best paper at the BlackBoxNLP workshop, <a href="http://aclweb.org/anthology/W18-5426">Giulianelli et al.</a> use diagnostic classifiers to keep track and improve number agreement in language models.</li><li>In another BlackBoxNLP paper, <a href="http://aclweb.org/anthology/W18-5423">Wilcox et al.</a> show that RNN language models can represent filler-gap dependencies and learn a particular subset of restrictions known as island constraints.  </li></ul><h2 id="datasets">Datasets</h2><p>Many new tasks and datasets were presented at the conference, many of which propose more challenging settings.</p><ul><li><strong>Grounded common sense inference</strong>: <a href="http://aclweb.org/anthology/D18-1009">SWAG</a> contains 113k multiple choice questions about a rich spectrum of grounded situations.</li><li><strong>Coreference resolution</strong>: <a href="http://aclweb.org/anthology/D18-1016">PreCo</a> contains 38k documents and 12.5M words, which are mostly from the vocabulary of English-speaking preschoolers.</li><li><strong>Document grounded dialogue</strong>: The dataset by <a href="http://aclweb.org/anthology/D18-1076">Zhou et al.</a> contains 4112 conversations with an average of 21.43 turns per conversation.</li><li><strong>Automatic story generation from videos</strong>: <a href="http://aclweb.org/anthology/D18-1117">VideoStory</a> contains 20k social media videos amounting to 396 hours of video with 123k sentences, temporally aligned to the video.</li><li><strong>Sequential open-domain question answering</strong>: <a href="http://aclweb.org/anthology/D18-1134">QBLink</a> contains 18k question sequences, with each sequence consisting of three naturally occurring human-authored questions.</li><li><strong>Multimodal reading comprehension</strong>: <a href="http://aclweb.org/anthology/D18-1166">RecipeQA</a> consists of 20k instructional recipes with multiple modalities such as titles, descriptions and aligned set of images and 36k automatically generated question-answer pairs.</li><li><strong>Word similarity</strong>: <a href="http://aclweb.org/anthology/D18-1169">CARD-660</a> consists of 660 manually selected rare words with manually selected paired words and expert annotations.</li><li><strong>Cloze style question answering</strong>: <a href="http://aclweb.org/anthology/D18-1257">CLOTH</a> consists of 7,131 passages and 99,433 questions used in middle-school and high-school language exams. </li><li><strong>Multi-hop question answering</strong>: <a href="http://aclweb.org/anthology/D18-1259">HotpotQA</a> contains 113k Wikipedia-based question-answer pairs.</li><li><strong>Open book question answering</strong>: <a href="http://aclweb.org/anthology/D18-1260">OpenBookQA</a> consists of 6,000 questions and 1,326 elementary level science facts.</li><li><strong>Semantic parsing and text-to-SQL</strong>: <a href="http://aclweb.org/anthology/D18-1425">Spider</a> contains 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains.</li><li><strong>Few-shot relation classification</strong>: <a href="http://aclweb.org/anthology/D18-1514">FewRel</a> consists of 70k sentences on 100 relations derived from Wikipedia.</li><li><strong>Natural language inference</strong>: <a href="http://aclweb.org/anthology/D18-1187">MedNLI</a> consists of 14k sentence pairs in the clinical domain.</li><li><strong>Multilingual natural language inference</strong>: <a href="http://aclweb.org/anthology/D18-1269">XNLI</a> extends the MultiNLI dataset to 15 languages. </li><li><strong>Task-oriented dialogue modeling</strong>: <a href="http://aclweb.org/anthology/D18-1547">MultiWOZ</a>, which won the best resource paper award, is a Wizard-of-Oz style dataset consisting of 10k human-human written conversations spanning over multiple domains and topics.</li></ul><p>Papers also continued the trend of <a href="http://ruder.io/acl-2018-highlights/">ACL 2018</a> of analyzing the limitations of existing datasets and metrics.</p><ul><li><strong>Text simplification</strong>: <a href="http://aclweb.org/anthology/D18-1081">Sulem et al.</a> show that BLEU is not a good evaluation metric for sentence splitting, the most common operation in text simplification.</li><li><strong>Text-to-SQL</strong>: <a href="http://aclweb.org/anthology/D18-1197">Yavuz et al.</a> show what it takes to achieve 100% accuracy on the WikiSQL benchmark.</li><li><strong>Reading comprehension</strong>: <a href="http://aclweb.org/anthology/D18-1546">Kaushik and Lipton</a> show in the best short paper that models that only rely on the passage or the last sentence for prediction do well on many reading comprehension tasks.</li></ul><h2 id="miscellaneous">Miscellaneous </h2><p>These are papers that provide a refreshing take or tackle an unusual problem, but do not fit any of the above categories.</p><ul><li><a href="http://aclweb.org/anthology/D18-1182">Stanovsky and Hopkins</a> propose a novel way to test word representations. Their approach uses Odd-Man-Out puzzles, which consists of 5 (or more) words and require the system to choose the word that does not belong with the others. They show that such a simple setup can reveal various properties of different representations.</li><li>A similarly playful way to test the associative properties of word embeddings is proposed by <a href="http://aclweb.org/anthology/K18-1029">Shen et al.</a> They use a simplified version of the popular game <a href="https://boardgamegeek.com/boardgame/178900/codenames">Codenames</a>. In their setting, a speaker has to select an adjective to refer to two out of three nouns, which then need to be identified by the listener.</li><li>Causal understanding is important for many real-world application, but causal inference has so far not found much adoption in NLP. <a href="http://aclweb.org/anthology/D18-1488">Wood-Doughty et al.</a> demonstrate how causal analyses can be conducted for text classification and discuss opportunities and challenges and for future work.</li><li>Gender bias and equal opportunity are big issues in STEM. <a href="http://aclweb.org/anthology/D18-1301">Schluter</a> argues that a <a href="https://en.wikipedia.org/wiki/Glass_ceiling"><em>glass ceiling</em></a> exists in NLP, which prevents high achieving women and minorities from obtaining equal access to opportunities. While the field of NLP has been consistently ~33% female, Schluter analyzes 52 years of NLP publication data consisting of 15.6k gender-labeled authors and observes that the growth level of female seniority standing (indicated by last-authorship on a paper) falls significantly below that of the male population, with a gap that is widening.</li><li><a href="http://aclweb.org/anthology/D18-1533">Shillingford and Jones</a> tackle both an interesting problem and utilize a refreshing approach. They seek to recover missing characters for long vowels and glottal stops in Old Hawaiian Writing, which are important for reading comprehension and pronunciation. They propose to compose a finite-state transducer—which incorporates domain information—with a neural network. Importantly, their approach only requires modern Hawaiian texts for training.</li></ul><h2 id="other-reviews">Other reviews</h2><p>You might also find these other reviews of EMNLP 2018 helpful:</p><ul><li><a href="https://chauff.github.io/2018-11-04-emnlp/">Claudia Hauff</a> lists 28 papers that stood out to her, focusing on dataset papers or papers with a strong IR component.</li><li><a href="https://www.patricklewis.io/post/emnlp2018/">Patrick Lewis</a> provides a comprehensive overview of the conference, covering many of the tutorials and workshops as well as highlights from each session. </li><li><a href="https://supernlp.github.io/2018/11/10/emnlp-2018/">A group of PhD students and postdocs at the University of Copenhagen</a> wrote another very comprehensive overview of the conference.</li></ul>]]></content:encoded></item><item><title><![CDATA[HackerNoon Interview]]></title><description><![CDATA[This post is an interview by fast.ai fellow Sanyam Bhutani with me. It covers my background, advice on getting started with NLP, writing technical articles, and more.]]></description><link>http://ruder.io/hackernoon-interview/</link><guid isPermaLink="false">5c76ff7c1b9b0d18555b9eaf</guid><category><![CDATA[natural language processing]]></category><dc:creator><![CDATA[Sebastian Ruder]]></dc:creator><pubDate>Tue, 02 Oct 2018 10:40:18 GMT</pubDate><media:content url="http://ruder.io/content/images/2018/10/wordcloud_interview.png" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="http://ruder.io/content/images/2018/10/wordcloud_interview.png" alt="HackerNoon Interview"><p>This post is an interview by fast.ai fellow Sanyam Bhutani with me.</p>
<p><em>This post originally appeared at <a href="https://hackernoon.com/interview-with-deep-learning-and-nlp-researcher-sebastian-ruder-91ddaf473c4b">HackerNoon</a> with a different introduction</em>.</p>
<p>I had the honour to be interviewed by <a href="https://twitter.com/bhutanisanyam1">Sanyam Bhutani</a>, a Deep Learning and Computer Vision practitioner and <a href="http://www.fast.ai/2018/01/17/international-spring-2018/">fast.ai fellow</a> who's been doing a <a href="https://hackernoon.com/interview-with-kaggle-competitions-grandmaster-kazanova-rank-3-dr-marios-michailidis-cc515194cb67">series</a> interviewing people that inspire him. To be honest, it feels surreal to be the one being interviewed. I hope my answers may be interesting or useful to some of you.</p>
<p><strong>Sanyam</strong>: Hello Sebastian, Thank you for taking the time to do this.</p>
<p><strong>Sebastian</strong>: Thanks for having me.</p>
<p><strong>Sanyam</strong>: You’re working as a research scientist today at AYLIEN, and you’re a Ph.D. student at Insight Research Centre for Data Analytics. Could you tell the readers about how you got started? What got you interested in NLP and Deep Learning?</p>
<p><strong>Sebastian</strong>: I was really into maths and languages when I was in high school and took part in competitions. For my studies, I wanted to combine the logic of maths with the creativity of language somehow but didn’t know if such a field existed. That’s when I came across Computational Linguistics, which seemed to be a perfect fit at the intersection of computer science and linguistics. I then did my Bachelor’s in Computational Linguistics at the University of Heidelberg in Germany, one of my favourite places in Europe. During my Bachelor’s, I got most excited by machine learning, so I tried to get as much as exposure to ML as possible via internships and online courses. I only heard about word2vec as I was finishing my undergrad in 2015; as I learned more about Deep Learning at the start of my Ph.D. later that year, it seemed to be most exciting direction, so I decided to focus on it.</p>
<p><strong>Sanyam</strong>: You started your research right after graduation. What made you pick research as a career path instead of the industry?</p>
<p><strong>Sebastian</strong>: After graduating, I was planning to get some industry experience first by working in a startup. A PhD was always something I had dreamed of, but I hadn’t seriously considered it at that point. When I discussed working with the Dublin-based NLP startup Aylien, they told me about the Employment-based Postgraduate Programme, a PhD programme that is hosted jointly by a university and a company, which seemed like the perfect fit for me. Combining research and industry work can be challenging at times, but has been rewarding for me overall. Most importantly, there should be a fit with the company.</p>
<p><strong>Sanyam</strong>: You’ve been working as a researcher for 3 years now. What has been your favorite project during these years?</p>
<p><strong>Sebastian</strong>: In terms of learning, delving into a new area where I don’t know much, reading papers, and getting to collaborate with great people. In this vein, my project working on multi-task learning at the University of Copenhagen was a great and very stimulating experience. In terms of impact, being able to work with Jeremy, interacting with the fastai community, and seeing that people find our work on language models useful.</p>
<p><strong>Sanyam</strong>: Natural Language Processing has arguably lagged behind Computer Vision. What are your thoughts about the current scenario? Is it a good time to get started as an NLP Practitioner?</p>
<p><strong>Sebastian</strong>: I think now is a great time to get started with NLP. Compared to a couple of years ago, we’re at a point of maturity where you’re not limited to just using word embeddings or off-the-shelf models, but you can compose your model from a wide array of components, such as different layers, pretrained representations, auxiliary losses, etc. There also seems to be a growing feeling in the community that many of the canonical problems (POS tagging and dependency parsing on the Penn Treebank, sentiment analysis on movie reviews, etc.) are close to being solved, so we really want to make progress on more challenging problems, such as “real” natural language understanding and creating models that truly generalize. For these problems, I think we can really benefit from people with new perspectives and ideas. In addition, as we can now train models for many useful tasks such as classification or sequence labelling with good accuracy, there are a lot of opportunities for applying and adapting these models to other languages. If you’re a speaker of another language, you can make a big difference by creating datasets others can use for evaluation and training models for that language.</p>
<p><strong>Sanyam</strong>: For the readers and the beginners who are interested in working on Natural Language Processing, what would be your best advice?</p>
<p><strong>Sebastian</strong>: Find a task you’re interested in for instance by browsing the tasks on NLP-progress. If you’re interested in doing research, try to choose a particular subproblem not everyone is working on. For instance, for sentiment analysis, don’t work on movie reviews but conversations. For summarization, summarize biomedical papers rather than news articles. Read papers related to the task and try to understand what the state-of-the-art does. Prefer tasks that have open-source implementations available that you can run. Once you have a good handle of how something works, for research, reflect if you were surprised by any choices in the paper. Try to understand what kind of errors the model makes and if you can think of any information that could be used to mitigate them. Doing error and ablation analyses or using synthetic tasks that gauge if a model captures a certain kind of information are great ways to do this.</p>
<p>If you have an idea how to make the task more challenging or realistic, try to create a dataset and apply the existing model to that task. Try to recreate the dataset in your language and see if the model performs equally well.</p>
<p><strong>Sanyam</strong>: Many job boards (For DL/ML) require the applicants to be post-grads or have research experience. For the readers who want to take up Machine Learning as a Career path, do you feel having research experience is a necessity?</p>
<p><strong>Sebastian</strong>: I think research experience can be a good indicator that you’re proficient with certain models and creative, innovative to come up with new solutions. You don’t need to do a Ph.D. or a research fellowship to learn these skills, though. Being proactive, learning about and working on a problem that you’re excited about, trying to improve the model, and writing about your experience is a good way to get started and demonstrate similar skills. In most applied ML settings, you won’t be required to come up with totally new ways to solve a task. Doing ML and data science competitions can thus similarly help you demonstrate that you know how to apply ML models in practice.</p>
<p><strong>Sanyam</strong>: Given the explosive growth rates in research, How do you stay up to date with the cutting edge?</p>
<p><strong>Sebastian</strong>: I’ve been going through the arXiv daily update, adding relevant papers to my reading list, and reading them in batches. Jeff Dean recently said during a talk at the Deep Learning Indaba that he thinks it’s better to read ten abstracts than one paper in-depth as you can always go back and read one of the papers in-depth. I agree with him. I think you want to read widely about as many ideas as possible, which you can catalogue and use for inspiration later. Having a good paper management system is key. I’ve been using Mendeley. Lately, I’ve been relying more on Arxiv Sanity Preserver to surface relevant papers.</p>
<p><strong>Sanyam</strong>: You also maintain a great blog, which I’m a great fan of. Could you share some tips on effectively writing technical articles?</p>
<p><strong>Sebastian</strong>: I’ve had the best experience writing a blog when I started out writing it for myself to understand a particular topic better. If you ever find yourself having to put in a lot of work to build intuition or do a lot of research to grasp a subject, consider writing a post about it so you can accelerate everyone else’s learning in the future. In research papers, there’s usually not enough space to properly contextualize a work, highlight motivations, and intuitions, etc. Blog posts are a great way to make technical content more accessible and approachable.</p>
<p>The great thing about a blog is that it doesn’t need to be perfect. You can use it to improve your communication skills as well as obtain feedback on your ideas and things you might have missed. In terms of writing, I think the most important thing I have learned is to be biased towards clarity. Try to be as unambiguous as possible. Remove sentences that don’t add much value. Remove vague adjectives. Write only about what the data shows and if you speculate, clearly say so.<br>
Get feedback on your draft from your friends and colleagues. Don’t try to make something 100% perfect, but get it to a point where you’re happy with it. Feeling anxiety when clicking that ‘Publish’ button is totally normal and doesn’t go away. Publishing something will always be worth it in the long-term.</p>
<p><strong>Sanyam</strong>: Do you feel Machine Learning has been overhyped?</p>
<p><strong>Sebastian</strong>: No.</p>
<p><strong>Sanyam</strong>: Before we conclude, any tips for the beginners who are afraid to get started because of the idea that Deep Learning is an advanced field?</p>
<p><strong>Sebastian</strong>: Don’t let anyone tell you that you can’t do this. Do online courses to build your understanding. Once you’re comfortable with the fundamentals, read papers for inspiration when you have time. Choose something you’re excited about, choose a library, and work on it. Don’t think you need massive compute to work on meaningful problems. Particularly in NLP, there are lot of problems with a small number of labelled examples. Write about what you’re doing and learning. Reach out to people with similar interests and focus areas. Engage with the community, e.g. the fastai community is awesome. Get on Twitter. Twitter has a great ML community and you can often get replies from top experts in the field way faster than via email. Find a mentor. If you write to someone for advice, be mindful of their time. Be respectful and try to help others. Be generous with praise and cautious with criticism.</p>
<p><strong>Sanyam</strong>: Thank you so much for doing this interview.</p>
<p>The cover image for this post was generated based on the content of the post using <a href="https://www.wordclouds.com/">wordcouds</a>.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item></channel></rss>